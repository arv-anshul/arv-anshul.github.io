{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> <p>Anshul Raj Verma</p> <p>Hello! I'm Anshul Raj Verma, a Data Scientist passionate about harnessing GenAI to solve real-world problems. Since 2021, I've sharpened my skills through a six-month Data Science &amp; AI internship and a series of self-driven projects, becoming proficient in Python and Data Science concepts. I've also built end-to-end GenAI applications using LangChain, LangGraph, LlamaIndex, and core MCP concepts\u2014experience that's fueled my drive to design intelligent, scalable solutions. </p> </p> <p> About </p> <p> Blogs </p> <p> Projects </p> <p> Resume </p> <p> GitHub </p> <p> Friends </p>"},{"location":"friends/","title":"Friends","text":"<ul> <li> <p><p></p></p> <p><p>Anshul Raj Verma \u2022 Data Scientist</p></p> <p><p>I am learning Data Science, ML and DL for 3 years. Very fluent in Python language, I had built many ML projects using frameworks like FastAPI and Streamlit. You can check all my projects on website. </p></p> <p><p> :simple-linkedin:{ .lg .light .hover-icon-bounce title=\"LinkedIn Profile\" } </p></p> </li> <li> <p><p></p></p> <p><p>Wcowin \u2022 College Student</p></p> <p><p>I am a junior from China. I love programming technology and have done some research on web development and AI large models. You can learn more about me through my website. </p></p> <p><p> </p></p> </li> <li> <p><p></p></p> <p><p>Somto Ogbe \u2022 Student</p></p> <p><p>I am a Python Developer from Nigeria with keen interest in Backend Development and Data Science. I've very strong knowledge of Django, Flask and FastAPI for Web development and also have a solid foundation in Machine Learning and Deep Learning especially in Computer Vision. I just love to see things work! </p></p> <p><p> :simple-linkedin:{ .lg .light .hover-icon-bounce title=\"LinkedIn profile\" } </p></p> </li> <li> <p><p></p></p> <p><p>Rahul Kumar \u2022 Data Scientist</p></p> <p><p>I am a passionate data science enthusiast with a knack for turning complex data into meaningful insights. As a beginner in the field, I am constantly learning and expanding my skill set. My expertise lies in utilizing Python, SQL, different python libraries, machine learning, statistics, and applied mathematics.</p></p> <p><p> :simple-linkedin:{ .lg .light .hover-icon-bounce title=\"LinkedIn profile\" } </p></p> </li> <li> <p><p></p></p> <p><p>Dipanjan Pathak \u2022 Data Scientist</p></p> <p><p>A data science enthusiast with full-stack web development expertise, passionate about problem-solving and learning new technologies. Focused on creating data-driven solutions, improving processes, and leveraging data science and development skills to build impactful applications and contribute to innovative, technology-driven projects. Check all my projects on website.</p></p> <p><p> :simple-linkedin:{ .lg .light .hover-icon-bounce title=\"LinkedIn profile\" } </p></p> </li> </ul>"},{"location":"resume/","title":"Anshul Raj Verma - Resume","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/bash-tips/","title":"Bash: Tips and Tricks","text":"<p>This page contains some Tips and Tricks which are helpful to work with Bash scripts or Terminals. I have mentioned only those tips and tricks which I uses most in my daily routine, BTW you can refer to links which I have shared to know all the tips and tricks.</p>"},{"location":"blog/bash-tips/#tips-by-asottile","title":"Tips by <code>asottile</code>","text":"<p>I will only mention those commands and shortcuts which helpful for me. You can check all of them using below links.</p> <ul> <li> Protips - YouTube Video</li> <li> Protips - Github Wiki Link</li> </ul>"},{"location":"blog/bash-tips/#command","title":"Command: <code>!!</code>","text":"<p>A substitution which contains the previous command, some useful invocations.</p> <pre><code>$ ls /proc/1/exe\nls: cannot access '/proc/1/exe': Permission denied\n\n$ sudo !!\nsudo ls /proc/1/exe\n/proc/1/exe\n</code></pre> <pre><code>$ ls /tmp/ | grep sys\nsystemd-private-e21be514f23449189063b6bd95ec13ef-bolt.service-MCMIaN\nsystemd-private-e21be514f23449189063b6bd95ec13ef-colord.service-yNWr06\nsystemd-private-e21be514f23449189063b6bd95ec13ef-fwupd.service-geozA1\n\n$ watch \"!!\"\nwatch \"ls /tmp/ | grep sys\"\n(opens up watch panel)\n</code></pre>"},{"location":"blog/bash-tips/#command_1","title":"Command: <code>!$</code>","text":"<p>A substitution which contains the last segment of the previous command.</p> <pre><code>$ $EDITOR test.py\n\n$ python !$\npython test.py\nHello world!\n</code></pre> <pre><code>$ mkdir new-project\n$ code !$  # Opens the `new-project folder in VSCode\n</code></pre>"},{"location":"blog/bash-tips/#shortcut-ctrl","title":"Shortcut: ++ctrl+\"\\\"++","text":"<p>++ctrl+\"\\\"++ sends <code>SIGQUIT</code> (default behavior: terminate + produce a core dump) which can be useful to kill things that normally catch ++ctrl+\"c\"++ (<code>SIGINT</code>).</p> <p> ++ctrl+\"\\\"++ is more powerful than ++ctrl+\"c\"++.</p>"},{"location":"blog/bash-tips/#additional-tricks","title":"Additional Tricks","text":""},{"location":"blog/bash-tips/#trick-cmd","title":"Trick: <code>cmd `!!`</code>","text":"<ul> <li>Run previous command with new command.</li> <li>Replace previous command with <code>`!!`</code> argument.</li> </ul> <pre><code>$ brew --prefix\n/opt/homebrew\n\n$ open `!!`\nopen `brew --prefix`\n# Now Finder.app opens at \"/opt/homebrew\" path\n</code></pre>"},{"location":"blog/bash-tips/#uninstall-apps-from-mac","title":"Uninstall Apps from Mac","text":"<p>Unistall and remove all app data from your Mac using this script. For more information see this youtube video.</p>"},{"location":"blog/bash-tips/#remove-macs-login-items","title":"Remove Mac's \"Login Items\"","text":"<ol> <li>Go to <code>/Library/LaunchAgents</code> and <code>/Library/LaunchDaemons</code> path.</li> <li>Check for the login item names and delete them.</li> <li>This propcess might asks for password.</li> </ol>"},{"location":"blog/bash-useful-commands/","title":"Useful Commands","text":""},{"location":"blog/bash-useful-commands/#bash","title":"Bash","text":""},{"location":"blog/bash-useful-commands/#display-ansi-colors-with-their-color-code","title":"Display ANSI colors with their color-code.","text":"<pre><code>function colormap() {\n    range_start=${1:-1}\n    range_end=${2:-255}\n\n    for i in $(seq $range_start $range_end); do\n        echo -en \"\\e[48;5;${i}m  ${(l:3::0:)i}  \\e[0m \"\n        [[ $((i % 10)) -eq 0 ]] &amp;&amp; echo\n    done\n    return 0\n}\n</code></pre> <p>The <code>colormap</code> function will print the ANSI colors with codes in your terminal.</p> <pre><code>$ colormap           # (000 - 255)\n$ colormap 200       # (200 - 255)\n$ colormap 100 120   # (100 - 120)\n</code></pre>"},{"location":"blog/bash-useful-commands/#docker","title":"Docker","text":""},{"location":"blog/bash-useful-commands/#clear-all-caches-images-containers-volumes-of-docker","title":"Clear all caches, images, containers, volumes of Docker.","text":"<pre><code>docker system prune -a --volumes --force\n</code></pre>"},{"location":"blog/bash-useful-commands/#git","title":"Git","text":""},{"location":"blog/bash-useful-commands/#beautiful-oneline-git-log","title":"Beautiful Oneline - <code>git log</code>","text":"<pre><code>alias glog=\"git log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit --branches\"\n</code></pre> <pre><code>$ glog # (1)!\n* 6914e04 - (HEAD -&gt; main, origin/main) \ud83d\udcdd Update docs/index.md (5 hours ago) &lt;Anshul Raj Verma&gt;\n*   3d91241 - Merge branch 'new/friends-page' (5 hours ago) &lt;Anshul Raj Verma&gt;\n|\\\n| * 6160ae5 - \ud83d\udc85 Update UI of friends.md (5 hours ago) &lt;Anshul Raj Verma&gt;\n| * 42b52d8 - \ud83d\udd28 Add friends.md to Home nav (6 hours ago) &lt;Anshul Raj Verma&gt;\n| * bef45f6 - \u2728 Setup to add friends on website (2 days ago) &lt;Anshul Raj Verma&gt;\n|/\n* 59340cf - ref: \u2728 Add starship.md in Bash (2 days ago) &lt;Anshul Raj Verma&gt;\n* ecc8e94 - \u2728 Add new nav item \"References\" (3 days ago) &lt;Anshul Raj Verma&gt;\n* 3bea521 - \u2728 Add Rust icon on about page (5 days ago) &lt;Anshul Raj Verma&gt;\n</code></pre> <ol> <li>Date: 10-04-2024</li> </ol>"},{"location":"blog/bash-useful-commands/#python","title":"Python","text":""},{"location":"blog/bash-useful-commands/#remove-all-cache-file-specify-to-a-python-project","title":"Remove all cache file specify to a Python project","text":"<pre><code>function pycls() {\n    find . -name '.DS_Store' -type f -delete\n    find . -type d -name \"__pycache__\" -exec rm -rfv {} \\; 2&gt;/dev/null\n    find . -type d -name \".ipynb_checkpoints\" -exec rm -rfv {} \\; 2&gt;/dev/null\n    find . -type d -name \".ruff_cache\" -exec rm -rfv {} \\; 2&gt;/dev/null\n    find . -type f -name \"*.pyc\" -exec rm -fv {} \\; 2&gt;/dev/null\n}\n</code></pre> <pre><code>$ pwd\n/User/Home  # For MacOS\n$ pycls  # Remove all cache files and folders in the directory and sub-dirs\n</code></pre>"},{"location":"blog/bash-useful-commands/#homebrew","title":"HomeBrew","text":""},{"location":"blog/bash-useful-commands/#update-clean-and-doctor","title":"Update, Clean and Doctor","text":"<pre><code>brew update &amp;&amp; brew cleanup --prune=all &amp;&amp; brew doctor\n</code></pre>"},{"location":"blog/bash-useful-commands/#uninstall-unused-dependencies","title":"Uninstall unused dependencies","text":"<pre><code>brew autoremove # (1)!\n</code></pre> <ol> <li>Check docs for <code>autoremove</code> command.</li> </ol>"},{"location":"blog/bash-useful-commands/#arc","title":"Arc","text":""},{"location":"blog/bash-useful-commands/#change-arcapp-icon","title":"Change ARC.app Icon","text":"<p>Source Gist</p> <pre><code>defaults write company.thebrowser.Browser currentAppIconName candy # favorite\n</code></pre> Candy Arc Hologram Neon Fluted Glass Schoolbook Colorful <code>candy</code> <code>hologram</code> <code>neon</code> <code>flutedGlass</code> <code>schoolbook</code> <code>colorful</code>"},{"location":"blog/basics-of-statistics/","title":"Basics of Statistics for ML","text":"<pre><code>graph TD;\n    Inferential_Statistics[\"Inferential Statistics\"]\n    Descriptive_Statistics[\"Descriptive Statistics\"]\n    Measure_of_Central_Tendency[\"Measure of Central Tendency\"]\n    Weighted_Mean[\"Weighted Mean\"]\n    Trimmed_Mean[\"Trimmed Mean\"]\n    Measure_of_Dispersion[\"Measure of Dispersion\"]\n    Standard_Deviation[\"Standard Deviation\"]\n    CV[\"Coefficient of Variation\"]\n    Five_Number_Summary[\"5 Number Summary\"]\n    Box_Plot[\"Box Plot / Whisker Plot\"]\n\n    Statistics --&gt; Descriptive_Statistics\n    Statistics --&gt; Inferential_Statistics\n    Descriptive_Statistics --&gt; Measure_of_Central_Tendency\n    Descriptive_Statistics --&gt; Measure_of_Dispersion\n    Measure_of_Central_Tendency --&gt; Mean\n    Measure_of_Central_Tendency --&gt; Median\n    Measure_of_Central_Tendency --&gt; Mode\n    Mean --&gt; Weighted_Mean\n    Mean --&gt; Trimmed_Mean\n    Measure_of_Dispersion --&gt; UniVariate\n    Measure_of_Dispersion --&gt; BiVariate\n    UniVariate --&gt; Range\n    UniVariate --&gt; Variance\n    UniVariate --&gt; Standard_Deviation\n    UniVariate --&gt; CV\n    UniVariate --&gt; Five_Number_Summary\n    Five_Number_Summary --&gt; Percentile\n    Five_Number_Summary --&gt; Box_Plot\n    BiVariate --&gt; Covariance\n    BiVariate --&gt; Correlation</code></pre>"},{"location":"blog/basics-of-statistics/#statistics","title":"Statistics","text":"<ul> <li> Descriptive Statistics Part 1</li> <li> Descriptive Statistics Part 2</li> <li> CampusX Notes - 1</li> <li> CampusX Notes - 2</li> <li> My Previous Notes</li> </ul>"},{"location":"blog/basics-of-statistics/#population","title":"Population","text":"<p>Population represents the whole/entire group of individual or object that we are interested in studying.</p>"},{"location":"blog/basics-of-statistics/#sample","title":"Sample","text":"<p>Sample is a subset of Population. It is smaller group of individual or object that we select from the population to study. Samples are used to estimate characteristics of the population.</p> <p>Things to care while selecting Sample from a Population</p> <ul> <li>Sample Size</li> <li>Random Data</li> <li>Representative</li> </ul> <p>Example: Population and Sample Data</p> <ol> <li>All Python Programmer on the earth VS Python Programmer in India</li> <li>All cricket fans VS Fans who were present in the stadium</li> <li>All students VS Who visit college for lectures</li> </ol>"},{"location":"blog/basics-of-statistics/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>Helps you to learn underlying information of the data. Generally, data is a Sample of the Population.</p> <p>The goal of statistical inference is to use the information obtained from the sample to make inferences about the population parameters.</p>"},{"location":"blog/basics-of-statistics/#measure-of-central-tendency","title":"Measure of Central Tendency","text":"<p>A measure of central tendency is a statistical measure that represents a typical or central value for a dataset. It provides a summary of the data by identifying a single value that is most representative of the dataset as a whole.</p> You can use methods like Mean, Median, Mode for this. <p>Mean: The mean is the sum of all values in the dataset divided by the number of values.</p> <p>Median: The median is the middle value in the dataset when the data is arranged in order.</p> <p>Mode: The mode is the value that appears most frequently in the dataset.</p>"},{"location":"blog/basics-of-statistics/#measure-of-dispersion","title":"Measure of Dispersion","text":"<p>A measure of dispersion is a statistical measure that describes the spread or variability of a dataset. It provides information about how the data is distributed around the central tendency (mean, median or mode) of the dataset.</p>"},{"location":"blog/basics-of-statistics/#variance","title":"Variance","text":"<p>The variance is the average of the squared differences between each data point and the mean. It measures the average distance of each data point from the mean and is useful in comparing the dispersion of datasets with different means.</p> Sample Variance Population Variance \\(\\sigma^2 = \\frac{\\sum\\_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\) \\(\\sigma^2 = \\frac{\\sum\\_{i=1}^{N} (x_i - \\mu)^2}{N}\\) <p>Why <code>n-1</code> in Sample Variance?</p> <p>By dividing by \\((n\u22121)\\), we make the sample variance an unbiased estimator of the population variance. This correction is particularly important when dealing with small sample sizes, as it helps to reduce bias in the estimation of the population variance.</p>"},{"location":"blog/basics-of-statistics/#standard-deviation","title":"Standard Deviation","text":"<p>The standard deviation is the square root of the variance. It is a widely used measure of dispersion that is useful in describing the shape of a distribution like Normal Distribution.</p> Sample Standard Deviation Population Standard Deviation \\(s = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\\) \\(\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}}\\)"},{"location":"blog/basics-of-statistics/#coefficient-of-variation-cv","title":"Coefficient of Variation (CV)","text":"<p>CV is the ratio of the standard deviation to the mean expressed as a percentage. It is used to compare the variability of datasets with mean. CV is a statistical measure that expresses the amount of variability in a dataset relative to the mean.</p>"},{"location":"blog/basics-of-statistics/#fracsigmamu-cdot-100-textcv","title":"\\(\\frac{\\sigma}{\\mu} \\cdot 100 = \\text{CV}\\%\\)","text":""},{"location":"blog/basics-of-statistics/#why-coefficient-of-variation-exits","title":"Why Coefficient of Variation exits?","text":"<p>This method is used to compare the variance of two different feature like Salary of Employees and Experience of Employees. But using CV you can compare the variability of those two feature.</p> <p>Steps</p> <ol> <li>Calculate Mean and Standard Deviation of both feature.</li> <li>Calculate CV of each feature using above formula.</li> <li>Now, you can compare both feature's CV to know which is feature has more     variability.</li> </ol> <p> Note: This statistical method is not widely used.</p>"},{"location":"blog/basics-of-statistics/#inferential-statistics","title":"Inferential Statistics","text":"<p>Using Inferential Statistics we make conclusions and predictions about a population based on a sample.</p> <p>It involves the use of probability theory to estimate the likelihood of certain events occurring, hypothesis testing to determine if a certain claim about a population is supported by the data, and regression analysis to examine the relationships between variables.</p>"},{"location":"blog/basics-of-statistics/#covariance","title":"Covariance","text":"<p>Covariance is a statistical measure that describes the degree to which two variables are linearly related. It measures how much two variables change together, such that when one variable increases, does the other variable also increase, or does it decrease?</p>"},{"location":"blog/basics-of-statistics/#textcovx-y-fracsum_i-0nx_i-barx-y_i-baryn-1","title":"\\(\\text{Cov}(x, y) = \\frac{\\sum_{i = 0}^{n}{(x_i - \\bar{x}) (y_i - \\bar{y})}}{n - 1}\\)","text":""},{"location":"blog/basics-of-statistics/#interpretation","title":"Interpretation","text":"<p>If the covariance between two variables is positive, it means that the variables tend to move together in the same direction. If the covariance is negative, it means that the variables tend to move in opposite directions. A covariance of zero indicates that the variables are not linearly related.</p>"},{"location":"blog/basics-of-statistics/#disadvantage","title":"Disadvantage","text":"<p>One limitation of covariance is that it does not tell us about the strength of the relationship between two variables, since the magnitude of covariance is affected by the scale of the variables.</p>"},{"location":"blog/basics-of-statistics/#variance-vs-covariance","title":"Variance V/S CoVariance","text":"<ul> <li>Covariance is a measure of how much two variables change together, while variance is a measure of how much a single variable changes from its mean.</li> <li>The variance of a variable is the average of the squared differences from the mean, while the covariance between two variables is the average product of their deviations from their respective means.</li> </ul>"},{"location":"blog/basics-of-statistics/#corelation","title":"Corelation","text":"<p>Correlation refers to a statistical relationship between two or more variables. Specifically, it measures the degree to which two variables are related and how they tend to change together.</p>"},{"location":"blog/basics-of-statistics/#textcorrelation-fractextcovx-ysigma-x-sigma-y","title":"\\(\\text{Correlation} = \\frac{\\text{Cov}(x, y)}{\\sigma x * \\sigma y}\\)","text":""},{"location":"blog/basics-of-statistics/#interpretation_1","title":"Interpretation","text":"<p>Correlation is often measured using a statistical tool called the correlation coefficient, which ranges from -1 to 1, A correlation coefficient of -1 indicates a perfect negative correlation, a correlation coefficient of 0 indicates no correlation, and a correlation coefficient of 1 indicates a perfect positive correlation.</p>"},{"location":"blog/basics-of-statistics/#correlation-and-causation","title":"Correlation and Causation","text":"<p>The phrase \"correlation does not imply causation\"<sup>1</sup> means that just because two variables are associated with each other, it does not necessarily mean that one causes the other.</p> <p>In other words, a correlation between two variables does not necessarily imply that one variable is the reason for the other variable's behavior.</p> <p>Example</p> <p>Suppose there is a positive correlation between the number of  firefighters present at a fire and the amount of  damage caused by the  fire. One might be tempted to conclude that the presence of  firefighters causes more  damage. However, this correlation could be explained by a third variable - the severity of the fire. More severe fires might require more firefighters to be present, and also cause more damage.</p> <p>Thus, while correlations can provide valuable insights into how different variables are related, they cannot be used to establish causality. Establishing causality often requires additional evidence such as experiments, randomized controlled trials, or well-designed observational studies.</p> <ol> <li> <p>\"correlation does not imply causation\": Agar koi ghatna correlated hai toh iska matlab yeh nhi ki pehle ghatna ki hone ki wajah se he dusri ghatna ho rhi hai.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/clustering-algo/","title":"Clustering Algorithms in ML","text":""},{"location":"blog/clustering-algo/#applications-of-clustering","title":"Applications of Clustering","text":"<ul> <li>Customer Segmentation: To show personalized ADs to customers.</li> <li>Data Analysis: Perform analysis to each cluster after performing clustering on the whole dataset.</li> <li>Semi Supervised Learning: Google Photos uses this technique to identify person's face and put them into a separate folder.</li> <li>Image Segmentation: You can create segments in photos to represent different objects in the photo.</li> </ul>"},{"location":"blog/clustering-algo/#kmeans","title":"KMeans","text":""},{"location":"blog/clustering-algo/#working-steps","title":"Working Steps","text":"<ol> <li>You tell the algorithm how many clusters are there in the data (this is assumption which is taken before initialization).</li> <li>The cluster's centroids are initialize with some random values.</li> <li>The distance of each data point is calculated with each cluster and then the data points are assigned to nearest clusters.</li> <li>After data points assignment to clusters, the centroid of the clusters is being calculated.</li> <li>If the centroid's position is:<ul> <li>Same as before then the algorithm stops.</li> <li>Not same as before then the STEP 3 is being re-calculated and the process goes on.</li> </ul> </li> </ol> KMeans in Diagram <pre><code>graph TD\n  subgraph Initialization\n    A(Initialize Centroids)\n    B[Calculate Distance and \\n Assign Data Points]\n  end\n\n  subgraph Iterative Process\n    C[Update Centroids]\n    D[Reassign Data Points]\n  end\n\n  subgraph Convergence Check\n    E[/Converged?/]\n  end\n\n  A --&gt;|Random Initialization| B\n  B --&gt;|Assign Nearest Centroid| C\n  C --&gt;|Recalculate Centroids| D\n  D --&gt;|Assign New Clusters| E\n  E --&gt;|Yes| stop{Stop}\n  E --&gt;|No| B</code></pre> <p>Some important terms!</p> <ul> <li>Inertia also known as within-cluster sum of squares (WCSS) in the context of K-Means clustering, is a measure that quantifies the compactness of clusters. It is calculated as the sum of the squared distances between each data point in a cluster and the centroid of that cluster.</li> <li>Elbow Method is way to decide the number of clusters present in a data. However, this is not a very good method to estimate clusters but it there to help you for that.</li> </ul>"},{"location":"blog/clustering-algo/#assumptions-of-kmeans","title":"Assumptions of KMeans","text":"<ul> <li>Spherical Cluster Shape: K-means assumes that the clusters are spherical and isotropic, meaning they are uniform in all directions. Consequently, the algorithm works best when the actual clusters in the data are circular (in 2D) or spherical (in higher dimensions).</li> <li>Similar Cluster Size: The algorithm tends to perform better when all clusters are of approximately the same size. If one cluster is much larger than others, K-means might struggle to correctly assign the points to the appropriate cluster.</li> <li>Equal Variance of Clusters: K-means assumes that all clusters have similar variance. The algorithm uses the Euclidean distance metric, which can bias the clustering towards clusters with lower variance.</li> <li>Clusters are Well Separated: The algorithm works best when the clusters are well separated from each other. If clusters are overlapping or intertwined, K-means might not be able to distinguish them effectively.</li> <li>Number of Clusters (k) is Predefined: K-means requires the number of clusters (k) to be specified in advance. Choosing the right value of k is crucial, but it is not always straightforward and typically requires domain knowledge or additional methods like the Elbow method or Silhouette analysis.</li> <li>Large n, Small k: K-means is generally more efficient and effective when the dataset is large (large n) and the number of clusters is small (small k).</li> </ul>"},{"location":"blog/clustering-algo/#resources-to-learn-from","title":"Resources to Learn From","text":"Abstract \u00a0  <p>Introduction</p> <ul> <li>Objective: Divide data into k clusters.</li> <li>Algorithm: Iterative process that minimizes the sum of squared distances between data points and their assigned cluster centroids.</li> </ul> <p>Algorithm Steps</p> <ul> <li>Initialization: Randomly select k initial centroids.</li> <li>Assignment: Assign each data point to the nearest centroid.</li> <li>Update Centroids: Recalculate centroids based on assigned points.</li> <li>Repeat: Iteratively repeat assignment and centroid update until convergence.</li> </ul> <p>Advantages</p> <ul> <li>Simple and computationally efficient.</li> <li>Scales well to large datasets.</li> </ul> <p>Disadvantages</p> <ul> <li>Sensitive to initial centroid selection.</li> <li>Assumes clusters are spherical and equally sized.</li> </ul>"},{"location":"blog/clustering-algo/#dbscan","title":"DBSCAN","text":"<p>FULL FORM</p> <p>Density-Based Spatial Clustering of Applications with Noise</p> <p>DBSCAN is a clustering algorithm commonly used in data mining and machine learning. It's particularly effective in identifying clusters of arbitrary shapes and handling noise in the data. DBSCAN does not require the number of clusters (like KMeans) to be specified beforehand and can discover clusters based on the density of data points.</p>"},{"location":"blog/clustering-algo/#important-terms","title":"Important Terms","text":"<ul> <li>Core Points: A data point is considered a core point if there are at least \"min_samples\" data points (including itself) within a specified distance, usually denoted as \"epsilon\" (\u03b5). Core points are the central points of clusters.</li> <li>Border Points: A data point is considered a border point if it is within the specified distance (\u03b5) of a core point but doesn't have enough neighboring points to be considered a core point itself. Border points are part of a cluster but are not central to it.</li> <li>Noise Points: Data points that are neither core points nor border points are considered noise points or outliers. These points do not belong to any cluster.</li> </ul>"},{"location":"blog/clustering-algo/#working-steps_1","title":"Working Steps","text":"<ol> <li>Initialization: Select an arbitrary data point that has not been visited.</li> <li>Density Query: Find all data points within distance \u03b5 from the selected point.</li> <li>Core Point Check: If the number of points found is greater than or equal to \"min_samples\", mark the selected point as a core point, and a cluster is formed.</li> <li>Expand Cluster: Expand the cluster by recursively repeating the process for all the newly found core points.</li> <li>Next Point Selection: Choose a new unvisited point and repeat the process until all data points have been visited.</li> </ol>"},{"location":"blog/clustering-algo/#resources-to-learn-from_1","title":"Resources to Learn From","text":"CampusX \u00a0  StatQuest \u00a0  Abstract \u00a0  <p>Introduction</p> <ul> <li>Objective: Identify clusters based on dense regions in data space.</li> <li>Algorithm: Utilizes density information, considering data points as core, border, or noise.</li> </ul> <p>Algorithm Steps</p> <ul> <li>Density Estimation: Define \u03b5 (eps) and minimum points for a neighborhood.</li> <li>Core Points: Identify dense regions with at least 'minPts' neighbors within \u03b5.</li> <li>Expand Clusters: Connect core points and expand clusters with border points.</li> </ul> <p>Advantages</p> <ul> <li>Doesn't assume spherical clusters.</li> <li>Can find clusters of arbitrary shapes.</li> <li>Robust to outliers.</li> </ul> <p>Disadvantages</p> <ul> <li>Sensitivity to parameter settings (\u03b5, minPts).</li> <li>Struggles with varying density clusters.</li> </ul>"},{"location":"blog/clustering-algo/#gaussian-mixture-models","title":"Gaussian Mixture Models","text":"<p>Gaussian Mixture Models (GMMs) are probabilistic models used for clustering and density estimation. Unlike k-means, which assigns data points to a single cluster, GMMs allow each data point to belong to multiple clusters with different probabilities. GMMs assume that the data is generated from a mixture of several Gaussian distributions.</p> <p>Here are the key concepts associated with Gaussian Mixture Models:</p> <ol> <li>Gaussian Distribution (Normal Distribution): A probability distribution that is characterized by its mean (\u03bc) and standard deviation (\u03c3). The probability density function of a Gaussian distribution is given by:</li> </ol> \\[ f(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\] <ol> <li> <p>Mixture Model: A combination of multiple probability distributions. In the case of GMMs, these are Gaussian distributions.</p> </li> <li> <p>Parameters of GMM:</p> <ul> <li>Weights \\((\u03c0i)\\): The probabilities associated with each component (Gaussian distribution). They represent the likelihood of a data point belonging to a specific cluster.</li> <li>Means \\((\u03bci)\\): The mean values of the Gaussian distributions.</li> <li>Covariance Matrices \\((\u03a3i)\\): The covariance matrices representing the shape and orientation of the Gaussian distributions.</li> </ul> </li> <li> <p>Probability Density Function of GMM:</p> </li> </ol> \\[ P(x) = \\sum_{i=1}^{k} \\pi_i \\cdot \\mathcal{N}(x; \\mu_i, \\Sigma_i) \\] <p>where \\(\\mathcal{N}(x; \\mu_i, \\Sigma_i)\\) is the probability density function of the \\(i^{th}\\) Gaussian distribution.</p>"},{"location":"blog/clustering-algo/#working-steps_2","title":"Working Steps","text":"<ol> <li> <p>Initialization: Initialize the parameters of the model, including the weights, means, and covariance matrices.</p> </li> <li> <p>Expectation-Maximization (EM) Algorithm:</p> <ul> <li>Expectation Step (E-step): Compute the probabilities that each data point belongs to each cluster (responsibility).</li> <li>Maximization Step (M-step): Update the model parameters (weights, means, covariance matrices) based on the assigned responsibilities.</li> </ul> </li> <li> <p>Convergence: Repeat the E-step and M-step until the model converges, i.e., the parameters stabilize.</p> </li> <li> <p>Prediction: Once trained, the model can be used to predict the cluster assignments or estimate the density of new data points.</p> </li> </ol> <p>GMMs are flexible and can model complex data distributions. They are widely used in various applications, such as image segmentation, speech recognition, and anomaly detection.</p>"},{"location":"blog/clustering-algo/#resources-to-learn-from_2","title":"Resources to Learn From","text":"Serrano Academy \u00a0  Abstract \u00a0  <p>Introduction</p> <ul> <li>Objective: Model data as a mixture of Gaussian distributions.</li> <li>Algorithm: Probability-based approach using the Expectation-Maximization (EM) algorithm.</li> </ul> <p>Algorithm Steps</p> <ul> <li>Initialization: Assign initial parameters for Gaussian distributions.</li> <li>Expectation Step: Estimate probability of each data point belonging to each cluster.</li> <li>Maximization Step: Update parameters based on the expected assignments.</li> <li>Repeat: Iteratively repeat the E-M steps until convergence.</li> </ul> <p>Advantages</p> <ul> <li>More flexible in capturing different cluster shapes.</li> <li>Provides probabilistic cluster assignments.</li> </ul> <p>Disadvantages</p> <ul> <li>Sensitive to initialization.</li> <li>Computationally more expensive than K-Means.</li> </ul>"},{"location":"blog/overview-decision-tree/","title":"Overview: Decision Tree","text":"<p>Decision tree is a very crucial algorithm in ML world because using this algorithm there are many important and some of the best algo of ML is made upon like RandomForest and Xgboost.</p> <p>That's why we going to take a overview of Decision Tree in this blog.</p>"},{"location":"blog/overview-decision-tree/#anatomy-of-decision-trees","title":"Anatomy of Decision Trees","text":"<p>A decision tree is a popular machine learning algorithm used for both classification and regression tasks. It is a tree-like model composed of nodes, where each internal node represents a decision based on a feature, each branch represents the outcome of that decision, and each leaf node represents the final prediction.</p> <ul> <li>Root Node: The topmost node that makes the initial decision.</li> <li>Internal Nodes: Decision nodes that split the data based on a particular feature.</li> <li>Branches: Outcomes of decisions, leading to the next set of nodes.</li> <li>Leaf Nodes: Terminal nodes providing the final predictions.</li> </ul> <pre><code>graph TD\n    RootNode(Root Node) --&gt; |Decision Feature 1| InternalNode1(Internal Node 1)\n    InternalNode1 --&gt; |Decision Feature 2| InternalNode2(Internal Node 2)\n    InternalNode1 --&gt; |Decision Feature 3| InternalNode3(Internal Node 3)\n\n    InternalNode2 --&gt; |Decision Feature 4| LeafNode1(Leaf Node 1)\n    InternalNode2 --&gt; |Decision Feature 5| LeafNode2(Leaf Node 2)\n\n    InternalNode3 --&gt; |Decision Feature 6| LeafNode3(Leaf Node 3)\n    InternalNode3 --&gt; |Decision Feature 7| LeafNode4(Leaf Node 4)</code></pre>"},{"location":"blog/overview-decision-tree/#loss-functions","title":"Loss Functions","text":"<p>Decision trees use various loss functions depending on the task:</p> <ul> <li>Classification: Commonly use metrics like Gini impurity or cross-entropy.</li> <li>Regression: Typically use mean squared error.</li> </ul>"},{"location":"blog/overview-decision-tree/#gini-impurity-in-decision-trees","title":"Gini Impurity in Decision Trees","text":""},{"location":"blog/overview-decision-tree/#how-gini-impurity-works","title":"How Gini Impurity Works?","text":"<p>Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled. For a binary classification problem, the Gini impurity (Gini index) for a node is calculated as follows:</p> \\[ \\text{Gini(D)} = 1 - \\sum_{i=1}^{c} (p_i)^2 \\] <p>Where:</p> <ul> <li>\\((D)\\) is the dataset at the node.</li> <li>\\((c)\\) is the number of classes.</li> <li>\\((p_i)\\) is the probability of choosing a data point of class \\(i\\).</li> </ul>"},{"location":"blog/overview-decision-tree/#example","title":"Example","text":"<p>Consider a node with 30 samples, distributed among two classes (A and B) as follows:</p> <ul> <li>Class A: 15 samples</li> <li>Class B: 15 samples</li> </ul> \\[ \\text{Gini(D)} = 1 - \\left(\\left(\\frac{15}{30}\\right)^2 + \\left(\\frac{15}{30}\\right)^2\\right) \\] \\[ \\text{Gini(D)} = 1 - \\left(\\frac{1}{4} + \\frac{1}{4}\\right) \\] \\[ \\text{Gini(D)} = 1 - \\frac{1}{2} = \\frac{1}{2} \\] <p>The goal during the tree-building process is to minimize the Gini impurity at each node.</p> Watch StatQuest video on Decision Tree  <p></p> Gini Impurity Clearly Explained  <p></p>"},{"location":"blog/overview-decision-tree/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":"<ul> <li> <p>Interpretability</p> <p>Decision trees provide a transparent and intuitive representation of decision-making. This makes them valuable for communication with non-technical stakeholders. A data scientist can easily explain the logic behind predictions, fostering trust in the model.</p> </li> <li> <p>No Feature Scaling Required</p> <p>Decision trees are not affected by the scale of features. This means that data scientists don't have to spend time and effort on feature scaling, making the preprocessing phase simpler and more straightforward.</p> </li> <li> <p>Handle Mixed Data Types</p> <p>Decision trees can handle both numerical and categorical data without the need for one-hot encoding. This is advantageous when dealing with datasets that contain a mix of data types.</p> </li> <li> <p>Require Less Data Preprocessing</p> <p>Decision trees are less sensitive to outliers and missing values compared to some other algorithms. This can save time during the data cleaning and preprocessing stages.</p> </li> <li> <p>Feature Importance</p> <p>Decision trees provide a natural way to assess the importance of different features in the prediction. Data scientists can easily identify which features contribute more significantly to the model's decision-making process.</p> </li> <li> <p>Overfitting</p> <p>Decision trees are prone to overfitting, especially when the tree is deep and captures noise in the training data. Data scientists need to carefully tune hyperparameters, such as the tree depth, to prevent overfitting and ensure better generalization to new data.</p> </li> <li> <p>Instability</p> <p>Small variations in the training data can lead to different tree structures. This instability can make decision trees sensitive to the specific training dataset, requiring caution when deploying the model in different environments.</p> </li> <li> <p>Not Suitable for Complex Relationships</p> <p>Decision trees may not capture complex relationships in the data as effectively as more advanced algorithms.</p> </li> <li> <p>Biased Toward Dominant Classes</p> <p>In classification problems with imbalanced classes, decision trees can be biased toward the dominant class. This can impact the model's performance, especially when accurate predictions for minority classes are crucial.</p> </li> <li> <p>Not Well-Suited for Regression</p> <p>While decision trees are excellent for classification tasks, they may not perform as well for regression tasks on continuous data. Other algorithms like linear regression or support vector machines might be more appropriate in such cases.</p> </li> </ul>"},{"location":"blog/overview-decision-tree/#conclusion","title":"Conclusion","text":"<p>In conclusion, decision trees are powerful tools with a clear structure and interpretability. Understanding their components, loss functions, and characteristics will help you effectively apply and interpret this versatile algorithm.</p>"},{"location":"blog/docker-fastapi-uv/","title":"Dockerize - FastAPI - UV","text":"<p>Let's dockerize my <code>yt-comment-sentiment</code> project which is a FastAPI app managed via UV.</p>"},{"location":"blog/docker-fastapi-uv/#official-docs","title":"Official Docs","text":"<ul> <li> FastAPI docs for Dockerization</li> <li> UV docs for Dockerization</li> <li> UV-FastAPI example docs for Dockerization</li> </ul>"},{"location":"blog/docker-fastapi-uv/#then-why-this","title":"Then, Why this?","text":"<p>I also consider above docs to fulfill my requirements but this includes:</p> <ul> <li>Multistage build to reduce final image size.</li> <li>Best and Flexible practices to use <code>uv</code> in Docker.</li> <li>Solution of some problems which I have encountered.</li> </ul>"},{"location":"blog/docker-fastapi-uv/#dockerfile","title":"<code>Dockerfile</code>","text":"<p>I am taking reference of my <code>yt-comment-sentiment</code> which I have developed and recently and continuously improving it.</p> <pre><code>FROM python:3.11-slim AS builder\n# (1)!\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/\n\n# Install gcc for wordcloud\n# (2)!\nRUN apt-get update &amp;&amp; apt-get install -y gcc &amp;&amp; apt-get clean\n\nWORKDIR /app\n\n# (3)!\nADD pyproject.toml uv.lock /app\n# Install dependencies with `--extra=backend` dependencies\nRUN uv sync --extra=backend --frozen --compile-bytecode --no-install-project\n\n# (4)!\n# Copy only necessary files/folders to reduce image size\nCOPY params.yaml /app\nCOPY backend /app/backend\nCOPY ml /app/ml\n\n# (5)!\nRUN uv sync --extra=backend --locked\n\n# Final stage\n# (6)!\nFROM python:3.11-slim AS final\n# (7)!\nCOPY --from=builder /app /app\n\nWORKDIR /app\n\n# Run backend using fastapi-cli\n# (8)!\nCMD [\".venv/bin/fastapi\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"backend/app.py\"]\n</code></pre> <ol> <li>See official docs by <code>uv</code>.</li> <li>I am using <code>wordcloud</code> in backend and it require <code>gcc</code> package to build wheels to work in python.</li> <li>Add to install dependencies of project. But there is     more in docs.</li> <li>You can also use <code>.dockerignore</code> to do this.</li> <li>Re-sync project, view in docs.</li> <li>Use multistage builds to reduce image size.</li> <li> <p>Here we import only required files from <code>builder</code> stage (which is <code>/app</code> directory) because it contains <code>.venv/</code>     directory (where all project deps were installed) and project files.</p> </li> <li> <p>Finally, use <code>fastapi-cli</code> to run you app.     See FastAPI docs.</p> </li> </ol> <p>If you get any other problem please refer to official docs.</p>"},{"location":"blog/how-to-eda/","title":"How To Perform EDA","text":"Source: https://realpython.com/polars-python/ <p>Performing EDA on a dataset is very difficult and time taking process because there is many thing you can do while performing EDA on your dataset.</p> <p>I will generally use <code>polars</code> library in this article.</p> <p>For plotting I'll generally use <code>seaborn</code> library.</p> <p>First perform univariate analysis without any column dropping, after that perform BiVariate analysis and then decide whether to drop a feature or not. Also, provide a argument which proofs that your dropping decision is right.</p>"},{"location":"blog/how-to-eda/#numerical-features","title":"Numerical Features","text":"<p>There are some basic Descriptive Analysis you can do with numerical column like mean, median, quantiles, etc. You can do this by calling the polars.DataFrame.describe method.</p>"},{"location":"blog/how-to-eda/#univariate","title":"Univariate","text":""},{"location":"blog/how-to-eda/#skewness","title":"Skewness","text":"<p>Skewness is a measure that describes the asymmetry or lack of symmetry in the distribution of a dataset. The value of skewness can provide insights into the shape and nature of the data distribution. Here's how to interpret skewness values:</p> Skewness Value Interpretation Negative Value The distribution is left-skewed or negatively skewed. This means the tail on the left side of the distribution is longer, and the bulk of the values are concentrated on the right side of the distribution. The mean is typically less than the median. Positive Value The distribution is right-skewed or positively skewed. This means the tail on the right side of the distribution is longer, and the bulk of the values are concentrated on the left side of the distribution. The mean is typically greater than the median. Zero The distribution is symmetric, with no skew. The mean, median, and mode are approximately equal, and the distribution is evenly balanced on both sides. <p>The magnitude of the skewness value also provides information about the degree of skewness:</p> <ul> <li>If the skewness value is close to 0 (between -0.5 and 0.5), the distribution is approximately symmetric with little to no skew.</li> <li>If the skewness value is significantly negative (below -1), it suggests strong left skewness.</li> <li>If the skewness value is significantly positive (above 1), it suggests strong right skewness.</li> </ul> <p>In summary, the interpretation of skewness values is as follows:</p> <ul> <li>Negative skewness: The distribution is left-skewed, with the tail extending towards the left.</li> <li>Positive skewness: The distribution is right-skewed, with the tail extending towards the right.</li> <li>Zero skewness: The distribution is symmetric, with no skew.</li> <li>Magnitude of skewness: Indicates the degree of skewness, with values close to 0 suggesting little to no skew, and values below -1 or above 1 suggesting strong skewness.</li> </ul> <p>Use polars.Series.skew method to calculate skewness of a numerical feature.</p>"},{"location":"blog/how-to-eda/#kurtosis","title":"Kurtosis","text":"<p>Kurtosis is a statistical measure that describes the degree of peakedness and tailedness of a probability distribution. It provides information about the shape of the distribution, specifically the proportion of data that is concentrated in the tails compared to the normal distribution.</p> <p>There are three main types of kurtosis:</p> <ol> <li>Mesokurtic: A distribution with a kurtosis value of 3, which is the same as a normal distribution. This indicates a moderate level of peakedness and tailedness.</li> <li>Leptokurtic: A distribution with a kurtosis value greater than 3. This indicates a higher, more peaked distribution with heavier, fatter tails compared to a normal distribution. Leptokurtic distributions have more outliers and extreme values.</li> <li>Platykurtic: A distribution with a kurtosis value less than 3. This indicates a flatter, more dispersed distribution with lighter, thinner tails compared to a normal distribution. Platykurtic distributions have fewer outliers and extreme values.</li> </ol> <p>The excess kurtosis is calculated by subtracting 3 from the kurtosis value. Positive excess kurtosis indicates a leptokurtic distribution, while negative excess kurtosis indicates a platykurtic distribution.</p> <p>In summary, kurtosis provides information about the shape of a distribution, specifically the concentration of values in the tails compared to a normal distribution. Higher kurtosis indicates more outliers and extreme values, while lower kurtosis indicates fewer outliers and a more dispersed distribution.</p> <p>Use polars.Series.kurtosis method to calculate kurtosis value of a numerical feature. This method has <code>fisher</code> argument, if <code>fisher=True</code> then normal is 0.0; if <code>fisher=False</code> then normal is 3.0.</p>"},{"location":"blog/how-to-eda/#distribution","title":"Distribution","text":"<ul> <li><code>kdeplot</code>: Plot distribution of data using Kernel Density Estimation.</li> <li><code>histplot</code>: Plot histogram of data.</li> <li><code>ecdfplot</code>: Plot empirical cumulative distribution functions.</li> <li><code>rugplot</code>: Plot marginal distributions by drawing ticks along the x and y axes.</li> </ul> <p>See <code>seaborn</code> library docs to plot different data distribution.</p>"},{"location":"blog/how-to-eda/#box-plot","title":"Box Plot","text":"<p>Plot the 5-number summary of data with BoxPlot.</p> <p>What is 5-number summary of a data?</p> <p>This represents the \\((\\text{Q1} - 1.5*\\text{IQR})\\), 25th %tile \\((\\text{Q1})\\), 50th %tile, 75th %tile \\((\\text{Q3})\\) and \\((\\text{Q3} + 1.5*\\text{IQR})\\) values of data.</p> <p>BoxPlot easily shows the outliers of the data.</p> <p></p>"},{"location":"blog/how-to-eda/#percentile","title":"Percentile","text":"<p>You can use mean or median to know the central value of data. But what about identifying specific data points relative to the entire dataset's distribution?</p> <p>Percentiles offer a solution by dividing the data into hundredths and determining where a particular value falls within that range. They provide valuable insights into the spread and distribution of data, aiding in comparisons and understanding the dataset's overall characteristics.</p> <p>You can use percentile to know dataset's outlier values. After ploting BoxPlot you can manually check the outlier values with 95%tile or 99%tile.</p> <p>Use polars.Series.quantile method to calculate nth quantile of dataset.</p>"},{"location":"blog/how-to-eda/#bivariate","title":"BiVariate","text":""},{"location":"blog/how-to-eda/#regression-plots","title":"Regression Plots","text":"<ul> <li>Regression Plot: Scatter plot but with a best-fit regression line in there.</li> <li>Residual Plot: Plot the residuals/error of a regression model and check whether it any linear relationship.</li> </ul> <p>See <code>seaborn</code> library docs for regression plots details.</p>"},{"location":"blog/how-to-eda/#relational-plots","title":"Relational Plots","text":"<ul> <li>Scatter Plot: Check the linear relationship between two datasets.</li> <li>Line Plot: Plot the relationship between <code>x</code> and <code>y</code> with many parameters such as <code>hue</code>, <code>size</code> and <code>style</code>.</li> </ul> <p>See <code>seaborn</code> library docs for relational plots details.</p>"},{"location":"blog/how-to-eda/#more-plots","title":"More Plots","text":"<ul> <li>FacetGrid</li> <li>PairPlot</li> <li>JointPlot</li> </ul> <p>Checkout <code>seaborn</code> docs for more details.</p>"},{"location":"blog/how-to-eda/#multivariate","title":"MultiVariate","text":""},{"location":"blog/how-to-eda/#correlation","title":"Correlation","text":""},{"location":"blog/how-to-eda/#multicollinearity","title":"MultiCollinearity","text":""},{"location":"blog/how-to-eda/#extra-concepts","title":"Extra Concepts","text":"<ul> <li>Homoscedasity (concept)</li> </ul>"},{"location":"blog/how-to-eda/#categorical-features","title":"Categorical Features","text":""},{"location":"blog/how-to-eda/#univariate_1","title":"Univariate","text":""},{"location":"blog/how-to-eda/#countplot","title":"CountPlot","text":""},{"location":"blog/how-to-eda/#bivariate_1","title":"BiVariate","text":""},{"location":"blog/how-to-eda/#contengency-table","title":"Contengency Table","text":""},{"location":"blog/how-to-eda/#multivariate_1","title":"MultiVariate","text":""},{"location":"blog/how-to-eda/#correlation_1","title":"Correlation","text":""},{"location":"blog/how-to-eda/#hypothesis-testing","title":"Hypothesis Testing","text":""},{"location":"blog/how-to-eda/#numerical-vs-numerical","title":"Numerical VS Numerical","text":""},{"location":"blog/how-to-eda/#z-test","title":"Z-test","text":""},{"location":"blog/how-to-eda/#t-test","title":"T-test","text":""},{"location":"blog/how-to-eda/#categorical-vs-categorical","title":"Categorical VS Categorical","text":""},{"location":"blog/how-to-eda/#chi-square","title":"Chi-Square","text":""},{"location":"blog/how-to-eda/#numerical-vs-categorical","title":"Numerical VS Categorical","text":""},{"location":"blog/how-to-eda/#oneway-anova","title":"OneWay ANOVA","text":""},{"location":"blog/how-to-eda/#twoway-anova","title":"TwoWay ANOVA","text":""},{"location":"blog/hypothesis-testing-introduction/","title":"Introduction To Hypothesis Testing","text":""},{"location":"blog/hypothesis-testing-introduction/#null-vs-alternative-hypothesis","title":"Null V/S Alternative Hypothesis","text":"Parameter Null Hypothesis Alternative Hypothesis Definition A null hypothesis is a statement in which there is no relation between the two variables. An alternative hypothesis is a statement in which there is some statistical relationship between the two variables. What is it? Generally, researchers try to reject or disprove it. Researchers try to accept or prove it. Testing Process Indirect and Implicit Direct and Explicit p-value Null hypothesis is rejected if the p-value is less than the alpha-value; otherwise, it is accepted. An alternative hypothesis is accepted if the p-value is less than the alpha-value otherwise, it is rejected. Notation \\(H_0\\) \\(H_1\\) Symbol Used Equality Symbol (=, \u2265, \u2264) Inequality Symbol (\u2260, &lt;, &gt;) <p>Abstract</p> <ul> <li>Null hypothesis assumes no effect or relationship, while the alternative hypothesis suggests the presence of an effect or relationship.</li> <li>Researchers try to reject or disprove the null hypothesis, whereas they aim to accept or prove the alternative hypothesis.</li> <li>Null hypothesis testing is indirect and implicit, while alternative hypothesis testing is direct and explicit.</li> <li>Null hypothesis is represented by H0 with an equal sign, while the alternative hypothesis is denoted by H1 with an unequal sign.</li> </ul>"},{"location":"blog/hypothesis-testing-introduction/#illustrate-hypothesis-testing","title":"Illustrate Hypothesis Testing","text":"Scenario <p>A researcher believes that a new teaching method improves student performance in mathematics compared to the traditional method. The average score of students using the new method is expected to be higher than the average score of students using the traditional method.</p> Hypotheses <p>Null Hypothesis \\((H_0)\\): The average score of students using the new teaching method is the same as or lower than the average score of students using the traditional method \\((H_0:\\mu_{\\text{new}} \\le \\mu_{\\text{traditional}})\\).</p> <p>Alternative Hypothesis \\((H_a)\\): The average score of students using the new teaching method is higher than the average score of students using the traditional method \\((H_a:\\mu_{\\text{new}} \\gt \\mu_{\\text{traditional}})\\).</p> <p>Steps</p> <ol> <li>Formulate Hypotheses: Establish the null and alternative hypotheses.</li> <li>Collect Data: Administer both teaching methods to two groups of students and collect their test scores.</li> <li>Calculate Test Statistic: Compute the test statistic, such as the t-test, based on the sample data.</li> <li>Compare to Critical Region: Compare the test statistic to critical values to determine if it falls in the critical region.</li> <li>Calculate p-value: Calculate the p-value associated with the test statistic.</li> <li>Make Conclusion: Based on the p-value and significance level, either reject the null hypothesis if the p-value is less than the significance level or fail to reject it.</li> </ol> <p>In this example, if the p-value is less than the chosen significance level (e.g., 0.05), you would reject the null hypothesis and conclude that there is evidence to support the claim that the new teaching method leads to higher student performance in mathematics. This demonstrates how hypothesis testing allows researchers to draw conclusions based on statistical evidence and make informed decisions.</p> Hypothesis Example (using ChatGPT) <p><p> Does a New Drug Reduce Blood Pressure? </p></p> <p>In this example, we'll demonstrate hypothesis testing using a hypothetical scenario where a pharmaceutical company has developed a new drug intended to reduce blood pressure in patients with hypertension. We'll design and conduct a hypothesis test to determine whether the new drug is effective in reducing blood pressure compared to a placebo.</p> <ol> <li> <p>Hypotheses Formulation</p> <ul> <li>Null Hypothesis (H0): The new drug has no effect on reducing blood pressure, and any observed differences are due to random chance.</li> <li>Alternative Hypothesis (Ha): The new drug effectively reduces blood pressure in patients, leading to a significant difference compared to the placebo.</li> </ul> </li> <li> <p>Data Collection</p> <ul> <li>We collect blood pressure measurements from two groups of participants: one receiving the new drug (treatment group) and the other receiving a placebo (control group).</li> <li>Each participant's blood pressure is measured before and after the treatment period.</li> </ul> </li> <li> <p>Experimental Design</p> <ul> <li>Randomly assign participants to either the treatment group (receiving the new drug) or the control group (receiving a placebo).</li> <li>Ensure blinding to minimize bias, where neither the participants nor the researchers know who is receiving the drug or the placebo.</li> </ul> </li> <li> <p>Data Analysis</p> <ul> <li>Calculate the mean blood pressure reduction for each group.</li> <li>Conduct a hypothesis test, such as a two-sample t-test, to compare the mean blood pressure reductions between the treatment and control groups.</li> </ul> </li> <li> <p>Interpretation of Results</p> <ul> <li>If the p-value is less than the chosen significance level (e.g., \u03b1 = 0.05), we reject the null hypothesis in favor of the alternative hypothesis.</li> <li>A statistically significant result indicates that the new drug has a significant effect on reducing blood pressure compared to the placebo.</li> <li>Conversely, if the p-value is greater than the significance level, we fail to reject the null hypothesis, suggesting no significant difference between the groups.</li> </ul> </li> <li> <p>Conclusion and Recommendations</p> <ul> <li>If the hypothesis test results support the alternative hypothesis, we conclude that the new drug is effective in reducing blood pressure.</li> <li>The pharmaceutical company can proceed with further clinical trials and regulatory approvals for the new drug.</li> <li>If the results do not support the alternative hypothesis, the company may need to reevaluate the drug's efficacy or explore alternative treatment options.</li> </ul> </li> </ol> Example Code (Python)<pre><code>import numpy as np\nfrom scipy.stats import ttest_ind\n\n# Simulated blood pressure data\ntreatment_group = np.array([140, 135, 150, 138, 132])  # Blood pressure reduction after new drug\ncontrol_group = np.array([145, 142, 148, 146, 140])     # Blood pressure reduction after placebo\n\n# Perform two-sample t-test\nt_statistic, p_value = ttest_ind(treatment_group, control_group)\n\n# Interpretation of results\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"The new drug significantly reduces blood pressure compared to the placebo.\")\nelse:\n    print(\"There is no significant difference in blood pressure reduction between the groups.\")\n\n# Additional analyses, visualization, and interpretation can be conducted as needed.\n</code></pre> <p>Explanation</p> <ul> <li>We simulate blood pressure reduction data for participants in the treatment and control groups.</li> <li>We use a two-sample t-test to compare the mean blood pressure reductions between the two groups.</li> <li>If the p-value is less than the significance level (e.g., 0.05), we conclude that the new drug is effective in reducing blood pressure.</li> <li>Otherwise, we fail to reject the null hypothesis, indicating no significant difference between the groups.</li> </ul>"},{"location":"blog/hypothesis-testing-introduction/#errors","title":"Errors","text":"<p>In hypothesis testing, there are two types of errors that can occur when making a decision about the null hypothesis: Type I error and Type II error.</p>"},{"location":"blog/hypothesis-testing-introduction/#type-1-error","title":"Type 1 Error","text":"<p>Type-I (False Positive) error occurs when the sample results, lead to the rejection of the null hypothesis when it is in fact true.</p> <p>In other words, it's the mistake of finding a significant effect or relationship when there is none. The probability of committing a Type I error is denoted by \u03b1 (alpha), which is also known as the significance level. By choosing a significance level, researchers can control the risk of making a Type I error.</p>"},{"location":"blog/hypothesis-testing-introduction/#type-2-error","title":"Type 2 Error","text":"<p>Type-II (False Negative) error occurs when based on the sample results, the null hypothesis is not rejected when it is in fact false.</p> <p>This means that the researcher fails to detect a significant effect or relationship when one actually exists. The probability of committing a Type II error is denoted by \u03b2 (beta). Trade-off between Type 1 and Type 2 errors</p> <p></p>"},{"location":"blog/hypothesis-testing-introduction/#where-can-be-hypothesis-testing-applied","title":"Where can be Hypothesis Testing Applied?","text":"<ol> <li>Testing the effectiveness of interventions or treatments: Hypothesis testing can be used to determine whether a new drug, therapy, or educational intervention has a significant effect compared to a control group or an existing treatment.</li> <li>Comparing means or proportions: Hypothesis testing can be used to compare means or proportions between two or more groups to determine if there's a significant difference. This can be applied to compare average customer satisfaction scores, conversion rates, or employee performance across different groups.</li> <li>Analysing relationships between variables: Hypothesis testing can be used to evaluate the association between variables, such as the correlation between age and income or the relationship between advertising spend and sales.</li> <li>Evaluating the goodness of fit: Hypothesis testing can help assess if a particular theoretical distribution (e.g., normal, binomial, or Poisson) is a good fit for the observed data.</li> <li>Testing the independence of categorical variables: Hypothesis testing can be used to determine if two categorical variables are independent or if there's a significant association between them. For example, it can be used to test if there's a relationship between the type of product and the likelihood of it being returned by a customer.</li> <li>A/B testing: In marketing, product development, and website design, hypothesis testing is often used to compare the performance of two different versions (A and B) to determine which one is more effective in terms of conversion rates, user engagement, or other metrics.</li> </ol>"},{"location":"blog/hypothesis-testing-introduction/#hypothesis-testing-in-ml-applications","title":"Hypothesis Testing in ML Applications","text":"<ol> <li>Feature selection: Hypothesis testing can help identify which features are significantly related to the target variable or contribute meaningfully to the model's performance. For example, you can use a t-test, chi-square test, or ANOVA to test the relationship between individual features and the target variable. Features with significant relationships can be selected for building the model, while non-significant features may be excluded.</li> <li>Hyperparameter tuning: Hypothesis testing can be used to evaluate the performance of a model trained with different hyperparameter settings. By comparing the performance of models with different hyperparameters, you can determine if one set of hyperparameters leads to significantly better performance.</li> <li>Assessing model assumptions: In some cases, machine learning models rely on certain statistical assumptions, such as linearity or normality of residuals in linear regression. Hypothesis testing can help assess whether these assumptions are met, allowing you to determine if the model is appropriate for the data.</li> <li>Model comparison: Hypothesis testing can be used to compare the performance of different machine learning models or algorithms on a given dataset. For example, you can use a paired t-test to compare the accuracy or error rate of two models on multiple cross- validation folds to determine if one model performs significantly better than the other.</li> </ol> <p>Question</p> <ol> <li>What is P-value?</li> <li>What are different types of Hypothesis Tests.</li> <li>Differences between Z-test and T-test?</li> </ol> <p>Resources</p> <ul> <li> P-value Explained by Data Scientist</li> <li> Session 1 on Hypothesis Testing.pdf</li> <li> Session 46 - Hypothesis Testing Part 2 | p-values | t-tests | DSMP 2023</li> </ul>"},{"location":"blog/intro-to-deep-learning/","title":"Intro to Deep Learning","text":""},{"location":"blog/intro-to-deep-learning/#machine-learning-vs-deep-learning","title":"Machine Learning VS Deep Learning","text":"<ol> <li>As the shape of data increases the ML models cannot able to capture its underlying patterns but deep learning    algorithms capture the complex relationship very well.</li> <li>ML algorithms uses different techniques to learn patterns from data like linear line, spliting criteria, etc. but    Perceptron is the building block of DL algorithms which helps to capture almost every patterns of the data.</li> </ol>"},{"location":"blog/intro-to-deep-learning/#perceptron","title":"Perceptron","text":"<p>Perceptron the building block of Deep Learning.</p> <p></p> \\[ \\sum = w_1x_1 + w_2x_2 + ... + w_nx_n + b \\] <p>Now after calculating the \\(\\sum\\) you will use a activation function \\(\\varphi\\) which is applied to the weighted sum to introduce non-linearity.</p> <p>For example, \\(\\varphi\\) can be a step function whose output is either 0 or 1.</p>"},{"location":"blog/intro-to-deep-learning/#important-points-on-perceptron","title":"Important Points on Perceptron","text":"<ul> <li>Perceptron split the data in two classes.</li> <li>Perceptron creates a lines in 2D, plane in 3D and hyperplane in 4D onwards.</li> <li>Perceptron's geometric intuition is very similar to Linear Regression algorithm.</li> <li>Perceptron is limited to classify only linearly (or sort of linear) separable classes.</li> </ul>"},{"location":"blog/intro-to-deep-learning/#limitation","title":"Limitation","text":"<p>Perceptron only works on linear data it doesn't learn non-linear data because perceptron is a linear model which draws a line/plane/hyperplane on datasets.</p>"},{"location":"blog/intro-to-deep-learning/#multi-layer-perceptron-mlp","title":"Multi-Layer Perceptron (MLP)","text":"<p>A Multi-Layer Perceptron (MLP) is a class of feedforward artificial neural networks that consist of multiple layers of nodes in a directed graph. Each node, except for the input nodes, represents a neuron that uses a non-linear activation function. MLPs are capable of learning complex patterns in data, including non-linear relationships, making them widely used in machine learning tasks like classification, regression, and feature extraction.</p> <p>By using a single perceptron, we are limited to learning only linear decision boundaries. This restricts its ability to model more complex datasets with inherent non-linear relationships. To overcome this limitation, we can add more perceptrons in a structured way to create a Multi-Layer Perceptron.</p>"},{"location":"blog/intro-to-deep-learning/#why-dont-we-use-only-one-layer-instead","title":"Why Don\u2019t We Use Only One Layer Instead?","text":"<p>A single-layer perceptron can only model linearly separable data. For example, tasks like distinguishing between two classes in XOR logic cannot be achieved by a single-layer perceptron, as its decision boundary is inherently linear. However, real-world datasets are often non-linear in nature.</p> <p>Adding hidden layers in an MLP allows the network to transform input features through non-linear activation functions, enabling it to create complex decision boundaries. These hidden layers progressively learn higher-order features, making MLP a universal approximator of functions, as proven by the Universal Approximation Theorem.</p>"},{"location":"blog/intro-to-deep-learning/#how-does-mlp-capture-non-linearity","title":"How Does MLP Capture Non-Linearity?","text":"<p>MLPs capture non-linearity through two primary mechanisms:</p> <ol> <li> <p>Non-Linear Activation Functions: Non-linear activation functions like ReLU, Sigmoid, or Tanh introduce the    ability to model complex patterns in the data. Without these functions, the MLP would behave like a linear model,    regardless of the number of layers.</p> </li> <li> <p>Layered Structure: Each hidden layer processes inputs to extract increasingly abstract features. These features,    when passed through activation functions, enable the network to learn representations that are non-linear    transformations of the original data.</p> </li> </ol> <p>By stacking layers, the network learns a hierarchy of features, where lower layers might capture simple patterns (like edges in an image), and deeper layers capture more abstract representations (like shapes or objects).</p>"},{"location":"blog/intro-to-deep-learning/#forward-pass-in-mlp","title":"Forward Pass in MLP","text":"<p>The forward pass is the process of passing input data through the network to compute the output predictions. It involves the following steps:</p> <ol> <li> <p>Input Layer: The input data is fed into the network as feature vectors.</p> </li> <li> <p>Hidden Layers: Each hidden layer applies a weighted sum of inputs followed by a bias term and an activation    function. Mathematically, for a hidden layer ( l ):</p> </li> </ol> Hidden layers in Notation \\[ h^{l} = f(W^{l}h^{(l-1)} + b^{l}) \\] <p>where:</p> <ul> <li>\\(W^{l}\\) is the weight matrix for layer \\(l\\).</li> <li>\\(b^{l}\\) is the bias vector for layer \\(l\\).</li> <li>\\(f\\) is the activation function (e.g., ReLU).</li> <li>\\(h^{(l-1)}\\) is the output from the previous layer.</li> </ul> <ol> <li>Output Layer: The final layer produces predictions, often applying a specific activation function (e.g., softmax    for classification or linear activation for regression).</li> </ol> Example using Keras<pre><code>from keras import Sequential\nfrom keras.layers import Dense\n\n# Define a simple MLP model\nmodel = Sequential([\n    Dense(16, activation='relu', input_shape=(4,)),  # Input layer\n    Dense(8, activation='relu'),                     # Hidden layer\n    Dense(1, activation='sigmoid')                  # Output layer\n])\n\nmodel.summary()\n</code></pre> <p>Info</p> <p>The forward pass results in the computation of predictions based on the current weights and biases.</p>"},{"location":"blog/intro-to-deep-learning/#backward-propagation-in-mlp","title":"Backward Propagation in MLP","text":"<p>The backward propagation algorithm is used to train the MLP by adjusting weights and biases to minimize the error between predicted and actual outputs. It works in the following steps:</p> <ol> <li> <p>Compute Loss: A loss function (e.g., Mean Squared Error for regression or Cross-Entropy Loss for classification)    measures the error between the predicted output and the true labels.</p> </li> <li> <p>Backpropagation of Errors: The error is propagated backward through the network using the chain rule of calculus    to compute the gradient of the loss function with respect to each weight and bias. The gradients for each parameter    are computed layer by layer in reverse order.</p> </li> <li> <p>Update Weights and Biases: Using the computed gradients, the weights and biases are updated using an optimization    algorithm like Gradient Descent or its variants (e.g., Adam, RMSprop):</p> </li> </ol> Backward Propagation with Notation \\[ W^{l} = W^{l} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{l}} \\] \\[ b^{l} = b^{l} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b^{l}} \\] <p>where:</p> <ul> <li>\\(\\eta\\) is the learning rate.</li> <li>\\(\\mathcal{L}\\) is the loss function.</li> </ul> Example using PyTorch<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define a simple MLP model\nclass SimpleMLP(nn.Module):\n    def __init__(self):\n        super(SimpleMLP, self).__init__()\n        self.fc1 = nn.Linear(4, 16)  # Input to hidden layer\n        self.fc2 = nn.Linear(16, 8) # Hidden to hidden layer\n        self.fc3 = nn.Linear(8, 1)  # Hidden to output layer\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        return x\n\n# Instantiate the model, loss function, and optimizer\nmodel = SimpleMLP()\ncriterion = nn.BCELoss()  # Binary Cross Entropy Loss\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Dummy data for training\ninputs = torch.randn(10, 4)  # Batch of 10 samples, 4 features each\ntargets = torch.randint(0, 2, (10, 1)).float()  # Binary targets\n\n# Training loop\nfor epoch in range(10):\n    # Forward pass\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n</code></pre> <p>Info</p> <p>By iteratively performing forward and backward passes over multiple epochs, the network learns the optimal parameters to minimize the loss and generalize to unseen data.</p>"},{"location":"blog/intro-to-deep-learning/#importance-of-mlp","title":"Importance of MLP","text":"<p>By combining these techniques, MLPs become powerful tools for modeling both linear and non-linear patterns in data. They form the foundation of many advanced deep learning architectures, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).</p>"},{"location":"blog/learn-docker/","title":"Learn Docker","text":"<p>Docker is a platform designed to simplify the process of creating, deploying, and managing applications using containers. Containers enable developers to package an application with all its dependencies into a standardized unit for seamless deployment across different environments.</p>"},{"location":"blog/learn-docker/#components-of-docker","title":"Components of Docker","text":"<ol> <li>Dockerfile</li> <li>.dockerignore</li> <li>docker-compose.yaml</li> </ol>"},{"location":"blog/learn-docker/#dockerfile","title":"<code>Dockerfile</code>","text":"<p>A <code>Dockerfile</code> serves as a blueprint for building Docker images, which are the executable packages containing everything needed to run an application - code, runtime, system tools, libraries, and settings. Let's break down the components of a <code>Dockerfile</code> and its significance in the context of Docker:</p>"},{"location":"blog/learn-docker/#context-on-dockerfile","title":"Context on Dockerfile","text":"<ol> <li> <p>Foundation of Image Creation: A <code>Dockerfile</code> specifies a sequence of instructions to assemble an image. It starts with a base image (e.g., Ubuntu, Alpine Linux, Python) and then layers additional configurations and dependencies on top of it.</p> </li> <li> <p>Clear and Reproducible Build Process: Each instruction in the <code>Dockerfile</code> represents a step in the image-building process. These steps are executed in order, and Docker caches intermediate layers, facilitating faster subsequent builds and ensuring consistency across environments.</p> </li> <li> <p>Key Components of a Dockerfile:</p> </li> <li> <p>Base Image: Specifies the starting point for the image.</p> </li> <li>Environment Setup: Includes commands to install packages, set environment variables, copy files into the image, etc.</li> <li>Application Configuration: Defines how the application should be configured inside the container.</li> <li>Startup Commands: Specifies the command to execute when the container starts.</li> </ol>"},{"location":"blog/learn-docker/#comprehensive-description-of-a-dockerfile","title":"Comprehensive Description of a Dockerfile","text":"<p>A typical <code>Dockerfile</code> consists of several sections:</p> <ol> <li> <p>FROM: Defines the base image. It's the starting point for the image build and often references an official or custom base image from a registry (e.g., <code>FROM python:3.11-slim</code>).</p> </li> <li> <p>WORKDIR: Sets the working directory inside the container where subsequent commands will be executed.</p> </li> <li> <p>COPY/ADD: Copies files or directories from the host machine into the container's filesystem. This includes application code, configuration files, etc.</p> </li> <li> <p>RUN: Executes commands during the image build process. Typically used for installing dependencies, setting up the environment, and other preparatory tasks.</p> </li> <li> <p>ENV: Sets environment variables within the container. These can define runtime configurations or paths.</p> </li> <li> <p>EXPOSE: Informs Docker that the container listens on specific network ports at runtime.</p> </li> <li> <p>CMD/ENTRYPOINT: Specifies the command that should be run when the container starts. <code>CMD</code> is used to provide default arguments for the <code>ENTRYPOINT</code> command, while <code>ENTRYPOINT</code> sets the primary command.</p> </li> </ol>"},{"location":"blog/learn-docker/#an-example-dockerfile-of-a-python-project","title":"An Example Dockerfile of a Python Project","text":"Dockerfile<pre><code># Use an official Python runtime as a parent image\nFROM python:3.10\n\n# Set the working directory to /app\nWORKDIR /app\n\n# Copy the required files and directory into the container at /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\n# Run main.py when the container launches\nCMD [\"python\", \"main.py\"]\n</code></pre> <pre><code># Build docker container\ndocker build -t my_python_container .\n\n# Run docker image\ndocker run -it my_python_container\n</code></pre>"},{"location":"blog/learn-docker/#importance-and-benefits","title":"Importance and Benefits:","text":"<ul> <li> <p>Portability: Dockerfiles enable developers to create consistent environments, ensuring that applications run identically across various systems and environments.</p> </li> <li> <p>Reproducibility: By capturing all dependencies and configurations in the Dockerfile, developers can replicate the same environment for development, testing, and production.</p> </li> <li> <p>Scalability and Efficiency: Docker's containerization allows for quick scaling and resource efficiency, enabling applications to be deployed and managed easily.</p> </li> </ul> <p>Summary</p> <p>A <code>Dockerfile</code> is the backbone of Docker-based applications, providing a clear, reproducible, and scalable approach to building containerized applications. It defines the entire setup and configuration needed to run an application within a container, promoting consistency and ease of deployment across different environments.</p> Resources to Learn Docker <p>PiyushGarg YouTube</p> <ul> <li> Docker - Hindi YouTube Playlist | PiyushGarg</li> <li> Docker In One Shot - Part 1</li> <li> Docker For Open Source Contributors - Part 2</li> <li> Deploying Docker Containers on AWS Elastic Container Service (ECS) | Container Orchestration</li> </ul> <p>Visual Studio Code - Docs</p> <ul> <li> Working with containers in Visual Studio Code</li> <li> Build and run a Python app in a container | VSCode</li> </ul>"},{"location":"blog/learn-docker/#docker","title":"<code>docker</code>","text":""},{"location":"blog/learn-docker/#1-docker-run","title":"1. <code>docker run</code>","text":"<ul> <li>Description: Starts a new container from an image.</li> <li>Example: <pre><code>docker run -it --name my_container ubuntu:latest\n</code></pre></li> <li><code>-it</code>: Starts an interactive terminal within the container.</li> <li><code>--name my_container</code>: Names the container as \"my_container\".</li> <li><code>ubuntu:latest</code>: Specifies the image to use (latest Ubuntu image).</li> </ul>"},{"location":"blog/learn-docker/#2-docker-ps","title":"2. <code>docker ps</code>","text":"<ul> <li>Description: Lists running containers.</li> <li>Example: <pre><code>docker ps\n</code></pre>   This command shows the containers' IDs, names, status, ports, and images.</li> </ul>"},{"location":"blog/learn-docker/#3-docker-images","title":"3. <code>docker images</code>","text":"<ul> <li>Description: Lists available images.</li> <li>Example: <pre><code>docker images\n</code></pre>   Displays a list of all downloaded Docker images along with their tags and sizes.</li> </ul>"},{"location":"blog/learn-docker/#4-docker-build","title":"4. <code>docker build</code>","text":"<ul> <li>Description: Builds an image from a Dockerfile.</li> <li>Example: <pre><code>docker build -t my_image:latest .\n</code></pre></li> <li><code>-t my_image:latest</code>: Tags the image as \"my_image\" with the \"latest\" tag.</li> <li><code>.</code>: Specifies the build context (current directory) containing the Dockerfile.</li> </ul>"},{"location":"blog/learn-docker/#5-docker-stop","title":"5. <code>docker stop</code>","text":"<ul> <li>Description: Stops a running container.</li> <li>Example: <pre><code>docker stop my_container\n</code></pre>   Stops the container named \"my_container\".</li> </ul>"},{"location":"blog/learn-docker/#6-docker-start","title":"6. <code>docker start</code>","text":"<ul> <li>Description: Starts a stopped container.</li> <li>Example: <pre><code>docker start my_container\n</code></pre>   Starts the container named \"my_container\" that was stopped.</li> </ul>"},{"location":"blog/learn-docker/#7-docker-rm","title":"7. <code>docker rm</code>","text":"<ul> <li>Description: Removes one or more containers.</li> <li>Example: <pre><code>docker rm my_container\n</code></pre>   Deletes the container named \"my_container\".</li> </ul>"},{"location":"blog/learn-docker/#8-docker-rmi","title":"8. <code>docker rmi</code>","text":"<ul> <li>Description: Removes one or more images.</li> <li>Example: <pre><code>docker rmi my_image:latest\n</code></pre>   Removes the image \"my_image\" with the \"latest\" tag.</li> </ul>"},{"location":"blog/learn-docker/#9-docker-exec","title":"9. <code>docker exec</code>","text":"<ul> <li>Description: Executes a command within a running container.</li> <li>Example: <pre><code>docker exec -it my_container bash\n</code></pre>   Executes the Bash shell (<code>bash</code>) in the running container named \"my_container\" interactively (<code>-it</code>).</li> </ul>"},{"location":"blog/learn-docker/#10-docker-logs","title":"10. <code>docker logs</code>","text":"<ul> <li>Description: Retrieves logs from a container.</li> <li>Example: <pre><code>docker logs my_container\n</code></pre>   Fetches the logs of the container named \"my_container\".</li> </ul> <p>These commands form the core of Docker usage for managing containers, images, building, starting/stopping containers, and interacting with containerized applications. They're essential for everyday Docker workflows in development, testing, and deployment scenarios.</p>"},{"location":"blog/learn-docker/#dockerignore","title":"<code>.dockerignore</code>","text":"<p><code>.dockerignore</code> is a file used to specify which files and directories to exclude when building a Docker image. It works similarly to <code>.gitignore</code> but for Docker. When building an image, Docker uses this file to determine which files should not be included in the context sent to the Docker daemon, thus reducing the image size and build time.</p>"},{"location":"blog/learn-docker/#example","title":"Example","text":"<p>An <code>.dockerignore</code> file might contain entries like <code>node_modules</code>, <code>*.log</code>, or any other files/directories that are not necessary for the image build process.</p> .dockerignore<pre><code># Byte-compiled files\n__pycache__\n*.pyc\n*.pyo\n*.pyd\n\n# Virtual environments\nvenv/\nenv/\n.venv/\n\n# Editor/IDE specific files\n.vscode/\n.idea/\n*.sublime-project\n*.sublime-workspace\n\n# Logs and temp files\n*.log\nlogs/\n*.tmp\n\n# Miscellaneous\n.DS_Store\nnode_modules/\n.cache/\n\n# My custom files for practicing\n*.arv\narv.*\n</code></pre>"},{"location":"blog/learn-docker/#differences-between-gitignore-and-dockerignore","title":"Differences between <code>.gitignore</code> and <code>.dockerignore</code>","text":"Aspect <code>.gitignore</code> <code>.dockerignore</code> Purpose Specifies files to ignore in Git Specifies files to exclude in Docker image builds Associated tool Git Docker File behavior Excludes files in Git operations Excludes files during image build Impact Affects version control only Affects image build process Use case Manages repository content Manages files in Docker context Ignoring patterns Glob patterns, file names Glob patterns, file names <p><code>.gitignore</code> and <code>.dockerignore</code> serve different purposes despite their similar naming conventions. While both control what files should be ignored/excluded, <code>.gitignore</code> operates within version control systems, allowing certain files not to be tracked. Conversely, <code>.dockerignore</code> is used during image building to exclude unnecessary files from the Docker context sent to the daemon, optimizing the image build process.</p> <p>Both files use similar syntax (like glob patterns) to specify what should be ignored, but their impact and contexts in which they're utilized differ significantly.</p>"},{"location":"blog/learn-docker/#doubts","title":"Doubts","text":"<ol> <li>How to write <code>Dockerfile</code> efficiently?</li> <li>How to use <code>.dockerignore</code>?</li> <li>How to run two apps with one <code>Dockerfile</code>?</li> <li>How to integrate Environment Variables in Docker and Python project?</li> <li>Learn about Python images present on Docker like <code>slim</code>, <code>alpine</code>, <code>bookworm</code>, etc.</li> <li>How to run multiple apps like FastAPI and Streamlit in one go?</li> <li>How do I integrate MongoDB image in my Python project?</li> </ol>"},{"location":"blog/learn-fastapi/","title":"Learn FastAPI","text":"<p>A better framework than Flask. Get production-ready code and API. With automatic interactive documentation. Based on (and fully compatible with) the open standards for APIs:\u00a0OpenAPI\u00a0(previously known as Swagger) and\u00a0JSON Schema.</p>"},{"location":"blog/learn-fastapi/#features","title":"Features","text":"<ol> <li>Automatic docs: Generate documentation for your API automatically.<ul> <li>Swagger UI: Interactive exploration, call and test your API directly from the browser. <pre><code>http://localhost:8000/docs\n</code></pre></li> <li>Redoc: Read only documentation. You can also download this doc. <pre><code>http://localhost:8000/redoc\n</code></pre></li> </ul> </li> <li>Response Validation: Use pydantic <code>BaseModel</code> as TypeHint in Python which automatically validate your responses.</li> <li>Starlette Features: <code>FastAPI</code>\u00a0is actually a sub-class of\u00a0<code>Starlette</code>.With\u00a0FastAPI\u00a0you get all of\u00a0Starlette's features (as FastAPI is just Starlette on steroids):<ul> <li>Seriously impressive performance. It is\u00a0one of the fastest Python frameworks available, on par with\u00a0NodeJS\u00a0and\u00a0Go.</li> <li>WebSocket\u00a0support.</li> <li>In-process background tasks.</li> <li>Startup and shutdown events.</li> <li>Test client built on HTTPX.</li> <li>CORS, GZip, Static Files, Streaming responses.</li> <li>Session and Cookie\u00a0support.</li> <li>100% test coverage.</li> <li>100% type annotated codebase.</li> </ul> </li> <li>Supports Asynchronous programming.</li> </ol> Important Links To Learn FastAPI <p>FastAPI Tutorials</p> <ul> <li> Amigoscode</li> <li> ArjanCodes</li> <li> Bitfumes</li> <li> CodeWithHarry</li> <li> CodeWithHarry - Tutorial Uses Some of the Classes</li> </ul> <p>Asynchronous Programming Tutorials</p> <ul> <li> ArjanCode - 1</li> <li> ArjanCode - 2</li> <li> NeuralNine</li> <li> NeuralNine - Requests Library</li> </ul>"},{"location":"blog/learn-fastapi/#some-advice-on-fastapi","title":"Some Advice On FastAPI","text":"<ul> <li>Use <code>fastapi.APIRouter</code> to separate out different API paths. See mine @arv-anshul/ecommerce-scrapper-api project for example.</li> <li>If you don't know, check the\u00a0\"In a hurry?\"\u00a0section about\u00a0<code>async</code>\u00a0and\u00a0<code>await</code>\u00a0in the docs.</li> <li>Learn builtin <code>asyncio</code> module in python to do Asynchronous Programming in python. See mine @arv-anshul/yt-watch-history project for example.</li> <li>Use  pydantic with FastAPI for data handling of APIs. See this docs section to know more about  Pydantic and  FastAPI compatiblity.</li> <li>You can use the <code>fastapi.testclient.TestClient</code> class to test FastAPI applications without creating an actual HTTP and socket connection, just communicating directly with the FastAPI code. Read more about it in the FastAPI docs for Testing - Tutorial.</li> <li>There are many other advance concepts in API world and some of them are Middleware, Dependency Injection, CORS, etc. For that see the FastAPI docs.</li> </ul> Extra Links around FastAPI <ul> <li> What are the best practices for structuring a FastAPI project? - Stack Overflow</li> <li> Advanced User Guide - FastAPI</li> <li> Custom Response - HTML, Stream, File, others - FastAPI</li> </ul>"},{"location":"blog/learn-stuff/","title":"Learn Stuff","text":""},{"location":"blog/learn-stuff/#learn-stuff-from-tutorials","title":"Learn Stuff from Tutorials","text":"<p>Shared some resources which I have used to learn these technologies. Enjoy!!</p> <p>Need to refactor this page and make it more verbose.</p>"},{"location":"blog/learn-stuff/#async-programming","title":"Async Programming","text":"<ul> <li> <p> Making multiple HTTP requests using Python (synchronous, multiprocessing, multithreading, asyncio)</p> </li> <li> <p> Demystifying Python's Async and Await Keywords</p> </li> <li> <p> Asyncio in Python | Tutorial</p> </li> </ul>"},{"location":"blog/learn-stuff/#bash","title":"Bash","text":"<ul> <li> <p> 50 Linux Commands Every Developer Must Know \ud83d\udd25</p> </li> <li> <p> you need to learn BASH Scripting RIGHT NOW!!</p> </li> </ul>"},{"location":"blog/learn-stuff/#css","title":"CSS","text":"<ul> <li> <p> Tailwind CSS Tutorial For Beginners in Hindi</p> </li> <li> <p> Learn Tailwind CSS</p> </li> </ul>"},{"location":"blog/learn-stuff/#docker","title":"Docker","text":"<ul> <li> Learn Docker</li> </ul>"},{"location":"blog/learn-stuff/#fastapi","title":"FastAPI","text":"<ul> <li> <p> Learn FastAPI Blog</p> </li> <li> <p> FastAPI - A python framework</p> </li> <li> <p> FastAPI Tutorial - Building RESTful APIs with Python</p> </li> <li> <p> FastAPI Tutorial in Hindi | Lets build a Notes app using FastAPI, MongoDB &amp; Bootstrap</p> </li> <li> <p> FastAPI Tutorials</p> </li> </ul>"},{"location":"blog/learn-stuff/#git","title":"Git","text":"<ul> <li> <p> Complete Git and GitHub Tutorial</p> </li> <li> <p> Complete GIT in 1 video</p> </li> <li> <p> Git CLI Cheatsheet \u00b7 GitHub Gist</p> </li> <li> <p> Atlassian Documentation on GIT</p> </li> </ul>"},{"location":"blog/learn-stuff/#html","title":"HTML","text":"<ul> <li> Front-End Development</li> </ul>"},{"location":"blog/learn-stuff/#langchain","title":"Langchain","text":"<ul> <li> LangChain Master Class For Beginners 2024 [+20 Examples, LangChain V0.2]</li> </ul>"},{"location":"blog/learn-stuff/#mongodb","title":"MongoDB","text":"<ul> <li> <p> MongoDB Tutorial in 1 Hour</p> </li> <li> <p> MongoDB aggregation Pipelines</p> </li> </ul>"},{"location":"blog/learn-stuff/#playwright","title":"Playwright","text":"<ul> <li> <p> Playwright Python</p> </li> <li> <p> Playwright &amp; Python - Beginner</p> </li> <li> <p> More Web Scraping Stuff</p> </li> <li> <p> Web Scraping with Playwright and Python</p> </li> </ul>"},{"location":"blog/learn-stuff/#polars","title":"Polars","text":"<ul> <li> <p> Why I chose Python &amp; Polars for Data Analysis</p> </li> <li> <p> Polars for Begginers</p> </li> <li> <p> One Shot Overview of Polars</p> </li> <li> <p> Getting Started with Polars</p> </li> </ul>"},{"location":"blog/learn-stuff/#pydantic","title":"Pydantic","text":"<ul> <li> <p> Pydantic Introduction - Models, Fields, Constrained Types, Validator Functions and Model Exports</p> </li> <li> <p> Do We Still Need Dataclasses? Pydantic Tutorial</p> </li> <li> <p> Why You Should Use Pydantic in 2024 | Tutorial</p> </li> </ul>"},{"location":"blog/learn-stuff/#pytest","title":"Pytest","text":"<ul> <li> <p> Getting started with Pytest</p> </li> <li> <p> Unit Testing in Python with pytest</p> </li> <li> <p> Pytest - FreeCodeCamp</p> </li> </ul>"},{"location":"blog/learn-stuff/#regex","title":"Regex","text":"<ul> <li> RegEx in Python</li> </ul>"},{"location":"blog/learn-stuff/#rust","title":"Rust","text":"<ul> <li> <p> The Rust Book</p> </li> <li> <p> Rust Programming Tutorial \ud83e\udd80</p> </li> <li> <p> Rust Talks</p> </li> <li> <p> Learn Rust Programming</p> </li> <li> <p> The Rust Lang Book</p> </li> </ul>"},{"location":"blog/learn-stuff/#sql","title":"SQL","text":"<ul> <li> <p> Basic DBMS and SQL Full</p> </li> <li> <p> SQL for Data Science</p> </li> <li> <p> Complete SQL Course For Data Science</p> </li> </ul>"},{"location":"blog/learn-stuff/#taskfile","title":"Taskfile","text":"<ul> <li> <p> Taskfile</p> </li> <li> <p> Overview to Taskfile</p> </li> </ul>"},{"location":"blog/learn-stuff/#web-scrapping","title":"Web Scrapping","text":"<ul> <li> <p> Learn Web Scraping</p> </li> <li> <p> Discovering Hidden APIs</p> </li> <li> <p> Web Scraping with Selenium and Python</p> </li> </ul>"},{"location":"blog/learn-web-scraping/","title":"Learn Web Scraping","text":"Source: https://realpython.com/python-web-scraping-practical-introduction <p>Web scraping is a very essential tool for programmers to learn to gather data from websites. Specifically, for Data Scientists web scraping is goto tool to gather data from websites. We can use <code>bs4.BeautifulSoup</code> or <code>selenium</code> in Python to scrape any website.</p> <p>You can see some of my projects where I scraped websites like 99acres.com, flipkart.com, housing.com and gather useful data for my Data Science projects like arv-anshul/campusx-real-estate.</p> <p> I have learned Web Scraping from YouTube only. </p>"},{"location":"blog/learn-web-scraping/#youtube-videos","title":"YouTube Videos","text":"<ul> <li> Corey Schafer<ul> <li>Working with JSON Data using the json Module</li> <li>Request Web Pages, Download Images, POST Data, Read JSON, and More</li> <li>Web Scraping with BeautifulSoup and Requests</li> <li>Web Scraping with Requests-HTML</li> </ul> </li> </ul>"},{"location":"blog/learn-web-scraping/#youtube-playlists","title":"YouTube Playlists","text":"<ul> <li> <p> John Watson Rooney</p> <ul> <li>Modern Web Scraping with Python</li> <li>Best Web Scraping Methods</li> </ul> </li> <li> <p> Indian Pythonista</p> <ul> <li>Discovering Hidden APIs</li> <li>Python for web</li> </ul> </li> </ul> <p>If you follow/learn these resources then you will understand how do Web Scraping works and how to do it.</p>"},{"location":"blog/learn-web-scraping/#my-python-package-for-web-scraping","title":"My Python Package For Web Scraping","text":"<p>I have done lots of project on Web Scraping but while doing those web scraping projects I doesn't found a good python package to handle/parse cURL command. But I found a package called @spulec/uncurl on GitHub but it is managed badly so that I cloned that project and refactor it well and republished as <code>curler</code> on PyPI.</p> <p> <code>curler</code> <code>curler</code> </p> <p> My Projects Related to Web Scraping <code>campusx-dsmp</code> <code>99acres-scrape</code> <code>ecommerce-scrapper-api</code> <code>pw-api</code> </p>"},{"location":"blog/ml-systems-dobuts/","title":"ML Systems Dobuts","text":"<p>I've been working on a project of Machine Learning where I am using  Docker to containerise my applications (frontend and backend). But I'm facing difficulties while using ML models in the containers.</p> <p>Question</p> <ol> <li>How to train the model and also use  MLFlow for model monitoring?</li> <li>I don't know how to integrate the ML models in the containers.<ol> <li>Should I deploy my models in cloud and from there I can fetch the models for prediction?</li> <li>Should I add the models into the container from which I can easily make prediction?</li> </ol> </li> </ol>"},{"location":"blog/ml-systems-dobuts/#mlflow","title":"MLFlow","text":"<p>In my project  <code>yt-watch-history</code>, I am using MLFlow (but not using it also) means I have written code to train the model with MLFlow but I can also train without it (and I always use this only).</p> <p>Question</p> <ol> <li>Should I train the model using MLFLow or just do the model monitoring while Hyper-parameter Tuning?</li> </ol>"},{"location":"blog/ml-systems-dobuts/#containers","title":"Containers","text":""},{"location":"blog/ml-systems-dobuts/#what-i-am-doing-right-now","title":"What I am doing right now?","text":"<p>I am training the models before starting the container and after, training the models and starting I do predictions using those models by mounting the directories (where models are stored).</p>"},{"location":"blog/ml-systems-dobuts/#what-should-i-have-to-do","title":"What should I have to do?","text":"<ol> <li>I can train the model locally and deploy it on the cloud and then fetch and store* the model while prediction.</li> <li>I can containerise the model too with the codes which makes it easy to use and locate. But comes with many disadvantages like scalability, container's size, model availability/redundancy and more.</li> </ol>"},{"location":"blog/ml-systems-dobuts/#databases","title":"Databases","text":"<p>I have been using this as optional step because I never use database model training purpose. I prefer to fetch the data from database and store them into local files and then use them for all the steps.</p> <p>How should I use them for model training?</p> <ol> <li>Should I always fetch data from databases for training?</li> <li>Should I fetch the data once and do all the required steps like data preprocessing, data transformation, features selection, model training and all? </li> </ol>"},{"location":"blog/outlier-univariate/","title":"Handle Outliers - Univariate","text":"<p>Handling outlier is a big task for data scientist. To handle the outliers we have many different methods to handle them i.e. IQR, Z-score, Mean-Median Imputation, Winsorization, etc. We are going to discuss only univariate methods to handle outliers.</p> <p> I have written this page as notes very time ago; so if there is any mistake please let me know I'll fix it. Thanks \ud83e\udd17</p>"},{"location":"blog/outlier-univariate/#deletion-based-approach","title":"Deletion Based Approach","text":""},{"location":"blog/outlier-univariate/#iqr","title":"IQR","text":"<p>In this method by using Inter Quartile Range(IQR), we detect outliers. IQR tells us the variation in the data set. Any value, which is beyond the range of \\(-1.5 \\ast IQR\\) to \\(1.5 \\ast IQR\\) treated as outliers.</p> <p>The concept of quartiles and IQR can best be visualized from the boxplot. It has the minimum and maximum point defined as \\(Q1 - 1.5 \\ast IQR\\) and \\(Q3 + 1.5 \\ast IQR\\) respectively. Any point outside this range is outlier.</p> <p>Cons</p> <p>It delete your many data point because even if there is only one data point in a row is act as outlier for their respective column then the row is being removed which means to remove one outlier you removed many essential data point from your dataset.</p> <pre><code>import pandas as pd\n\ndef apply_iqr_deletion(df: pd.DataFrame, columns: list[str], *tiles: tuple[float, float]):\n    \"\"\"\n    Deletes outliers from given columns which are out of range of minimum and maximum percentile values.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        DataFrame to be cleaned.\n    columns: list[str]\n        List of columns to be cleaned.\n    *tiles: tuple[float, float]\n        Tuples of (minimum percentile, maximum percentile) for each column.\n\n    Returns\n    -------\n    DataFrame with deleted outliers data points.\n\n    Raises\n    ------\n    ValueError: If `len(columns) != len(tiles)`.\n    \"\"\"\n    if len(columns) != len(tiles):\n        raise ValueError('len(columns) != len(tiles)')\n\n    for col, tile in zip(columns, tiles):\n        mini, maxi = df[col].quantile[tile]\n        df = df[(df[col]&gt;mini) &amp; (df[col]&lt;maxi)]\n    return df\n</code></pre> <p>Summary</p> <p>If you have less number of outliers in your data then apply <code>apply_iqr_deletion</code> function but if you have many outliers than a threshold value then use <code>apply_iqr_capping</code> function to cap the outliers within a range.</p>"},{"location":"blog/outlier-univariate/#z-score","title":"Z-Score","text":"<p>This method assumes that the variable has a Gaussian distribution. It represents the number of standard deviations an observation is away from the mean.</p> <p>In this method we calculate the z-score with \\(Z = \\frac{(x_i - \\bar{x})}{\\sigma}\\) of the feature then set a threshold (generally as \u00b13) then remove the data point which are \\(\\ge 3\\) and \\(\\le -3\\).</p> <p>Tip</p> <p>You can also calculate absolute value of every z-score then just one constraint is required as \\(\\ge 3\\).</p> <p>Cons</p> <ul> <li>It deletes the rows which contains outlier which leads to data loss. And generally, losing the data is not good because it creates bias in the model and you doesn't inference well.</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndef apply_zscore_deletion(df: pd.DataFrame, columns: List[str], threshold: float = 3.0) -&gt; pd.DataFrame:\n    \"\"\"\n    Deletes outliers from given columns using z-score method.\n\n    Args:\n        df: pd.DataFrame\n            DataFrame to be cleaned.\n        columns: List[str]\n            List of columns to be cleaned.\n        threshold: float\n            Threshold for z-score.\n\n    Returns:\n        pd.DataFrame\n            DataFrame with deleted outliers data points.\n    \"\"\"\n\n    if not isinstance(threshold, (float, int)):\n        raise TypeError(\"Threshold must be a float or integer value.\")\n\n    z_scores = np.abs(stats.zscore(df[columns]))\n    df = df[columns][z_scores &lt;= threshold]\n    return df\n</code></pre> <p>Summary</p> <ul> <li>It uses mean and standard deviation of the population data which is generally not available so we need to apply hypothesis testing to ensure that sample mean and sample standard deviation is being used instead of population parameters.</li> </ul> <p>Doubt</p> <ul> <li>Why do we calculate Z-Score because it requires population standard deviation which is not available for every for every datasets.</li> <li>We should use T-Score instead.</li> </ul>"},{"location":"blog/outlier-univariate/#capping-based-approach","title":"Capping Based Approach","text":""},{"location":"blog/outlier-univariate/#winsorization","title":"Winsorization","text":"<p>It\u00a0is a way to minimise the influence of\u00a0outliers\u00a0in your data by either:</p> <ul> <li>Assigning the outlier a lower weight.</li> <li>Changing the value so that it is close to other values in the set.</li> </ul> <p>Pros</p> <ul> <li>It doesn't delete the rows where outliers lie instead it clip those outliers with your defined percentile values for each column.</li> </ul> <p>Cons</p> <ul> <li>If there is many outlier values in the column/feature then after clipping the distribution of column/feature will change.</li> </ul> <pre><code>import pandas as pd\n\ndef apply_winsorization(df: pd.DataFrame, columns: list[str], *tiles: tuple[float, float]):\n    \"\"\"\n    Caps outliers in given columns to the minimum and maximum percentile values.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        DataFrame to be cleaned.\n    columns: list[str]\n        List of columns to be cleaned.\n    *tiles: tuple[float, float]\n        Tuples of (minimum percentile, maximum percentile) for each column.\n\n    Returns\n    -------\n    DataFrame with capped outliers data points.\n\n    Raises\n    ------\n    ValueError: If `len(columns) != len(tiles)`.\n    \"\"\"\n    if len(columns) != len(tiles):\n        raise ValueError('len(columns) != len(tiles)')\n\n    for col, tile in zip(columns, tiles):\n        mini, maxi = df[col].quantile[tile]\n        df[col] = df[col].clip(mini, maxi)\n    return df\n</code></pre> <p>Summary</p> <p>Use this method because it uses capping technique to handle outliers.</p> Important Links <ul> <li> Detecting and Treating Outliers | How to Handle Outliers</li> <li> Detect and Remove the Outliers in a Dataset | by Dilip Valeti | Medium</li> </ul>"},{"location":"blog/regression-interview-questions/","title":"Regression Interview Questions","text":"5 Assumptions of Linear Regression"},{"location":"blog/regression-interview-questions/#assumptions-of-linear-regression","title":"Assumptions of Linear Regression?","text":"<p>There are about 5 main assumption while training a Linear Regression Model which are:</p>"},{"location":"blog/regression-interview-questions/#1-linear-relationship-between-input-and-output-data","title":"1. Linear Relationship Between Input And Output Data","text":"<p>Relationship of every input feature must be linear with output feature.</p>"},{"location":"blog/regression-interview-questions/#2-multi-collinearity","title":"2. Multi-Collinearity","text":"<p>What is Multi-Collinearity?</p> <p>Multicollinearity is a phenomena where two or more independent variables are highly correlated. In other words, one predictor variable can be used to predict the value of another. This creates redundant information, skewing the results in a regression model.</p> <p>Input data must not correalate with each other, they must be independent of each other. One can use VIF or correlation matrices to know whether their input data is correlated.</p> <p> See this section for better explanation.</p>"},{"location":"blog/regression-interview-questions/#3-normally-distributed-residuals","title":"3. Normally Distributed Residuals","text":"<p>What are Residuals?</p> <p>Represent the vertical distance between a data point and the regression line. They are the errors of the model which the model can't able to capture while training.</p> <p>The distribution of the residuals must be normally distributed. One can analyse this using KDE or QQ-Plot.</p>"},{"location":"blog/regression-interview-questions/#4-homoscedacity","title":"4. Homoscedacity","text":"<p>Homoscedasticity refers to constant variance in a regression model's residuals. Cons include potential bias and inefficiency. Visualize homoscedasticity using scatter plots \u2014 residuals vs. predicted values should show an even spread, indicating consistent variance. In Python, seaborn or matplotlib can create such plots for regression diagnostics. </p>"},{"location":"blog/regression-interview-questions/#5-no-auto-correlation-of-error","title":"5. No Auto-Correlation Of Error","text":"<p>Autocorrelation of errors in regression models refers to the correlation between the error terms at different time points or observations. Positive autocorrelation indicates that errors in one period are correlated with errors in previous periods. This violates the assumption of independence, impacting model reliability. Diagnostic plots or statistical tests, like the Durbin-Watson test, can assess autocorrelation in regression residuals. </p> Resources <p>CampusX</p> <ol> <li> What are the main Assumptions of Linear Regression? | Top 5 Assumptions of Linear Regression</li> <li> Presented all 5 assumptions in Notebook</li> </ol>"},{"location":"blog/regression-interview-questions/#why-multi-collinearity-is-a-problem","title":"Why multi-collinearity is a problem?","text":"<p>Multicollinearity occurs when two or more independent variables in a regression model are highly correlated. It is bad because it inflates standard errors, leading to unstable and unreliable coefficients. This makes it difficult to isolate the individual effect of each variable, reducing the model's interpretability and predictive accuracy.</p> <p>Detection: Use correlation matrices or variance inflation factor (VIF). High correlation coefficients or VIF values (&gt;5) indicate multicollinearity.</p> <p>Removal: Options include excluding one of the correlated variables, combining them, or using regularization techniques like Ridge/Lasso regression that penalize large coefficients.</p> Resources <p>CampusX</p> <ol> <li> Why Multicollinearity is Bad? What is Multicollinearity? How to detect and remove Multicollinearity</li> </ol>"},{"location":"blog/regression-interview-questions/#difference-bw-person-correlation-and-multi-collinearity","title":"Difference b/w \"Person Correlation\" and Multi-collinearity.","text":"<p>The main difference between Pearson Correlation and Multicollinearity lies in their applications within regression analysis.</p> <p>Pearson Correlation is a measure of the linear relationship between two numerical variables, ranging from -1 to 1, where 0 indicates no correlation and values close to 1 or -1 indicate a strong linear relationship</p> <p>On the other hand, Multicollinearity refers to a situation where independent variables in a regression model are highly correlated with each other, which can lead to issues such as inflated coefficients and weakened statistical measures like p-values</p> <p>While Pearson Correlation focuses on the relationship between two variables, Multicollinearity deals with the interplay among multiple independent variables in a regression context, impacting the model's interpretability and reliability. </p>"},{"location":"blog/regression-interview-questions/#what-is-vif-why-vif-5","title":"What is VIF? Why VIF &gt; 5?","text":"<p>VIF is a measure that quantifies how much the variance of an estimated regression coefficient increases if your predictors are correlated. A VIF greater than 5 or 10 is often considered indicative of multicollinearity.</p> <p>VIF &gt; 5 suggests that the variance of the coefficient estimate is 5 times higher due to multicollinearity, making the estimate less reliable and harder to interpret.</p> <p>Use VIF in python as <code>from statsmodels.stats.outliers_influence import variance_inflation_factor</code>.</p>"},{"location":"blog/regression-interview-questions/#what-is-r-squared-r2r2-score","title":"What is R-squared (R\u00b2/R2) score?","text":"<p>R-Squared (R\u00b2) is a statistical measure used to determine the proportion of variance in a dependent variable that can be predicted or explained by an independent variable.</p> <p>In other words, R-Squared shows how well a regression model (independent variable) predicts the outcome of observed data (dependent variable).</p> <p>R-Squared is also commonly known as the coefficient of determination. It is a goodness of fit model for linear regression analysis. </p>"},{"location":"blog/regression-interview-questions/#what-does-an-r-squared-value-mean","title":"What Does an R Squared Value Mean?","text":"<p>A R-Squared value shows how well the model predicts the outcome of the dependent variable. R-Squared values range from 0 to 1.</p> <p>An R-Squared value of 0 means that the model explains or predicts 0% of the relationship between the dependent and independent variables. And a value of 1 indicates that the model predicts 100% of the relationship, and a value of 0.5 indicates that the model predicts 50%, and so on.</p> Formula for R Squared \\[ \\text{R}^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} \\] Symbol Description \\(\\text{R}^2\\) Coefficient of determination \\(\\text{RSS}\\) Sum of squares of residuals \\(\\text{TSS}\\) Total sum of squares Resources <p> FreeCodeCamp</p> <ul> <li> What is R Squared? R2 Value Meaning and Definition</li> </ul>"},{"location":"blog/regression-interview-questions/#how-is-adjusted-r2-score-is-different-from-r2-score","title":"How is Adjusted R2 score is different from R2 score?","text":"<p>Adjusted R2 score is different from R2 score in the way they handle the addition of new predictors to a multiple regression model.</p> <p>While R2 score increases or remains the same as new predictors are added to the model, even if the newly added predictors are independent of the target variable and don't add any value to the predicting power of the model.</p> <p>On the other hand, Adjusted R2 score only increases if the newly added predictor improves the model's predicting power. It helps determine the goodness of fit for a multiple regression model by considering the number of predictors and the sample size. Adjusted R2 penalizes the model for useless variables, while R2 does not, making Adjusted R2 a more reliable measure of goodness of fit for multiple regression problems. </p> Resources <ul> <li> Demystifying R-Squared and Adjusted R-Squared</li> </ul>"},{"location":"blog/regression-interview-questions/#what-if-there-is-one-feature-related-to-another-then-what-should-we-do-with-them","title":"What-if there is one feature related to another then, what should we do with them?","text":"<ol> <li>Keep any one of them after checking the correlation with target feature \\((y)\\) whichever has low correlation; drop them and keep only one.</li> <li>Use <code>sklearn.decomposition.PCA</code> (with <code>sklearn.compose.ColumnTransformer</code>) to combine them as one and use that transformed column.</li> </ol>"},{"location":"blog/regression-interview-questions/#what-is-regularization-why-to-use-regularization-what-happen-in-regularization","title":"What is Regularization? - Why to use Regularization? - What happen in Regularization?","text":"<p>You can use regularization to penalize the model while trining when mode is trying to overfit with training data. Generally, there are three main types of regularization is used L1 (Lasso), L2 (Ridge) and (L1 + L2) ElasticNet.</p> <p> My blog on Regularization</p>"},{"location":"blog/regularization-in-ml/","title":"Regularization in ML","text":"<p>Regularization is used to solve the problem of overfitting caused while training a ML model. In regularization, the model is penalized for overfitting on train data means whenever model tries to predict on training data it add some penalty to the loss function in term of coefficients of the model.</p> <p>What is Overfitting?</p> <p>A ML model is said to be \"overfitting\" when it performs well on training dataset, but the performance is comparatively poor on the test/unseen dataset.</p> <p>What is Underfitting?</p> <p>An ML model is said to \"Underfitting\" when it does not performs well on both the train as well as test dataset.</p>"},{"location":"blog/regularization-in-ml/#types-of-regularization","title":"Types of Regularization","text":"<p>There are three main type of regularization in ML:</p> <ol> <li>L1 Regularization (Lasso)</li> <li>L2 Regularization (Ridge)</li> <li>L1 + L2 Regularization (ElasticNet)</li> </ol>"},{"location":"blog/regularization-in-ml/#l2-regularization-ridge","title":"L2 Regularization (Ridge)","text":"<p>Ridge Regression is a technique used in regression analysis to tackle the problem of overfitting, particularly when dealing with multiple correlated predictors or features. The L2 regularization technique, also known as Ridge Regression, adds a penalty term to the standard linear regression equation, which helps to mitigate the effects of multi-collinearity.</p> <p>The objective function for Ridge Regression is given by:</p> \\[ \\text{minimize}\\left( \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right) \\] <p>Where:</p> <ul> <li>\\((n)\\) represents the number of data points.</li> <li>\\((p)\\) represents the number of predictors or features.</li> <li>\\((x_{ij})\\) denotes the value of the \\((j^{th})\\) feature for the \\((i^{th})\\) data point.</li> <li>\\((y_i)\\) represents the observed output for the \\((i^{th})\\) data point.</li> <li>\\((\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p)\\) are the regression coefficients.</li> <li>\\((\\lambda)\\) is the hyperparameter that controls the regularization strength.</li> </ul> <p>The first part of the equation is the standard least squares regression term, which aims to minimize the squared difference between the predicted and actual output. The second part is the penalty term, which is the sum of squares of the coefficients \\((\\beta)\\) multiplied by the regularization parameter \\((\\lambda)\\).</p> <p>The hyperparameter \\((\\lambda)\\) controls the trade-off between fitting the model to the training data and preventing overfitting by keeping the coefficients small. A larger \\((\\lambda)\\) leads to a stronger penalty, effectively shrinking the coefficients toward zero. This helps to reduce the model's complexity, making it less sensitive to the noise in the data.</p> <p>Ridge Regression is a powerful tool in situations where multi-collinearity among predictors exists. By adding this penalty term, it stabilizes the coefficients and reduces their variance, thus improving the model's generalization and robustness when dealing with new, unseen data.</p> <p>Implementing Ridge Regression involves finding the optimal values for the coefficients by minimizing the combined error and penalty term. Various optimization algorithms such as gradient descent or closed-form solutions can be employed for this purpose.</p> <p>In conclusion, Ridge Regression with L2 regularization offers a balance between fitting the data and preventing overfitting, making it a valuable technique in the realm of regression analysis.</p>"},{"location":"blog/regularization-in-ml/#l1-regularization-lasso","title":"L1 Regularization (Lasso)","text":"<p>Lasso Regression, short for \"Least Absolute Shrinkage and Selection Operator,\" is a method used in regression analysis to handle overfitting and perform feature selection by adding a penalty term to the standard linear regression equation. The L1 regularization technique in Lasso Regression introduces sparsity by imposing a penalty based on the absolute values of the regression coefficients.</p> <p>The objective function for Lasso Regression is given by:</p> \\[ \\text{minimize}\\left( \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right) \\] <p>Where:</p> <ul> <li>\\((n)\\) represents the number of data points.</li> <li>\\((p)\\) represents the number of predictors or features.</li> <li>\\((x_{ij})\\) denotes the value of the \\((j^{th})\\) feature for the \\((i^{th})\\) data point.</li> <li>\\((y_i)\\) represents the observed output for the \\((i^{th})\\) data point.</li> <li>\\((\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p)\\) are the regression coefficients.</li> <li>\\((\\lambda)\\) is the hyperparameter that controls the regularization strength.</li> </ul> <p>The first part of the equation is the standard least squares regression term that minimizes the squared difference between the predicted and actual output. The second part is the penalty term, which is the sum of the absolute values of the coefficients \\((\\beta)\\) multiplied by the regularization parameter \\((\\lambda)\\).</p> <p>The hyperparameter \\((\\lambda)\\) controls the trade-off between fitting the model to the training data and keeping the coefficients small. In Lasso Regression, the absolute value of the coefficients' sum is used as the penalty. This has the effect of forcing some coefficients to be exactly zero, effectively performing variable selection by eliminating less influential features. The sparsity induced by L1 regularization makes Lasso Regression particularly useful when dealing with datasets with a large number of features, as it can automatically perform feature selection.</p> <p>Implementing Lasso Regression involves finding the optimal values for the coefficients by minimizing the combined error and penalty term. Various optimization techniques like coordinate descent or sub-gradient methods can be employed to achieve this.</p> <p>In conclusion, Lasso Regression with L1 regularization is a valuable tool for not only preventing overfitting but also performing automatic feature selection by shrinking certain coefficients to zero. This makes it a popular choice when dealing with high-dimensional datasets and seeking a more interpretable and sparse model.</p>"},{"location":"blog/regularization-in-ml/#elasticnet","title":"ElasticNet","text":"<p>This is the combination of both L1 and L2 regularization.</p> \\[ \\text{minimize}\\left( \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij}\\beta_j \\right)^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2 \\right) \\] <p>Acknowledgement</p> <ul> <li> Regularization Techniques</li> <li> CampusX YouTube</li> </ul>"},{"location":"blog/resume-tips/","title":"Resume Tips","text":"<p>NOTE: I am writing this article from the POV of Data Scientists.</p>"},{"location":"blog/resume-tips/#important-points-to-follow","title":"Important Points To Follow","text":"<p>By implementing these strategies, you can elevate your Resume/CV to effectively communicate your qualifications and stand out among the competition in today's job market.</p> <ul> <li> <p>Keep It Concise</p> <p>Limit your CV to a single page to maintain reader interest and showcase your most relevant experiences succinctly.</p> </li> <li> <p>Optimize Layout</p> <p>Minimize white spaces and prioritize content placement for a visually appealing and easy-to-read document.</p> </li> <li> <p>Highlight Relevant Projects</p> <p>Tailor your CV to the job role by including specific projects that demonstrate your skills and expertise in alignment with the position.</p> </li> <li> <p>Include Important Links</p> <p>Provide links to your LinkedIn, GitHub, and Kaggle profiles to offer recruiters a deeper insight into your professional background and achievements.</p> </li> <li> <p>Showcase Certifications</p> <p>Feature 2 to 4 certifications that highlight your commitment to continuous learning and validate your proficiency in relevant areas.</p> </li> <li> <p>Emphasize Technical Skills</p> <p>Dedicate a section to showcasing your technical prowess, listing skills such as Statistics, GenAI, Power BI, and other job-specific technologies to demonstrate your capability to excel in the role.</p> </li> <li> <p>Tweak CV For Job Role</p> <p>Tweak your CV before submitting for job role, company's requirements; don't add unnecessary projects or tech skills other than required one.</p> </li> </ul>"},{"location":"blog/resume-tips/#how-to-list-projects","title":"How to list projects?","text":"<ul> <li>Project starts with Project Title.</li> <li>Define project assessment in bullet points.</li> <li>Must write the results (like RMSE, MAE of model). TLDR; Result has to be a number afterall.</li> <li>Dedicate one or two points to define how you solve the problem using this project.</li> </ul>"},{"location":"blog/resume-tips/#formatting","title":"Formatting","text":"<ul> <li>Result numbers must be in bold.</li> <li>Use Sans Serif fonts like Calibri or Poppins.</li> <li>Use font size between 10 to 12.</li> <li>Too many formatting style must be restricted.</li> </ul>"},{"location":"blog/resume-tips/#writing","title":"Writing","text":"<ul> <li>Check Grammatical errors. Use ChatBots to tackle this.</li> <li>Use Action Verbs in your Resume. Checkout some well-known words below </li> <li>\ud83d\ude45 Don't add summary which shows you that you wants to grow or you'll help the company to grow and something like this.</li> <li>You can skip your Profile Summary or About Me section; if your CV got bigger. But try to keep it short and crisp.</li> </ul> List of Action Verbs <p> \u00a0 Download My Resume </p>"},{"location":"blog/starship-prompt/","title":"Starship Prompt","text":"I have introduced to Starship Prompt a week ago and it makes my shell prompt amazing.  <p>Prerequisites</p> <ul> <li>Install Starship Prompt from official documentation.</li> <li>Setup your shell (<code>bash</code>, <code>zsh</code> or <code>fish</code>) to use Starship from official documentation</li> </ul> <p>Note</p> <ul> <li>I am using MacOS. So, for other OS the paths may differ.</li> <li>I have used Nerd Fonts thats why there are some <code>symbol</code> which  may not appear as they are.</li> <li>I have defined some conventions to define my <code>starship.toml</code> file. If you want to know them read the sections where I describe my conventions.</li> </ul>"},{"location":"blog/starship-prompt/#customization","title":"Customization","text":"<p>You can customize your prompt for each programming languages you uses like  Python,  JavaScript,  Rust and more. See official documentation to know more.</p>"},{"location":"blog/starship-prompt/#example-python","title":"Example:  Python","text":"<pre><code>[python]\nsymbol = \"\ue606\" # (1)!\nstyle = \"arv_python\"\nformat = \"[\ue0b6](fg:$style)[$symbol( $version)[( \\\\($virtualenv\\\\))](bold bg:$style)](bg:$style)[\ue0b4](fg:$style)\"\n</code></pre>"},{"location":"blog/starship-prompt/#example-docker","title":"Example:  Docker","text":"<pre><code>[docker_context]\nsymbol = \"\uf308\" # (1)!\nstyle = \"arv_docker\"\nformat = \"[\ue0b6](fg:$style)[$symbol ($context)](bg:$style)[\ue0b4](fg:$style)\"\n</code></pre> Conventions <ol> <li>Enclosed Modules: Each modules enclose with circular end.</li> </ol>"},{"location":"blog/starship-prompt/#color-palette","title":"Color Palette","text":"<p>Did you see <code>style = \"arv_python\"</code> and <code>style = \"arv_docker\"</code> in above examples. Those are my custom defined palette as <code>\"arv-anshul\"</code>.</p> <pre><code>palette = \"arv-anshul\"\n\n[palettes.arv-anshul]\narv_dir = \"203\"\narv_docker = \"026\"\narv_git = \"063\"\narv_python = \"028\"\narv_custom = \"236\"\n</code></pre> Print ANSI Colormap <pre><code># Use this function to print ANSI colormap\nfunction colormap() {\n    range_start=${1:-1}\n    range_end=${2:-255}\n\n    for i in $(seq $range_start $range_end); do\n        echo -en \"\\e[48;5;${i}m  ${(l:3::0:)i}  \\e[0m \"\n        [[ $((i % 10)) -eq 0 ]] &amp;&amp; echo\n    done\n    return 0\n}\n</code></pre> <p>If you doc't want to use ANSI color format then you also use <code>starship</code>'s pre-defined colors: <code>black</code>, <code>red</code>, <code>green</code>, <code>blue</code>, <code>yellow</code>, <code>purple</code>, <code>cyan</code>, <code>white</code>. You can optionally prefix these with <code>bright-</code> to get the bright version (e.g. <code>bright-white</code>). See in documentation</p> <pre><code>palette = \"arv-anshul-color\"\n\n[palettes.arv-anshul-color]\narv_dir = \"bright-red\"\narv_docker = \"bright-blue\"\narv_git = \"blue\"\narv_python = \"green\"\narv_custom = \"black\"\n</code></pre> Conventions <ol> <li>Palette Preffix: Palette's <code>keys</code> must have a preffix (in my case it is <code>\"arv_\"</code>).</li> <li>ANSI Codes: I have defined colors in ANSI Codes. See this Gist to know more about ANSI Codes.</li> <li>Own Color Palette: I have defined programming language-wise (or module-wise) colors which makes easy to change/manipulate the color of any language.</li> </ol> <p>You can also create your own custom color palette in <code>~/.config/starship.toml</code>.</p>"},{"location":"blog/starship-prompt/#extra-customization-with-custom-modules","title":"Extra customization with custom modules","text":"<pre><code>[custom.github]\ndetect_folders = [\".github\"]\nformat = \"[$symbol]($style)\"\nstyle = \"bg:arv_custom\"\nsymbol = \"\uea84 \" # (1)!\n\n[custom.mkdocs]\ndetect_files = [\"mkdocs.yaml\", \"mkdocs.yml\"]\ndetect_folders = [\"docs\"]\nformat = \"[$symbol]($style)\"\nstyle = \"bg:arv_custom\"\nsymbol = \"\udb85\udd17 \" # (2)!\n\n[custom.vscode]\ndetect_folders = [\".vscode\"]\nformat = \"[$symbol]($style)\"\nstyle = \"bg:arv_custom\"\nsymbol = \"\udb82\ude1e \" # (3)!\n</code></pre> Conventions <p>I have used <code>custom</code> modules to just show some desirable icons in the prompt but you can do a lot of thing using <code>custom</code> modules (the possibilities are endless).</p> <ol> <li>Show Custom Icons: I used <code>custom</code> modules to show icons by detecting files and folders. For example, prompt will show  icons when <code>.github</code> folder is present in the current directory.</li> </ol> <p>Refer to official documentation to know more about custom modules.</p> <p>Tip</p> <p>From official documentation</p> <p>Issue #1252 contains examples of <code>custom</code> modules. You can go there for inspiration and if you have an interesting example not covered there, feel free to share it there!</p> <p></p> If you want to make prompt to look like mine! Click below <p> \u00a0 starship.toml </p>"},{"location":"blog/tree-vs-regression-models/","title":"Tree VS Regression Models","text":"<p>Tree based models and Regression models are widely used Machine Learning models. So more you know about them is better for you. Also, many concepts from these models are borrowed by advance Machine Learning models like Gradient Boosting, XGBoost, etc.</p> <p>These models are also great choice for :fontawesome-user-tie: interviewers so from these models they ask many interview questions. This blog mainly focuses on tree based models.</p>  Aspect Decision Trees \u00a0 Regression Models Type Can be used for classification and regression Used for predictive modeling, predicting continuous values Decision Boundaries Bisect the space into smaller regions, fitting lines to divide the space exactly Focuses on predicting outcomes based on previous data or trends Interpretability Easy to understand and interpret due to their flowchart-like structure Interpretability varies based on the complexity of the model Advantages Easy to interpret, visualize, and require minimal data preparation Effective in predicting continuous values based on historical data Disadvantages Prone to overfitting noisy data, especially with deeper trees May struggle with capturing complex relationships in the data How They Work Split the dataset based on data homogeneity, using measures like entropy for classification trees and Sum of Squared Errors for regression trees Focuses on fitting a curve or surface to the data points to predict continuous values"},{"location":"blog/tree-vs-regression-models/#1-what-are-the-main-differences-between-tree-based-models-and-regression-models","title":"1. What are the main differences between tree based models and regression models?","text":"Aspect Tree Based Models Linear Models Approach Tree based models uses Divide &amp; Conquer approach to learn the data essence by making cut into data space to create small spaces which segregate the homogenous data. Regression models tries to create/fit a line in-between the data space, to get the essence of the data and tries to keep the value of loss function minimum as possible. Non-Linearity Tree models can handle both linear and non-linear data. Regression models performs better with linearly separable data but it fails to capture the essence of non-linear data. Computational Complexity Tree models are more complex than linear models, especially training complexity of Ensemble techniques as it finds optimal solution iteratively. Regression models has an upper edge here because it requires find the optimal co-efficient which minimizes the loss function and for that it uses Gradient Descent techniques. Effects of Outliers Tree models are generally robust to outliers because they creates multiple decision boundaries which separates the outliers easily and model doesn't affect much from it. Regression models are highly sensitive to outliers because when the data has outliers the best fit line will try to fit the outliers values to reduce the loss function. To reduce this sensitivity we have Regularization concepts. Handling Null Values Some tree based algorithms/models can handle null values like XGBoost. Linear model can't handle null values at all . Preprocess the data before training linear models. Interpretable Only Decision Trees are interpretable but libraries like SHAP can explain ensemble tree models. RandomForest can be used to calc features importance. Linear models are very interpretable because it calculates the features co-efficient with target feature."},{"location":"blog/tree-vs-regression-models/#2-can-you-describe-a-real-world-application-where-you-would-prefer-to-use-a-randomforest-over-a-logistic-regression-model","title":"2. Can you describe a real-world application where you would prefer to use a RandomForest over a logistic regression model?","text":"<p>We can preffer RandomForest over Logistic Regression in the many scenarios like:</p> <ol> <li>When data has outliers.</li> <li>When data is hight dimensional.</li> <li>When data is non-lineear.</li> <li>When data is imbalanced.</li> <li>RandomForest algorithm is robust to overfitting while training.</li> <li>Also, RandomForest can handle categorical features better than logistic regression because you can use <code>OrdinalEncoder</code> to encode categorical feature instead of <code>OneHotEncoder</code>.</li> <li>By the way, you can use libraries like <code>h2o</code> which can handle null values too.</li> </ol>"},{"location":"blog/tree-vs-regression-models/#3-what-is-the-impact-of-outliers-on-decision-tree","title":"3. What is the impact of outliers on Decision Tree?","text":"<p>Decision Trees can handle Outliers easily because it segregate them using decision boundaries in the initial steps. However, if the Decision Tree becomes overfitted to a training dataset, it can become more sensitive to outliers, potentially affecting the model's performance. Also, you can Regularization methods to tackle outliers.</p> <p>Mainly outliers can affect Decision Trees while working with regression problems (only in those leaf-nodes where outliers are present/classified/calculated). Prediction of model is affected in those leaf-nodes where outliers are calculated (this is proven) but this is not the case in classification problems.</p>"},{"location":"blog/tree-vs-regression-models/#4-what-is-the-role-of-pruning-in-decision-tree-what-is-post-pruning-and-pre-pruning","title":"4. What is the role of pruning in Decision Tree, what is Post-Pruning and Pre-Pruning?","text":"<ul> <li> <p> Role of Pruning in Decision Trees</p> <ol> <li>Pruning in Decision Trees is crucial to prevent overfitting and enhance the model's ability to generalize by simplifying the tree structure.</li> <li>It involves removing parts of the tree that do not contribute significantly to predictive power, making the model more interpretable and effective.</li> </ol> </li> </ul> <ul> <li> <p> Pre-Pruning</p> <ol> <li>Pre-pruning involves stopping the tree's growth before it fits the entire training set.</li> <li>It focuses on setting hyperparameters to control the tree's size during construction, preventing overfitting by limiting its complexity.</li> </ol> </li> <li> <p> Post-Pruning </p> <ol> <li>Post-pruning allows the tree to grow fully and then removes nodes that do not add substantial predictive power.</li> <li>Techniques like cost-complexity pruning are commonly used in post-pruning to simplify the tree by selecting the subtree with the smallest cost based on a complexity parameter and the number of leaf nodes.</li> </ol> </li> </ul> <p>Summary</p> <ol> <li>Pre-pruning: Penalize (by cutting the nodes) the model while training.</li> <li>Post-pruning: First fully train the model then after penalize the model by cutting down the nodes.</li> </ol>"},{"location":"blog/tree-vs-regression-models/#cost-complexity-function-in-decision-tree","title":"Cost Complexity Function in Decision Tree","text":"<p>The cost complexity function in decision trees is a crucial concept related to pruning techniques. It involves a tradeoff between error (cost) and tree size (complexity) to find an optimal tree. The cost complexity of a tree, denoted as \\(R_{c_p}(T)\\), is the sum of its risk (error) and a \"cost complexity\" factor \\(c_p\\) multiplied by the tree size \\(T\\). This function is used in cost complexity pruning to minimize the cross-validated prediction error and prevent overfitting. By adjusting the cost complexity parameter \\(c_p\\), decision trees can be pruned effectively to improve generalization to test data. </p> <p> <code>plot_cost_complexity_pruning</code></p>"},{"location":"blog/tree-vs-regression-models/#5-how-missing-values-are-handled-in-tree-based-algorithms-like-xgboost","title":"5. How missing values are handled in tree based algorithms like XGBoost?","text":"<p>XGBoost only handle missing values present in Input features. It doesn't handle null values present in Output/Target feature. You have to preprocess or remove the null values of Output feature.</p> <p> Not completed!</p>"},{"location":"blog/tree-vs-regression-models/#6-what-is-the-difference-between-id3-c45-and-cart-algorithms","title":"6. What is the difference between ID3, C4.5, and CART algorithms?","text":"Algorithm ID3 C4.5 CART Type Iterative Dichotomiser 3 Iterative algorithm, extension of ID3 Classification and Regression Trees Handling of Numeric Attributes Less effective for numeric attributes Handles numeric and categorical attributes Handles numeric and categorical attributes Splitting Criteria Information Gain Gain Ratio Gini diversity index for classification tests Pruning Does not handle pruning Prunes trees to avoid overfitting Prunes trees using a complex model with parameters estimated by cross-validation Binary Tests Always binary tests Allows two or more outcomes Binary tests"},{"location":"blog/tree-vs-regression-models/#7-how-would-you-approach-a-situation-where-your-tree-based-model-is-overfitting","title":"7. How would you approach a situation where your tree-based model is overfitting?","text":"<p>I can apply pruning techniques which penalize the model if it tries to overfit while training. We have many hyperparameters in DecisionTree, RandomForest classes apply pre-pruning on models.</p> <ol> <li><code>max_depth</code>: The maximum depth of the tree. Prevent the tree to grow after specified depth.</li> <li><code>min_samples_split</code>: The minimum number of samples required to split an internal node.</li> <li><code>min_samples_leaf</code>: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</li> <li><code>min_weight_fraction_leaf</code>: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</li> <li><code>max_features</code>: The number of features to consider when looking for the best split.</li> <li><code>max_leaf_nodes</code>: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If <code>None</code> then unlimited number of leaf nodes.</li> <li><code>min_impurity_decrease</code>:  A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</li> <li><code>class_weight</code>: Weights associated with classes in the form <code>{class_label: weight}</code>. If <code>None</code>, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.</li> <li><code>ccp_alpha</code>: Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed.</li> </ol> <p> <code>sklearn.tree.DecisionTreeClassifier</code></p>"},{"location":"blog/tree-vs-regression-models/#8-discuss-the-role-of-shrinkage-learning-rate-in-boosting-algorithms-how-does-it-contribute-to-model-performance-and-robustness","title":"8.  Discuss the role of shrinkage (learning rate) in boosting algorithms. How does it contribute to model performance and robustness?","text":"<ul> <li>The learning rate, also known as shrinkage, plays a crucial role in boosting algorithms by determining how fast or slow a model updates its weights based on the gradient of the loss function.</li> <li>A fixed learning rate can lead to challenges such as overshooting the optimal point with a high value or slow convergence with a low value, highlighting the importance of dynamic adjustments during training.</li> <li>Adaptive learning rate schedules, like decay learning rates, gradually reduce the learning rate as training progresses, helping to avoid overshooting the minimum and fine-tune model parameters more precisely.</li> <li>Decay learning rates can be implemented using various methods like fixed or exponential decay rates, step or inverse decay functions, and are essential for efficient and effective model training, especially in complex and non-convex problems.</li> <li>By adjusting the learning rate dynamically, decay learning rates ensure that the model progresses effectively towards the optimal solution, balancing the size of steps taken during training to enhance model optimization and robustness.</li> </ul> <p>This question comes under Boosting algorithms which is advance topic.</p>"},{"location":"blog/tree-vs-regression-models/#9-discuss-the-computational-complexity-of-training-a-decision-tree-and-how-it-scales-with-the-size-of-the-dataset","title":"9. Discuss the computational complexity of training a Decision Tree and how it scales with the size of the dataset.","text":"<ul> <li>The computational complexity of training a Decision Tree is influenced by the size of the dataset and the number of dimensions in the feature space.</li> <li>Heuristic algorithms are commonly used to compute Decision Trees from training data, aiming to minimize the size of the resulting tree, which impacts the computational complexity of the process.</li> <li>Understanding the computational complexity of training Decision Trees is essential for optimizing algorithms and improving efficiency in machine learning tasks, especially when dealing with large datasets and high-dimensional feature spaces.</li> </ul>"},{"location":"blog/tree-vs-regression-models/#10-how-do-tree-based-algorithms-handle-imbalanced-datasets-and-what-are-the-implications-for-model-performance-and-interpretation","title":"10. How do tree-based algorithms handle imbalanced datasets, and what are the implications for model performance and interpretation?","text":"<p>There is a hyperparameter in DecisionTree class called <code>class_weigth</code> where you can assign weights to each class label or you can provide <code>\"balanced\"</code> which will automatically assign weights to each class labels.</p>"},{"location":"blog/tree-vs-regression-models/#how-do-you-manually-assign-weights","title":"How do you manually assign weights?","text":"<p>It depends on your domain knowledge or you can use hit &amp; try method.</p>"},{"location":"blog/tree-vs-regression-models/#how-do-balanced-value-assign-weights","title":"How do <code>\"balanced\"</code> value assign weights?","text":"<p>It assigns \\(\\frac{1}{n}\\) weight value to each class labels where \\(n\\) is the count of data points present the following class.</p> <p>Similar blogs</p> <p>  Decision Tree  Regression Interview Questions  Regularization in ML </p>"},{"location":"blog/finetune-transformers/","title":"Finetune Transformers","text":"<p>Learning how to finetune a BERT model using PyTorch/TensorFlow from HuggingFace for your usecase is a art in itself because there are so many ways and methods to do it that you will not able to figure out which is the best for my usecase. BTW, you can always refer to HuggingFace documentation.</p> For example! <ol> <li>Choose between  PyTorch and  TensorFlow. (let choose )</li> <li>If you are importing your dataset with <code>pandas</code> or <code>polars</code> then need to create a custom class by inheriting    <code>torch.utils.data.Dataset</code> class.</li> <li>Then need to tokenize the data and need to use <code>DataLoader</code> and Data Collator.</li> <li>Then use a for-loop to train and validate the model.</li> </ol> <p>But there is a easy way to finetune, by using objects like <code>transformers.TrainingArguments</code> and <code>transformers.Trainer</code> which reduces the manual looping complexity.</p>"},{"location":"blog/finetune-transformers/#finetune-process","title":"Finetune Process","text":""},{"location":"blog/finetune-transformers/#load-data","title":"Load Data","text":"<p>Import dataset your method such as <code>pandas</code>, <code>polars</code> or other ways.</p>"},{"location":"blog/finetune-transformers/#preprocess-data","title":"Preprocess Data","text":"<p>Precess the data and check the <code>labels</code>.</p> <p>Related Docs</p> <ul> <li> Preprocess Data</li> </ul>"},{"location":"blog/finetune-transformers/#train-val-test-dataset","title":"Train-Val-Test Dataset","text":"<p>Split the data into train, validation and test data. Before doing this you have consider many things like:</p> <ul> <li>How to tokenize the data with certain <code>padding</code>, <code>truncation</code>, <code>max_length</code>, <code>return_tensors</code>, etc.?</li> <li>Do you need to shuffle the data? (Only shuffle train dataset)</li> <li>Which object you will use to store the data? (<code>datsets.Dataset</code> or <code>torch.utils.data.DataLoader</code>)</li> <li>Representation or data type of labels column? This will be different for different type of problems you have to   make sure that the data is in correct format.</li> <li>Is DataCollator required?</li> </ul>"},{"location":"blog/finetune-transformers/#tokenize-data","title":"Tokenize Data","text":"<p>You need to tokenize the data before sending it to model to trained on. It is done using  respective model's tokenizer. You can tokenize the data separately or in batch (recommended).</p> <p>Related Docs</p> <ul> <li> Padding and Truncation</li> </ul>"},{"location":"blog/finetune-transformers/#batch-creation","title":"Batch Creation","text":"<p>You have to cast the dataset into a object which supports the batching.</p> <p>Related Docs</p> <ul> <li> <code>datasets.Dataset</code></li> <li><code>torch.utils.data.Dataset</code></li> <li><code>torch.utils.data.DataLoader</code></li> </ul>"},{"location":"blog/finetune-transformers/#data-collator","title":"Data Collator","text":"<p>Data collators are objects that will form a batch by using a list of dataset elements as input.</p> <p>Related Docs</p> <ul> <li> Data Collator</li> <li><code>transformers.DataCollatorWithPadding</code></li> </ul>"},{"location":"blog/finetune-transformers/#load-tokenizer","title":"Load Tokenizer","text":"<p>A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a \u201cFast\u201d implementation based on the Rust library  Tokenizers.</p> <p>Related Docs</p> <ul> <li> Tokenzier</li> <li> AutoTokenzier</li> </ul>"},{"location":"blog/finetune-transformers/#load-model","title":"Load Model","text":"<p>A pre-trained model which we are going to finetune using our custom dataset.</p> <p>Related Docs</p> <ul> <li> Auto Classes</li> <li> Model Configuration</li> <li><code>transformers.AutoModel</code></li> <li><code>transformers.BERTModel</code></li> </ul>"},{"location":"blog/finetune-transformers/#bert-model","title":"BERT Model","text":"<p>Related Docs</p> <ul> <li><code>transformers.BERTModel</code></li> <li><code>transformers.BERTConfig</code></li> <li><code>transformers.BertForSequenceClassification</code></li> </ul>"},{"location":"blog/finetune-transformers/#model-trainingfinetuning","title":"Model Training/Finetuning","text":"<p>Related Docs</p> <ul> <li> Optimization Strategies</li> <li><code>transformers.Trainer</code></li> <li> Better Fine Tuning by Matt Williams</li> </ul>"},{"location":"blog/finetune-transformers/#peft-methods","title":"PEFT Methods","text":"<p>PEFT offers parameter-efficient methods for finetuning large pretrained models by training a smaller number of parameters using a reparametrization method like LoRA and more.</p> <p>Related Docs</p> <ul> <li> PEFT Quicktour</li> <li>LoRA</li> <li>LoRA Methods</li> <li>Quantization</li> </ul>"},{"location":"blog/finetune-transformers/#evaluate-model","title":"Evaluate Model","text":"<p>Related Docs</p> <ul> <li> Evaluate Library</li> <li>Evalute a TextClassification Model</li> </ul>"},{"location":"blog/finetune-transformers/#model-predictioninference","title":"Model Prediction/Inference","text":"<p>Related Docs</p> <ul> <li> Model Outputs</li> <li><code>transformers.modeling_outputs.SequenceClassifierOutput</code></li> </ul>"},{"location":"blog/finetune-transformers/#training-with-pytorch","title":"Training with Pytorch","text":"<p>You can either finetune a pretrained model in native PyTorch or with <code>transformers.Trainer</code> class (recommended).</p> <p>Read this documentation by  HuggingFace \"Fine-tune a pre-trained model\" where they explain how to finetune a pre-trained model using both the methods separately.</p> <p>Also refer to this tutorial by same team for \"Text Classification\".</p>"},{"location":"blog/job-applying-agent/","title":"Job Applying Agent","text":""},{"location":"blog/job-applying-agent/#requirements","title":"Requirements","text":"Requirements Description Extra Framework Langchain We will eventually try to drop it. But if this is a showcase project then a framework is best. Job Opening Data Jobs where agent will apply Maybe agent will fetch data on-the-go by web scraping. User Login (LinkedIn) Need to login the user on the platform in order to apply"},{"location":"blog/job-applying-agent/#agent-workflow","title":"Agent Workflow","text":"<ul> <li>LinkedIn Profile Scraper: Scrape LinkedIn profile of user and get relevant info.</li> <li>Parse User's Resume: Parse info from user's Resume.</li> <li>Retrive relevant jobs: On the basis of user's parsed info agent will retrive relevant jobs for user after   analyzing jobs' description.</li> </ul> LLM Agent Task Detailed Process Requirements/Dependencies Considerations Receive &amp; Parse Job Data - Accept job listing details (title, description, requirements) via API/webhook or direct scraping.- Parse and structure the data for further analysis. - Access to structured job data.- Data parsing modules (e.g., regex, NLP parsers). - Handle variations in data format.- Ensure data completeness before processing. Extract Key Job Attributes - Analyze job description to identify required skills, qualifications, and responsibilities.- Highlight keywords and competencies to match user profile. - NLP techniques for entity extraction and keyword analysis.- Domain-specific dictionaries or models. - Handle ambiguous or vague descriptions.- Ensure extraction accuracy. Align with User Profile &amp; History - Retrieve relevant parts of the user\u2019s resume and past experiences.- Map job requirements with user skills to determine fit and emphasize strengths. - Access to the user\u2019s structured profile data.- Matching algorithms to align job needs with profile attributes. - Maintain user data privacy.- Address gaps between job requirements and user profile gracefully. Generate Custom Application Materials - Construct a tailored prompt for generating a cover letter (or resume adjustments) using the extracted job and user information.- Generate draft content via the LLM. - Access to a capable LLM (e.g., GPT-4) with prompt engineering.- Template management and customization frameworks. - Balance personalization with professionalism.- Verify that generated content is factually consistent with the user\u2019s data. Iterative Refinement &amp; Quality Check - Run generated content through self-evaluation steps (e.g., re-prompting for clarity, checking for tone, grammar, and job-specific relevance). - Additional LLM prompts for self-review.- External grammar and style-check tools if necessary. - Allow for multiple iterations to refine quality.- Optionally incorporate human feedback or pre-defined quality metrics. Simulate Application Interface Integration - Prepare the final content to be inserted into LinkedIn\u2019s application forms (e.g., cover letter text, resume attachments).- Format the content to meet form requirements. - Knowledge of LinkedIn\u2019s input fields and formatting constraints.- Integration layer with the application automation module. - Ensure compatibility with the LinkedIn interface.- Handle edge cases like character limits or specific formatting needs. Execute Automated Submission - Pass the finalized content to the automation module (e.g., Selenium script) for form submission.- Monitor confirmation messages or error responses. - Stable API/web automation scripts.- Communication channel between LLM agent and submission module. - Robust error handling is critical.- Confirm that submission was successful and log the outcome. <p>Generated with  ChatGPT.</p>"},{"location":"blog/job-applying-agent/#problems","title":"Problems","text":""},{"location":"blog/job-applying-agent/#applying-to-jobs-in-behalf-of-user","title":"Applying to Jobs in behalf of user.","text":"<p>If we are going to implement this then this might be the most heavy situation because we have to deal with multiple steps like:</p> <ol> <li>User Login.</li> <li>Migrating to each job listing page.</li> <li>Applying to job by filling the form including uploading user's Resume.</li> </ol> <p>Due to this problem we should first focus on creating an AI agent which find relevant jobs for user by seeing their LinkedIn profile and their Resume.</p>"},{"location":"blog/job-applying-agent/#agent-flow","title":"Agent Flow","text":"<ul> <li>Linkedin profile URL.</li> <li>Resume as PDF</li> <li>A tool will extract info like experience, job type, job mode, and more.</li> <li>A tool will scrape relevant jobs on the basis of user's extracted data.</li> <li>Sort the scraped jobs by most relevant for user.</li> </ul>"},{"location":"blog/job-applying-agent/#langchain-perception","title":"LangChain Perception","text":"<ul> <li>How to use messages? (With .stream method)</li> <li>How to implement async tools? (For profile scraping and resume parsing purpose)</li> </ul>"},{"location":"blog/job-applying-agent/#job-finder-raycast-note","title":"\ud83d\udd0e Job Finder - Raycast Note","text":""},{"location":"blog/job-applying-agent/#current-plan","title":"Current Plan","text":"<p>We planned to provided a platform where both Employee and Employer list their skills and requirements and then we will provide a interface where both Employee and Employer will finder their match on the basis of their skills and requirements.</p>"},{"location":"blog/job-applying-agent/#problem-with-current-plan","title":"Problem (with Current Plan)","text":"<ul> <li>As we are no one who is going to use product the idea Employer listing is hard to be gain success on, so I decided to to drop it. (read next)</li> <li>We can scrape websites like LinkedIn, Naukri, etc. and gather data related to Jobs and make the platform only for Employee (as name suggests Job Finder).</li> </ul>"},{"location":"blog/job-applying-agent/#platform-usage","title":"Platform Usage","text":"<ul> <li>User will upload its Resume/CV on our website.</li> <li>We will parse their CV using OCR and extract a structure information for our usecase.</li> <li>We will then asks the user to fill a partially filled form to get full info about the user.</li> <li>The form is already filled by the info we extracted from user\u2019s CV.</li> <li>Then, we will recommend most similar Jobs we got in our database.</li> </ul>"},{"location":"blog/job-applying-agent/#future-plans","title":"Future Plans","text":"<ul> <li>We may also provide a service to create a resume by filling a simple form.</li> </ul>"},{"location":"blog/job-applying-agent/#references","title":"References","text":"<ul> <li>jsonresume.org: For custom resume creation.</li> </ul>"},{"location":"blog/job-applying-agent/#resourses","title":"Resourses","text":"<ul> <li>https://towardsdatascience.com/ai-powered-information-extraction-and-matchmaking-0408c93ec1b9/?sk=623aaf8837df5630ed60c40e66766553</li> <li> </li> </ul>"},{"location":"blog/job-applying-agent/#httpsgithubcomluillyferesume-insights","title":"https://github.com/luillyfe/resume-insights","text":""},{"location":"blog/redirect-search-engine/","title":"Redirect Search Engine","text":"<p>A search engine which can redirect to the website according to the user's query.</p>"},{"location":"blog/redirect-search-engine/#approach","title":"Approach","text":"<ol> <li>Get user's query.</li> <li>Search with minimal engines like DuckDuckGo, Google Search, Bing, etc. and get their most relevant result.</li> <li>Finally redirect to the top resultant URL.</li> <li>Alternatively, we can also recommend most relevant results in dropdown menu for ease of redirection for user.</li> </ol>"},{"location":"blog/redirect-search-engine/#why-someone-use-this-search-engine","title":"Why someone use this search engine?","text":"<p>Due to ease of website navigation from traditional search engine.</p>"},{"location":"blog/redirect-search-engine/#cons","title":"Cons","text":"<p>Someone can use search engine like Perplexity to get it's query's result in natural language response.</p>"},{"location":"blog/redirect-search-engine/#questions","title":"Questions","text":"<ul> <li>Is this a good product to build?</li> <li>Is this a good product to sustain in future?</li> <li>Is this product really help peoples in their daily life?</li> <li>Is this product really outsmart current competitors?</li> <li>What are more cons of this product from POV of current competitors?</li> </ul>"},{"location":"project/","title":"My Projects Index","text":"<p>  Other Projects </p>"},{"location":"project/#youtube-comment-sentiment","title":"YouTube Comment Sentiment","text":"<p>An end-to-end machine learning project to predict sentiment of comments on YouTube videos. It uses NLP and ML to predict sentiment of comments. Backend made using FastAPI and hosted on Render.com. Frontend made using VueJs and hosted on GitHub Pages.</p> <p><p> </p></p>"},{"location":"project/#canvas-ai","title":"Canvas AI","text":"<p>Parse and Analyse image containing mathematical expression or problem (it can be in a drawing format) using AI to give a structured response by leveraging Langchain framework. Also created API out of it using FastAPI which has containerized with Docker.</p> <p><p> </p></p>"},{"location":"project/#youtube-watch-history","title":"YouTube Watch History","text":"<p>A streamlit app where you can upload your YouTube Watch History Data to see insights on your viewing pattern. Your data will go through a ML Model which predicts the ContentType of each video your have watch. The app fetches more details of each video through YouTube API. There is also a Channel Recommender System in the app which recommend you similar channels on the basis of channel's video titles and tags they had used.</p> <p><p> </p></p>"},{"location":"project/#spotify-analysis","title":"Spotify Analysis","text":"<p>Analyse your Spotify Streaming data and get some insights from it like whom &amp; when you listen your favorite Tracks, Artists, Playlists or Albums.  Created Jupyter Notebook where you can upload your data and get the analysis by running the notebook.</p> <ul> <li> <p>Analyse your Streaming History, Personal Playlists.</p> </li> <li> <p>Know your top tracks and artists by year, month, week and day.</p> </li> <li> <p>Know your most played track, artist, playlist by year and month.</p> </li> </ul> <p><p> </p></p>"},{"location":"project/#e-commerce-scrapper-api-fastapi","title":"E-commerce Scrapper API - FastAPI","text":"<p>Scrape data from E-commerce websites like Flipkart by searching products like iphone, laptops and more. Also you can export the scrapped data into JOSN format. API will do all the works for you, you have to just make a request with some endpoints.</p> <ul> <li> <p>Modular Coding, Async Programming.</p> </li> <li> <p>I have also written some tests using <code>fastapi</code> builtin <code>TestClient()</code>. See Here</p> </li> </ul> <p><p> </p></p>"},{"location":"project/#indian-real-estate-analyser-and-price-prediction","title":"Indian Real Estate Analyser and Price Prediction","text":"<p>Analyse real estate of India. I have scrapped data from 99acres.com. You can also make prediction of property prices by providing you requirements like BHK, AREA, LOCATION and more.</p> <ul> <li> <p>I have made this project end-to-end.</p> </li> <li> <p>Uses <code>streamlit</code> library to show the analysis with beautiful plots from <code>plotly</code>.</p> </li> </ul> <p><p> </p></p>"},{"location":"project/#freeapi-in-python-using-fastapi","title":"FreeAPI in Python using FastAPI","text":"<p>Created similar APIs present in freeapi.app project using Python with FastAPI and MongoDB. I've used Docker so that anyone can easily reproduce this project in their system. I haven't deployed this project but you can use Docker to spawn the APIs in local. You'll get endpoints for todo, quotes, github user and repository and fun endpoints.</p> <p><p> </p></p>"},{"location":"project/#curler","title":"Curler","text":"<p>Curler is a tool which let's you parse <code>curl</code> command into python with ease. Also, you can use that parsed object with <code>requests</code>, <code>httpx</code> like library to make request with <code>curl</code> commands.</p> <p><p> </p></p>"},{"location":"project/#web-scrapping-99acrescom","title":"Web Scrapping 99acres.com","text":"<p>Scrape 99acres.com with a Streamlit Web-UI and download the data in CSV format.</p> <ul> <li>You have to use <code>cURL</code> command to scrape the data. (you can get the curl command from browser's network tab).</li> </ul> <p><p> </p></p>"},{"location":"project/#campusx-resources","title":"CampusX Resources","text":"<p>This project involves gathering data from a course website's HTML structure, followed by developing Python scripts for parsing and extracting essential data. HTTP requests are then made to obtain session resources, with robust testing and data structure maintenance ensuring integrity. Utilizing <code>mkdocs</code> and <code>mkdocs-material</code>, a professional web page is generated and hosted on GitHub Pages via CI/CD with GitHub Actions.</p> <p><p> </p></p>"},{"location":"project/campusx/","title":"CampusX Resources","text":"<p>Get all the resources like Notes and Notebooks provided in CampusX Courses.</p>"},{"location":"project/campusx/#praise-by-nitish-sir","title":"Praise by Nitish Sir","text":""},{"location":"project/campusx/#links-to-resources","title":"Links to Resources","text":"<ul> <li> Downloaded Resources: Resources are uploaded in my GitHub repo as files. You can get all the course's resources like <code>.pdf</code>, <code>.ipynb</code>, <code>.docx</code>, <code>.pptx</code>, <code>.xlsx</code> and <code>.py</code> files.</li> <li> See Resources: Resources are listed on a webpage where you can access the content descriptions for all sessions, with the teacher providing helpful links to enhance your understanding of each session.</li> </ul>"},{"location":"project/canvas-ai/","title":"Canvas AI","text":"<p>Parse and Analyse image containing mathematical expression or problem (it can be in a drawing format) using AI to give a structured response by leveraging Langchain framework. Also created API out of it using FastAPI which has containerized with Docker.</p>"},{"location":"project/canvas-ai/#prompt-examples","title":"Prompt Examples","text":"<p>These are some samples which I have used to verify my AI and be sure that it is working fine. You can see the images and their respective structured responses (responses are written inside code-blocks, just below each image).</p>"},{"location":"project/canvas-ai/#example-1","title":"Example 1","text":"<pre><code>{\n  \"expression\": \"x=3;y=4;x+y=?\",\n  \"result\": \"7\",\n  \"explanation\": \"The sum of x and y is 7\"\n}\n</code></pre>"},{"location":"project/canvas-ai/#example-2","title":"Example 2","text":"<pre><code>{\n  \"expression\": \"(x + y)^2\",\n  \"result\": \"225\",\n  \"explanation\": \"Substitute the values of x and y in the expression and simplify using BODMAS.\"\n}\n</code></pre>"},{"location":"project/canvas-ai/#example-3","title":"Example 3","text":"<pre><code>{\n  \"expression\": \"10 km/hr * (50 meters / 1000 meters/km) * (3600 seconds / 1 hour)\",\n  \"result\": \"18 seconds\",\n  \"explanation\": \"The car is travelling at 10 km/hr and it has to cover 50 meters, so we can calculate the time it takes to cover the distance.\"\n}\n</code></pre> <pre><code>{\n  \"expression\": \"10 km/hr * 50 meters\",\n  \"result\": \"13.89 seconds\",\n  \"explanation\": \"First convert 10 km/hr to meters/second, then divide 50 meters by the speed to get time.\"\n}\n</code></pre>"},{"location":"project/canvas-ai/#example-4","title":"Example 4","text":"<pre><code>{\n  \"expression\": \"\u221a(10\u00b2 + 10\u00b2)\",\n  \"result\": \"14.14 m\",\n  \"explanation\": \"The diagonal of a square forms a right-angled triangle with two sides of the square. Using Pythagoras theorem, we can calculate the diagonal.\"\n}\n</code></pre>"},{"location":"project/canvas-ai/#example-5","title":"Example 5","text":"<pre><code>{\n  \"expression\": \"pi*3^2*12, pi*3^2*5\",\n  \"result\": \"360pi m^3, 45pi m^3\",\n  \"explanation\": \"The tank is a cylinder with radius 3m and height 12m. The volume of a cylinder is pi*r^2*h. The filled water is also cylindrical with radius 3m and height 5m.\"\n}\n</code></pre>"},{"location":"project/credit-risk-modeling/","title":"Credit Risk Modeling - Project","text":""},{"location":"project/credit-risk-modeling/#problem-statement","title":"Problem Statement","text":"<p>Creating a machine learning model that can precisely segregate customers into class of giving credit based on past financial data and other pertinent borrower characteristics including income, credit score, and loan details is the aim. The likelihood that a borrower will fail on a loan should be estimated by the model, allowing it to determine the risk of lending to them. Such models can help financial institutions identify and measure their total risk exposure, set appropriate risk limits, and make informed investment decisions.</p>"},{"location":"project/credit-risk-modeling/#project-workflow","title":"Project WorkFlow","text":""},{"location":"project/credit-risk-modeling/#challenges-faced","title":"Challenges Faced","text":"<p>One of the biggest challenges in credit risk modelling is the limited availability of relevant and reliable data. Credit risk models require historical data on loan performance, default rates, and economic indicators to accurately assess the likelihood of default.</p> <p>Challenges include data availability, data quality, complex modelling, and regulatory compliance.</p> <p>Example: One common challenge faced by financial institutions is obtaining accurate and reliable data for credit risk modelling purposes.</p>"},{"location":"project/credit-risk-modeling/#detail-model-description","title":"Detail Model Description","text":"<ol> <li>There are two datsets. We need to solve the challenges that are faced by the bank during credit lending.</li> <li>First dataset is (i) Internal bank dataset and second dataset is (ii) Civil external dataset.</li> <li>The target variable is Approved_Flag which contain 4 categories ['P2', 'P1', 'P3', 'P4'], segregating the customer into class of giving the credit. P1 being the category where the bank can easier give the credit to that customer whereas P4 being the category where it is not a good idea to give the credit to that customer, as it can increase the NPA accounts(Non-Performing assets) of the bank.</li> <li>There are total 84 columns in two datasets. 26 columns in the first dataset and 62 columns in the second dataset.</li> <li><code>PROSPECTID</code> col is a common column in both the first and second datasets indicating unique customer ID.</li> <li>To find association between numerical and numerical columns we will perform VIF test (Variance Inflation Factor). Reject columns whose p value is greater than a particular threshold.</li> <li>For feature selection, we will perform Chi2-Square test and ANOVA test, since the target column is multi-class categorical column.</li> <li>By checking the p_value of each column w.r.t target variable, we can decide if it's statistically significant or not.</li> <li>Made two models. One without credit score and another with credit score.</li> <li>It is observed that the accuracy of model without credit score feature has dramatically decreases.</li> <li>Without credit score the accuracy is 77% and with credit score the accuracy is 99%.</li> </ol>"},{"location":"project/credit-risk-modeling/#dataset-columns-description","title":"Dataset Columns Description","text":""},{"location":"project/credit-risk-modeling/#bank-dataset","title":"Bank Dataset","text":"Column Description <code>pct_tl_open_L6M</code> Percent accounts opened in last 6 months <code>pct_tl_closed_L6M</code> percent accounts closed in last 6 months <code>Tot_TL_closed_L12M</code> Total accounts closed in last 12 months <code>pct_tl_closed_L12M</code> percent accounts closed in last 12 months <code>Tot_Missed_Pmnt</code> Total missed Payments <code>CC_TL</code> Count of Credit card accounts <code>Home_TL</code> Count of Housing loan accounts <code>PL_TL</code> Count of Personal loan accounts <code>Secured_TL</code> Count of secured accounts <code>Unsecured_TL</code> Count of unsecured accounts <code>Other_TL</code> Count of other accounts <code>Age_Oldest_TL</code> Age of oldest opened account <code>Age_Newest_TL</code> Age of newest opened account"},{"location":"project/credit-risk-modeling/#civil-dataset","title":"Civil Dataset","text":"Column Description <code>time_since_recent_payment</code> Time Since recent Payment made <code>max_recent_level_of_deliq</code> Maximum recent level of delinquency <code>num_deliq_6_12mts</code> Number of times delinquent between last 6 and last 12 months <code>num_times_60p_dpd</code> Number of times 60+ dpd <code>num_std_12mts</code> Number of standard Payments in last 12 months <code>num_sub</code> Number of sub standard payments - not making full payments <code>num_sub_6mts</code> Number of sub standard payments in last 6 months <code>num_sub_12mts</code> Number of sub standard payments in last 12 months <code>num_dbt</code> Number of doubtful payments <code>num_lss</code> Number of doubtful payments in last 12 months <code>recent_level_of_deliq</code> Number of loss accounts in last 12 months <code>CC_enq_L12m</code> Credit card enquiries in last 6 months <code>PL_enq_L12m</code> Personal Loan enquiries in last 6 months <code>time_since_recent_enq</code> Personal Loan enquiries in last 12 months <code>enq_L3m</code> Enquiries in last 6 months <code>last_prod_enq2</code> Lates product enquired for <code>first_prod_enq2</code> First product enquired for <code>MARITALSTATUS</code> Marital Status <code>EDUCATION</code> Education level <code>AGE</code> Age <code>GENDER</code> Sex <code>Time_With_Curr_Empr</code> Time with current Employer <code>CC_Flag</code> Credit card Flag <code>PL_Flag</code> Personal Loan Flag <code>pct_PL_enq_L6m_of_ever</code> Percent enquiries PL in last 6 months to last 6 months <code>pct_CC_enq_L6m_of_ever</code> Percent enquiries CC in last 6 months to last 6 months <code>HL_Flag</code> Housing Loan Flag <code>GL_Flag</code> Gold Loan Flag <code>Approved_Flag</code> Priority levels"},{"location":"project/credit-risk-modeling/#important-notes-from-both-the-dataset","title":"Important Notes From Both The Dataset","text":"<ol> <li>The shape of bank internal dataset of customer is (51336, 26).</li> <li>The shape of civil dataset is (51336, 62)</li> <li>The common column in both datset is <code>PROSPECTID</code> which is unique ID for each customer.</li> <li>The value \"-99999\" in both the datasets are null values.</li> <li>We will remove all the null values if data lost is less than 20% of the total dataset.</li> <li>Total trade lines is total no of accounts of a customer.</li> </ol>"},{"location":"project/credit-risk-modeling/#eda","title":"EDA","text":""},{"location":"project/credit-risk-modeling/#unique-values-in-categorical-columns","title":"Unique values in categorical columns","text":"Column Unique Values MARITALSTATUS Married, Single EDUCATION 12TH, GRADUATE, SSC, POSTGRADUATE, UNDERGRADUATE, OTHERS, PROFESSIONAL GENDER M, F last_prod_enq2 PL, ConsumerLoan, AL, CC, others, HL first_prod_enq2 PL, ConsumerLoan, others, AL, HL, CC Approved_Flag P2, P1, P3, P4 <p>After performing all the statistics tests (i.e. Chi-Square, VIF and ANOVA test), it is found that only 43 columns are important out of 82 columns.</p> <pre><code>[\n  'Age_Newest_TL', 'Age_Oldest_TL', 'Approved_Flag', 'CC_enq_L12m', 'CC_Flag', 'CC_TL', 'EDUCATION',\n  'first_prod_enq2', 'GENDER', 'GL_Flag', 'HL_Flag', 'Home_TL', 'last_prod_enq2', 'MARITALSTATUS',\n  'max_recent_level_of_deliq', 'NETMONTHLYINCOME', 'num_dbt_12mts', 'num_dbt', 'num_deliq_6_12mts',\n  'num_lss', 'num_std_12mts', 'num_sub_12mts', 'num_sub_6mts', 'num_sub', 'num_times_60p_dpd',\n  'pct_CC_enq_L6m_of_ever', 'pct_PL_enq_L6m_of_ever', 'pct_tl_closed_L12M', 'pct_tl_closed_L6M',\n  'pct_tl_open_L6M', 'PL_enq_L12m', 'PL_Flag', 'PL_TL', 'recent_level_of_deliq', 'Secured_TL',\n  'time_since_recent_enq', 'time_since_recent_payment', 'Time_With_Curr_Empr', 'Tot_Missed_Pmnt',\n  'Tot_TL_closed_L12M', 'Unsecured_TL', 'enq_L3m', 'Other_TL',\n]\n</code></pre>"},{"location":"project/credit-risk-modeling/#data-visualization","title":"Data Visualization","text":""},{"location":"project/credit-risk-modeling/#age-distribution-graph","title":"Age Distribution Graph","text":"<ul> <li>The bank is majorly targeting people between 20 to 40.</li> <li>The data distribution of this dataset is majorly spread between the age group of 20 to 40.</li> </ul>"},{"location":"project/credit-risk-modeling/#age-of-oldest-loantrade-line-account-in-months","title":"Age of Oldest loan/Trade Line account (In Months)","text":""},{"location":"project/credit-risk-modeling/#time-since-recent-enquiry-in-months","title":"Time since recent enquiry (In Months)","text":""},{"location":"project/credit-risk-modeling/#credit-score-distribution","title":"Credit Score Distribution","text":"<p>Most of the data distribution of credit score column is spread between 660 and 700 which fall under P2 category and that\u2019s why majorly category in target column is P2 category only.</p>"},{"location":"project/credit-risk-modeling/#90-percentile-monthly-income-distribution","title":"90 percentile Monthly Income Distribution","text":"<p>This column illustrate that the salary income majority of people falls between 20k to 35k. It can be observed that the bank is mainly targeting those people whose is under 50k per month</p>"},{"location":"project/credit-risk-modeling/#marital-status-distribution-graph","title":"Marital Status Distribution Graph","text":"<p>This column indicates that 73.1% of people who are applying for the loan are married.</p>"},{"location":"project/credit-risk-modeling/#education-distribution-graph","title":"Education Distribution Graph","text":"<p>The Graduate and 12th pass population contribute significantly to the dataset</p>"},{"location":"project/credit-risk-modeling/#gender-distribution-graph","title":"Gender Distribution Graph:","text":"<p>Gender wise, the dataset shows that 88.7% who are applying for loan are male or we can say that the bank is targeting male candidate more.</p>"},{"location":"project/credit-risk-modeling/#last-product-enquiry-graph","title":"Last Product Enquiry Graph","text":""},{"location":"project/credit-risk-modeling/#first-product-enquiry-graph","title":"First Product Enquiry Graph","text":""},{"location":"project/credit-risk-modeling/#distribution-of-target-variable-categories","title":"Distribution of Target Variable Categories","text":"<p>60% of the people in the dataset falls under P2 category for loan approval.</p>"},{"location":"project/credit-risk-modeling/#data-visualization-observations","title":"Data Visualization - Observations","text":"<ol> <li>73% people are married people in the dataset.</li> <li>This datset contains have 88% of people as men who are likely to taken loan from the bank.</li> <li>Graduate people have more likelihood of taking or applying for loans. Banks also have more likelihood of approving of loans to the graduate or educated people.</li> <li>Previous loans taken by the people in this dataset is other loan or consumer loans (such as furniture loan, fridge loan etc).</li> <li>Most of the people in the dataset flows under P2 category for loan approval.</li> </ol>"},{"location":"project/credit-risk-modeling/#minimum-maximum-and-median-value-of-credit_score-for-each-category","title":"Minimum, Maximum and Median value of <code>Credit_Score</code> for each category","text":"<p>The min and max credit score is P3 category is 489 and 776 respectively. This range indicates that for P3 category creates a big ambiguity for the model to predict the output accurately. For P1 and P2 categories, it is easier for the model to predict as it range from (701, 809) and (689 and 700) respectively.</p>"},{"location":"project/credit-risk-modeling/#minimum-maximum-and-median-value-of-age-for-each-category","title":"Minimum, Maximum and median value of <code>AGE</code> for each category","text":"<p>Min age for all the category is 21. Max age varies from 63 to 67. It can be observed that for P1 category the median age is higher as compare to other categories and as the category decrease median age also decreses.</p>"},{"location":"project/credit-risk-modeling/#maximum-and-median-value-of-age-oldest-trade-line-accounts-for-each-category","title":"Maximum and median value of Age Oldest Trade Line accounts for each category","text":""},{"location":"project/credit-risk-modeling/#maximum-and-median-value-of-time-with-current-enquiry-accounts-for-each-category","title":"Maximum and median value of Time with current enquiry accounts for each category","text":""},{"location":"project/credit-risk-modeling/#observation-of-numerical-and-categorical-cols-wrt-target-variable-ie-approved_flag","title":"Observation of numerical and categorical cols w.r.t target variable i.e. (Approved_Flag)","text":"<ol> <li>P1 category range is (701-809)</li> <li>P2 category range is(669-700)</li> <li>P3 category range is (489-776)</li> <li>P3 category of target variable are the most ambiguous category. This can be observed by looking at the credit score min and max value for P3 category which range from 489 to 776, whereas in case of P2 it's ranges from 669 to 701.</li> <li>Due to the most ambiguous category i.e. P3, during the predict also, the accuracy of the model is significantly decreases due to the most ambiguous category.</li> <li>The median age who are getting P1 category loan are bit older than other categories. For eg. median age for P1 category is 40 whereas for P2 category it is 33 and for P3 category it is 31. Therefore it can be assumed that as the age increases, loan approval becomes easier.</li> </ol>"},{"location":"project/credit-risk-modeling/#model-training","title":"Model Training","text":"<p>We used ensemble techniques (bagging and boosting) to train the model. Mainly we used RandomForestClassifier and XGBoostClassifier for classification. However, it is observed that XGBoost classifier has better accuracy as compare to RandomForest classifier. With accuracy 99% XGBoost is the best ML algorithm for the dataset with credit score feature is included. However, when credit score feature is excluded, there is a significant drop in the accuracy(76%) because the P3 category is most ambiguous category, the accuracy of the model is significantly decreases.</p>"},{"location":"project/credit-risk-modeling/#classification-report-of-randomforest","title":"Classification Report of <code>RandomForest</code>","text":""},{"location":"project/credit-risk-modeling/#using-credit_score-feature","title":"Using <code>Credit_Score</code> feature","text":"precision recall f1-score support P1 0.94 1.00 0.97 1224 P2 1.00 1.00 1.00 6397 P3 1.00 0.95 0.98 1595 P4 1.00 1.00 1.00 1309 accuracy 0.99 10525 macro avg 0.99 0.99 0.99 10525 weighted avg 0.99 0.99 0.99 10525"},{"location":"project/credit-risk-modeling/#without-credit_score-feature","title":"Without <code>Credit_Score</code> feature","text":"precision recall f1-score support P1 0.82 0.70 0.75 1224 P2 0.79 0.93 0.86 6397 P3 0.45 0.21 0.28 1595 P4 0.75 0.70 0.72 1309 accuracy 0.77 10525 macro avg 0.70 0.63 0.65 10525 weighted avg 0.74 0.77 0.74 10525"},{"location":"project/credit-risk-modeling/#classification-report-of-xgboost","title":"Classification Report of <code>XGBoost</code>","text":"Class P1 P2 P3 P4 Precision 0.813 0.826 0.434 0.772 Recall 0.788 0.912 0.305 0.698 F1 Score 0.800 0.866 0.358 0.733"},{"location":"project/credit-risk-modeling/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Using <code>skopt.BayesSearchCV</code>, we got the best parameter for our XGBoost model. The parameters are:</p> <pre><code>{\n  \"alpha\": 10,\n  \"colsample_bytree\": 0.9,\n  \"learning_rate\": 1.0,\n  \"max_depth\": 3,\n  \"n_estimators\": 100,\n  \"num_classes\": 4,\n  \"objective\": \"multi:softmax\",\n}\n</code></pre> <p>With these parameters, accuracy increases by 1% when the model is trained without <code>Credit_Score</code> feature.</p>"},{"location":"project/others/","title":"Other Projects","text":"<p>My small projects, which you may either find useful or see to learn new concepts.</p> <ul> <li> <p> Scrape Hockey Data</p> <p>Scrape Hockey data from Hockey website. Already scraped data present on Kaggle.</p> </li> <li> <p> Resume Generator</p> <p>Generate your Resume/CV using JSON data.</p> </li> <li> <p> Md Badges CLI</p> <p>CLI tool to get SimpleIcons badges in multiple format, written in Rust.</p> </li> <li> <p> Web Scrape 99acres.com</p> <p>Scrape 99acres.com with python using curl and export data as CSV.</p> </li> <li> <p> Personal Website</p> <p>Website from markdown files using mkdocs-material.</p> </li> <li> <p> Election 2024 Dashboard</p> <p>A live streamlit dashboard to track the seats share of political partries. Data is being fetched from ECI website.</p> </li> <li> <p> Custom YouTube Theme</p> <p>A Distraction Free Stylus CSS theme for YouTube.com.</p> </li> <li> <p> Markdown Badge</p> <p>CLI tool to get simpleicon badges for markdown file.</p> </li> <li> <p> Attendance System</p> <p>Attendance System with MongoDB and Streamlit.</p> </li> <li> <p> IPL APIs</p> <p>IPL API using Flask framework and ipl dataset.</p> </li> <li> <p> Ineuron Projects Docs Generator</p> <p>A automated way to create project's document for Ineuron Portal Projects.</p> </li> <li> <p> Family Tree</p> <p>Create family tree using python only and export it in JSON/Mermaid format.</p> </li> <li> <p> Easy Analysis</p> <p>A python package to perform Data Analysis easily.</p> </li> </ul>"},{"location":"project/spotify-analysis/","title":"Spotify Analysis","text":"<p>Analyse your Spotify Streaming data and get some insights from it like whom &amp; when you listen your favorite Tracks, Artists, Playlists or Albums.  Created Jupyter Notebook where you can upload your data and get the analysis by running the notebook. </p> <p>How to get my Spotify data?</p> <p>You can request your Spotify data from official website.</p> <p>Know more about your personal data on Spotify.</p> Spotify data file structure <pre><code>spotify-data\n\u251c\u2500\u2500 Read_Me_First.pdf         # Introductory document\n\u251c\u2500\u2500 Follow.json               # Data about user's followers\n\u251c\u2500\u2500 Identifiers.json          # Identification information (\ud83d\ude45 Do Not Share!)\n\u251c\u2500\u2500 Identity.json             # User identity details (\ud83d\ude45 Do Not Share!)\n\u251c\u2500\u2500 Inferences.json           # Inferred data from user activity\n\u251c\u2500\u2500 Marquee.json              # Marquee-related information\n\u251c\u2500\u2500 Payments.json             # Payment details (\ud83d\ude45 Do Not Share!)\n\u251c\u2500\u2500 Playlist1.json            # Details about a specific playlist\n\u251c\u2500\u2500 SearchQueries.json        # User's search queries\n\u251c\u2500\u2500 StreamingHistory0.json    # User's Streaming history part 1\n\u251c\u2500\u2500 StreamingHistory1.json    # User's Streaming history part 2\n\u251c\u2500\u2500 Userdata.json             # General user data (\ud83d\ude45 Do Not Share!)\n\u2514\u2500\u2500 YourLibrary.json          # User's Spotify library details like likes, albums, artists, etc.\n</code></pre>"},{"location":"project/spotify-analysis/#philosophy","title":"Philosophy","text":"<p>I want to analyse my Spotify's Streaming History in a way from which I can know my listening pattern over the time. The way Spotify give the Spotify Wrapped at the end of every year.</p>"},{"location":"project/spotify-analysis/#the-dashboard","title":"The Dashboard","text":"<p>Currently not developed!</p> <p>If you want to join for this contact me on my socials  </p> <p>There has been a dashboard (using  Streamlit) where other users can upload their <code>StreamingHistory.json</code> files to see analysis on their history.</p>"},{"location":"project/spotify-analysis/#some-awesome-insights","title":"Some Awesome Insights","text":"<p>To gather insights from <code>json</code> files I've used  <code>polars</code> and their builtin <code>.plot</code> accessor which uses <code>hvplot</code> library under-the-hood.</p> <p> See the Jupyter Notebooks to see all the insights.</p> Reference ShortCode Description T/A Track/Artist T/As Track/Artist(s) P/A Playlist/Album P/As Playlist/Album(s) <p>These shortcodes used below for better readability. </p>"},{"location":"project/spotify-analysis/#using-streaminghistoryjson","title":"Using <code>StreamingHistory.json</code>","text":"<p>File Info</p> <p>Contains User's Streaming History with <code>trackName</code>, <code>artistName</code>, <code>msPlayed</code> and <code>endTime</code>. \u00a0 </p> <ul> <li> Top T/As in whole dataset.</li> <li> Top T/As in each month.</li> <li> Monthly most listend Tracks and Artists.</li> <li> First day when T/A was played.</li> <li> No.of distinct T/As listened in each month/year.</li> <li> A T/A streaming in barplot (which shows how you stream that during time-to-time).</li> <li> Which daytime user listen most and whom.</li> <li> Tracks which have listened most times in a day.</li> <li> Tracks streaming streak (by day/week).</li> <li> T/As which only played once</li> <li> Dates when user does not played any track</li> </ul>"},{"location":"project/spotify-analysis/#using-playlistjson","title":"Using <code>Playlist.json</code>","text":"<p>File Info</p> <p>Contains User's Created Playlist Data with all the tracks added in the playlist.</p> <ul> <li> No. of T/As and Albums in each Playlist</li> <li> Playlist MinutesPlayed</li> <li> Most streamed P/As</li> <li> Check any Track present in multiple Playlists</li> <li> Streaming timline of P/As (with <code>plot.line()</code>)</li> <li> Playlist-wise top T/As</li> </ul>"},{"location":"project/spotify-analysis/#using-yourlibraryjson","title":"Using <code>YourLibrary.json</code>","text":"<p>Currently Working!</p>"},{"location":"project/spotify-analysis/#using-marqueejson","title":"Using <code>Marquee.json</code>","text":"<p>Currently Working!</p>"},{"location":"project/yt-comment-sentiment/","title":"YouTube Comment Sentiment - End-to-End Project","text":"<p>You can read about this project's each segments in details in their respective pages at ML Side, Backend Side and Frontend Side.</p> <p>After reading above pages you'll get to know about:</p> <ul> <li>What had I used?</li> <li>How did I done?</li> <li>I have also shared some common mistakes you could encounter while building this.</li> </ul>"},{"location":"project/yt-comment-sentiment/#frontend-screenshots","title":"Frontend Screenshots","text":""},{"location":"project/yt-comment-sentiment/backend/","title":"YT Comment Sentiment - Backend Side","text":"Technology Description A Python library for building and training machine learning models. A collaboration platform for machine learning, hosting data and MLflow models. A platform to manage the ML lifecycle, including model tracking and deployment. A modern web framework for building APIs with Python, known for its speed. A Python library for data validation. Used to validate API data. A testing framework for Python, used to test the FastAPI application. An API to access and manage YouTube video data, including comments. A cloud platform for hosting APIs, websites, and applications."},{"location":"project/yt-comment-sentiment/backend/#what-i-followed-to-know","title":"What I followed to know?","text":"<p>Important</p> <ol> <li>As I am learning Python, Data Science and Machine Learning for more than 3 years. I don't have to look around to    learn new things to build this. This part is kind of easy for me.</li> <li>But as I said earlier, the documentations and ChatGPT is most important resources you can onto. </li> </ol> <ul> <li>Need to get an API key from Google Developer Console to interact with YouTube Data API.</li> <li>Need to create an account on DagsHub to store/track MLFlow experiments and models.</li> <li>Created a DVC pipeline to run the MLFlow experiments seemlessly using <code>dvc repro</code> command.</li> <li>After creating the FastAPI app, I've used <code>pytest</code> to test it and also setup a <code>pre-commit</code> for it.</li> <li>Deployment on render.com.</li> </ul>"},{"location":"project/yt-comment-sentiment/backend/#what-type-of-problems-i-have-faced","title":"What type of problems I have faced?","text":""},{"location":"project/yt-comment-sentiment/backend/#rendercom","title":"Render.com","text":"<ul> <li> <p>As I have used <code>uv</code> to manage my project but render.com doesn't support <code>uv</code> out-of-the-box so I have used <code>pip</code> to   use <code>uv</code> for dependencies installation.</p> <pre><code>pip install uv &amp;&amp; uv sync --extra=backend --compile-bytecode\n</code></pre> </li> <li> <p>Also, render.com only serve apps on port under <code>$PORT</code> env (which <code>10000</code> most of the times) so make sure to   explicitly provide while running app through <code>uvicorn</code> or <code>fastapi-cli</code> CLI.</p> <pre><code># For uvicorn\nuvicorn run --host 0.0.0.0 --port $PORT backend.app:app\n\n# For fastapi-cli\nfastapi run --host 0.0.0.0 --port $PORT backend/app.py\n</code></pre> </li> </ul>"},{"location":"project/yt-comment-sentiment/backend/#docker","title":"Docker","text":"<ul> <li> <p>I am using <code>wordcloud</code> to create a plot in a FastAPI route. While building docker image FROM <code>python:3.11-slim</code> image, I am getting error because <code>wordcloud</code> package needs <code>gcc</code> package to build wheels. So you need to explicitly install <code>gcc</code> before install <code>wordcloud</code> as python package.</p> <pre><code># Install gcc for wordcloud\nRUN apt-get update &amp;&amp; apt-get install -y gcc &amp;&amp; apt-get clean\n\n# Now install project dependencies including wordcloud\n# ...\n</code></pre> </li> <li> <p>Also use multi-stage builds in <code>Dockerfile</code> to reduce the image size.   See <code>uv</code> docs.</p> </li> </ul>"},{"location":"project/yt-comment-sentiment/frontend/","title":"YT Comment Sentiment - Frontend Side","text":"Technology Description A fast and efficient package manager for JavaScript projects, known for its disk space usage. A customizable component library for building elegant UIs in modern web applications. A utility-first CSS framework for creating custom designs quickly and efficiently. A next-generation frontend build tool for blazing-fast development and hot module replacement. A progressive JavaScript framework for building user interfaces and single-page applications."},{"location":"project/yt-comment-sentiment/frontend/#what-i-followed","title":"What I followed?","text":"<p>Important</p> <ol> <li>Documentation is the most important resource for your learning and I followed it thoroughly.</li> <li>Took help of  ChatGPT to solve bugs and asked questions related to these frameworks/tools.</li> </ol> <ul> <li>Followed only one YouTube video to learn  VueJs.</li> <li>Used shadcn/vue as components library. Just followed the docs.</li> <li>Whole <code>build_frontend.yaml</code> Github Action workflow written by ChatGPT, isn't it amazing .</li> </ul> <p>Although, I didn't have much knowledge of Vue, Vite and ShadCN like frameworks/tools but their documentations and ChatGPT helped me so much that I am able to learn, build, diagnose and deploy the frontend in almost 2-3 days.</p> <p>Yes, some credits goes my past knowledge of programming because that's why I able to figure out how do things works and how handle them by doing right things.</p>"},{"location":"project/yt-comment-sentiment/frontend/#what-type-of-problems-i-have-faced","title":"What type of problems I have faced?","text":"<p>\u0939\u0947 \u092d\u0917\u0935\u093e\u0928! While learning and working on this frontend project, sometime I get messed up with very silly typo mistakes in JavaScript.</p> <p>The things you are building happily in local will totally change when you are trying to deploy it and I have faced this too .</p>"},{"location":"project/yt-comment-sentiment/frontend/#vuejs","title":"VueJs","text":"<ul> <li>How to emit data from Child component to Parent component in VueJs?<ul> <li>GFG Article</li> <li>Vue School Article</li> <li>Also take a look into official docs to know best practices for this.</li> </ul> </li> <li>How to create and work with forms?</li> <li>How to use <code>v-model</code>?</li> </ul>"},{"location":"project/yt-comment-sentiment/frontend/#vite","title":"Vite","text":"<ul> <li>While deciding how to setup proxy in Docker because I've setup it using Vite and it works like charm in local but   while writing <code>Dockerfile</code> I am able to figure out the solution.<ul> <li>Proxy is used in VueJs YouTube video and thats why I have followed it.</li> <li>See this docs</li> </ul> </li> <li>While   handle environment variables in <code>nginx.conf</code>   file in Docker environment. I have used ChatGPT read articles but didn't get to solution.</li> <li>How to work with different <code>.env</code> files in Vite.</li> </ul>"},{"location":"project/yt-comment-sentiment/ml/","title":"YT Comment Sentiment - ML Side","text":""},{"location":"project/yt-comment-sentiment/ml/#data-handling-steps","title":"Data Handling Steps","text":""},{"location":"project/yt-comment-sentiment/ml/#data-gathering","title":"Data Gathering","text":"<p>For comment's sentiment prediction we need a data which has Comments and its corresponding Sentiment. And for that we have used dataset used in the course.</p>"},{"location":"project/yt-comment-sentiment/ml/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li>Preprocess by lowercasing the words.</li> <li>Cleaned the texts by removing stopwords and punctuations.</li> <li>Applied lemmetization using <code>WordNetLemmatizer</code>.</li> <li>Then, stemming using <code>PorterStemmer</code>.</li> </ul>"},{"location":"project/yt-comment-sentiment/ml/#eda","title":"EDA","text":"<ul> <li>Checked target column's distribution.</li> <li>Performed intensive EDA by creating many additional features using comment's chars, words and sentences.</li> <li>Generated wordcloud to see different sentiment's frequent words.</li> </ul> <p> EDA Notebook</p>"},{"location":"project/yt-comment-sentiment/ml/#model-building-steps","title":"Model Building Steps","text":"<ul> <li> <p>Comment Vectorization <sup>[text-to-vec]</sup></p> <ul> <li>Before transforming performed some basic preprocessing steps on comments like lowercasing, lemmetization and   stemming to make vectors more consistent.</li> <li>Evaluated multiple vectorization methods like BOW and TF-IDF.</li> <li>Also, performed hyperparameter tuning on vectorization methods by tuning parameters like <code>n_gram</code> and   <code>max_features</code>.</li> <li>Chosen TF-IDF Vectorizer model to transform comment texts into vectors which passes into ML Model.</li> </ul> </li> <li> <p>Feature Engineering</p> <ul> <li>Created multiple new features using comments' texts like word count, etc. which help the model to learn the comments' sentiment better.</li> </ul> </li> <li> <p>Hyperparameter Tuning</p> <ul> <li>Used Bayesian Optimization Technique to perform hyperparameter tuning on models.</li> <li>Tuned models on their most important parameters.</li> <li>Logged best parameter of each models with MLFlow to evaluate further.</li> </ul> </li> <li> <p>Evaluation</p> <ul> <li>Used MLFlow UI to check which model is performing well on the dataset.</li> <li>Evaluated on:<ol> <li>Overall <code>accuracy_score</code></li> <li>Different sentiment's <code>r1_score</code>, <code>precision</code> and <code>f1_score</code> value.</li> </ol> </li> </ul> </li> </ul>"},{"location":"project/yt-watch-history/","title":"YouTube Watch History","text":""},{"location":"project/yt-watch-history/#overview","title":"Overview","text":"<p>A streamlit app where you can upload your YouTube Watch History Data to see insights on your viewing pattern. Your data will go through a ML Model which predicts the ContentType of each video your have watch. The app fetches more details of each video through YouTube API. There is also a Channel Recommender System in the app which recommend you similar channels on the basis of channel's video titles and tags they had used. </p>"},{"location":"project/yt-watch-history/#explanation","title":"Explanation","text":"<ol> <li> <p>Project Introduction: Hello, I am Anshul Raj Verma and I am exited to share my end-to-end Machine Learning    project where I've used FastAPI, Streamlit, MongoDB and Docker as tech stack. Also this is version 2 of the    project because the version 1 got very complicated and its hard to modify and refactor the codes there that's    why created new version 2 where I am trying keep better attention on project architecture.</p> </li> <li> <p>Project Overview: This project consists a streamlit app where you can upload your YouTube Watch History Data    to see insights on your viewing pattern. Your data will go through a ML Model which predicts the ContentType of    each uploaded video. The app fetches more details of each video through YouTube API. There is also a Channel    Recommender System in project which recommend you similar channels on the basis of channel's videos title and tags    they had used.</p> </li> <li> <p>Components of Project: The project is divided into three major components Backend API, ML &amp; Frontend.</p> <ul> <li>Backend API: This is a FastAPI app which interacts with MongoDB database where YouTube videos details were   stored and it also fetches YouTube videos details from official YouTube Data API (for this you requires the   <code>API_KEY</code>).</li> <li>ML: Here the code for ML Model were present through they will get trained and do predcitions on user's   uploaded data after some preprocessing. The ML Models get served as API through a FastAPI app.</li> <li>Frontend: Here all the above components meets and work together to show awesome insights on user's uploaded   data. This is a streamlit web app where users can upload their watch history data and see insights. Here above API   services were called to fetch videos details from official YouTube API, to store data in database, to make   predcitions using ML Models, to recommend channels and etc.</li> </ul> </li> <li> <p>Project Architecture: I have created some diagrams to showcase the project's architecture and for that created a    dedicated page.</p> </li> <li> <p>Containerization with Docker: All the three components (Backend API, ML &amp; Frontend) of project were    containerized using docker and used <code>docker compose</code> to wrap all three images in a container.    <code>uv</code> is used to install packages in docker images. <code>mongodb</code> image is used as database for    the project.</p> </li> <li> <p>More in Project: As I am learning MLOps concepts I am trying to implement them in this project because planning    to add DVC and MLFlow into the project.</p> </li> </ol>"},{"location":"project/yt-watch-history/channel_reco/","title":"Channel Recommender System","text":"<p>I've built <code>contentType</code> prediction pipeline using videos titles. Now, I am thinking that what if I can recommend similar channels on the basis of their subscribed channels. I can recommend channels using channel's videos titles and videos tags.</p>"},{"location":"project/yt-watch-history/channel_reco/#training-pipeline","title":"Training Pipeline","text":""},{"location":"project/yt-watch-history/channel_reco/#data-ingestion-and-preprocessing","title":"Data Ingestion and Preprocessing","text":"<ul> <li>System import data from two types of sources <code>\"db\"</code> (database) and <code>\"file\"</code> (local file).</li> <li>Next, I validate the data on the basis of columns present in the data.</li> <li>Then data goes for preprocessing step, during this step data is being clean and all the required feature has been extracted from it using  library.</li> </ul>"},{"location":"project/yt-watch-history/channel_reco/#model-overview","title":"Model Overview","text":"<p>As you know I am working with videos title and tags which are are textual data so I've used <code>TfidfVectorizer</code> (for text to vector conversion). I've used two <code>TfidfVectorizer</code> for each column (<code>title</code> and <code>tags</code>) and then used <code>ColumnTransformer</code> to create a (sort of) chain transformation step.</p> channel_reco/steps/model.py<pre><code>def get_vectorizer() -&gt; ColumnTransformer:\n    title_transformer = TfidfVectorizer(\n        max_features=7000,\n        ngram_range=(1, 2),\n        preprocessor=preprocess_title,# (1)!\n        stop_words=\"english\",\n    )\n    tags_transformer = TfidfVectorizer(\n        max_features=5000,\n        ngram_range=(1, 2),\n        preprocessor=preprocess_tags,# (2)!\n        stop_words=\"english\",\n    )\n    transformer = ColumnTransformer(\n        [\n            (\"title_trf\", title_transformer, \"title\"),\n            (\"tags_trf\", tags_transformer, \"tags\"),\n        ]\n    )\n    return transformer\n</code></pre> <ol> <li>Function to preprocess texts of <code>\"title\"</code> column.</li> <li>Function to preprocess texts of <code>\"tags\"</code> column.</li> </ol>"},{"location":"project/yt-watch-history/channel_reco/#data-to-export","title":"Data to Export","text":"<p>Now, I've successfully built the pipeline and trained the system but there comes a question that how to reccommend a channel and for that I've to export some essential data like the vectorized array (vectorized videos titles and tags) with its metadata like <code>channelId</code> and <code>channelTitle</code>. To tackle this thing I've combine these data and created a <code>pl.DataFrame</code> and then export it as <code>parquet</code> format.</p> channel_reco/steps/training.py<pre><code>def training(\n    input_data: Literal[\"db\", \"file\"],\n):\n    # Extra code hidden...\n\n    df = preprocess_data(df)\n    transformer = get_vectorizer()\n    transformed_data = transformer.fit_transform(df.to_pandas())\n\n    # Combine transformed_data, channelId, channelTitle as DataFrame\n    title_tags_trf_df = df.select(\"channelId\", \"channelTitle\").with_columns(\n        pl.lit(transformed_data.toarray()).alias(\"transformed_data\")  # type: ignore\n    )\n\n    dump_object(transformer, CH_RECO_TRANSFORMER_PATH) # (1)!\n    # Export dataframe as parquet format for lesser size\n    title_tags_trf_df.write_parquet(CH_RECO_TRANSFORMER_DATA_PATH)\n</code></pre> <ol> <li>\ud83d\udc40 Here, I'm exporting <code>ColumnTransformer</code> object.</li> </ol> <p>What is <code>parquet</code> format?</p> <p>Parquet is a columnar storage format that provides compression benefits and is particularly suitable for analytical queries.</p>"},{"location":"project/yt-watch-history/channel_reco/#prediction-pipeline","title":"Prediction Pipeline","text":"<p>I'm calling this step as Prediction Pipeline \ud83d\ude42 because it doesn't feels good to call Reccommendation Pipeline \ud83d\ude1e.</p> <p>Here, I've to get any channel's data (videos titles and tags) to transform using stored <code>ColumnTransformer</code> object. After, transforming the data I've calculated <code>cosine_similarity</code> between new channel's vector and vector which I have stored on training step and from that whichever channel has greater similarity value is being reccommended to the user \ud83e\udd29.</p> channel_reco/steps/pipeline.py<pre><code>def prediction(data: pl.DataFrame):\n    # Extra code hidden...\n\n    transformer: ColumnTransformer = load_object(CH_RECO_TRANSFORMER_PATH)\n    transformer_data = pl.read_parquet(CH_RECO_TRANSFORMER_DATA_PATH)\n\n    transformed_data = transformer.transform(data.to_pandas())\n    similarity = cosine_similarity(\n        np.array(transformer_data[\"transformed_data\"].to_list()),\n        transformed_data.toarray(),  # type: ignore\n    )\n    return transformer_data.select(\"channelId\", \"channelTitle\").with_columns(\n        pl.lit(np.ravel(similarity)).alias(\"similarity\"),\n    )\n</code></pre>"},{"location":"project/yt-watch-history/channel_reco/#extra","title":"Extra","text":"<p>Reccommendation System Summary</p> <ul> <li>Ingesting data from database or local file. I had made API endpoint to fetch data from datbase.</li> <li>Using  Polars library for data manipulation.</li> <li>This recommender system trained on Youtube Channel's Videos titles and tags which means it recommend on the basis of the channel's videos contents like title and tags.</li> <li>Used <code>TfidfVectorizer</code> for text-to-vec conversion.</li> </ul>"},{"location":"project/yt-watch-history/channel_reco/#provide-weights-to-vectorizer","title":"Provide Weights to Vectorizer","text":"<p>Previously, I thought that I can add a functionality to provide weights to each vectorizer (<code>TfidfVectorizer</code>) to make the system more robust and I had achieved it (See Notebook) but not feels good while actual implementation because it creates so much objects to store and makes the prediction (recommendation) step complex.</p> <p>I have to store each vectorizer, vectorized data (title and tags) and the metadata (<code>channelId</code> and <code>channelTitle</code>) too which this pipeline complex and hard to keep track of objects.</p>"},{"location":"project/yt-watch-history/channel_reco/#adding-more-features","title":"Adding More Features","text":"<p>I have tried to add more features like <code>categoryName</code> (channel owner provide category of the video while uploading) and <code>contentTypePred</code> (a feature I have predicted using ML) but I found it difficult to implement and it doesn't show much effect while reccommending. That's why I thought a different idea to implement this.</p> <p>I can filter the reccommended channels on the basis of <code>categoryName</code> and <code>contentTypePred</code> in the frontend part (yeah this not the right way of doing this but I'll think about it later).</p> <ul> <li> Code on GitHub</li> <li> Pipeline in Notebook</li> </ul> <p>\ud83d\ude4f Thank You for reading this. I am Anshul Raj Verma and I am a Data Scientist.</p>"},{"location":"project/yt-watch-history/ctt/","title":"ContentType Prediction","text":"<p>I am building the ContentType Prediction System from scratch, this it is more robust, flexible and scalable.</p> <p> </p> ML System Diagram for \"ContentType Prediction System\" <ul> <li>I have created custom <code>sklearn</code> transformers to transform the datasets. </li> <li>I also implemented the model monitoring part using abstraction classes. I do monitoring using <code>mlfow</code>.</li> <li>I also write scripts for the reference about how to monitor, train and predict models, through this I want to give you some idea that how does this pipeline works. </li> </ul>"},{"location":"project/yt-watch-history/ctt/#custom-transformers-using-sklearn","title":"Custom Transformers using <code>sklearn</code>","text":"<p>Yesterday, I have learned how to create a custom transformer using <code>sklearn</code> API.</p> <p>I find it very useful and and ver elegant way to create pipelines with it. They are very simple to use and implement when you get it right.</p> A high level info about custom transformers. <ul> <li>Create a class which inherit two <code>sklearn</code> classes from <code>sklearn.base</code> module <code>TransformerMixin</code> and <code>BaseEstimator</code>.</li> <li>Now, you have to define <code>fit</code> and <code>transform</code> methods in your class.</li> <li>And you are ready to use this custom transformer.</li> </ul> <p>Remember this is not a fully pleged custom class because there are numerous things you have to keep in mind while making a custom transformer using <code>sklearn</code> API.</p> <p>References</p> <ul> <li> Professional Preprocessing with Pipelines in Python - NeuralNine</li> <li> Developing a Custom Scikit-learn Transformer and Estimatior - Ploomber</li> <li> Developing custom scikit-learn transformers and estimators</li> </ul>"},{"location":"project/yt-watch-history/ctt/#monitoring-with-mlflow","title":"Monitoring with <code>mlflow</code>","text":"<p>1st Draft</p> <p>I have think a custom monitoring pipeline where you pass the model and params with the training and testing set. Then it calculate the score and log it with <code>mlflow</code>.</p> <p> </p> MLFlow UI showing scores of multiple trained models on bar chart."},{"location":"project/yt-watch-history/v2-architecture/","title":"Architecture for Version 2","text":""},{"location":"project/yt-watch-history/v2-architecture/#application-interaction","title":"Application Interaction","text":"<ul> <li>Architecture of <code>backend</code>, <code>ml</code> and <code>frontend</code>.</li> <li>How do <code>backend</code>, <code>ml</code> and <code>frontend</code> interacts?</li> </ul> Code for Diagram <pre><code>styleMode plain\n\nContainer 1 [icon: docker, color: blue] {\nML Models [icon: machine-learning] {\n    Conetnt Type Predictor [icon: machine-learning]\n    Channel Recommender System [icon: machine-learning]\n}\nML Models API [icon: fastapi, color: green]\n}\n\nYouTube Data v3 API [icon: youtube, color: red]\nContainer 2 [icon: docker, color: blue] {\nMongoDB [icon: mongodb, color: green] {\n    Video Details [icon: database]\n    Channel Details [icon: database]\n}\nBackend API [icon: fastapi, color: green]\n}\n\nContainer 3 [icon: docker, color: blue] {\nFrontend [icon: streamlit, color: red]\n}\n\nUser [icon: user]\n\nUser &gt; Container 3\nContainer 3 &gt; Backend API, ML Models API\nBackend API &gt; MongoDB\nBackend API &gt; YouTube Data v3 API\nML Models API &gt; ML Models\n</code></pre>"},{"location":"project/yt-watch-history/v2-architecture/#frontend-and-backend-api-interaction","title":"Frontend and Backend API Interaction","text":"<ul> <li>What happens when user upload their <code>watch-history.json</code> data to see analysis?</li> <li>More and better data retrival for analysis.</li> <li>Use of YouTube Data v3 API to fetch more details of watched videos.</li> </ul> <pre><code>sequenceDiagram\n    participant Frontend as Frontend\n    participant API as API\n    participant ChannelDetails as ChannelDetails\n    participant VideoDetails as VideoDetails\n\n    Frontend -&gt;&gt; API: Request all IDs from User's watch history\n\n    Note over Frontend, API: User's watch history IDs\n\n    activate Frontend\n        API -&gt;&gt; ChannelDetails: Exclude IDs already available in DB\n        ChannelDetails --&gt;&gt; API: IDs not in Database\n        API --&gt;&gt; Frontend: Filtered IDs\n    deactivate Frontend\n\n    Note over Frontend, API: Filtered IDs\n\n    Frontend -&gt;&gt; API: Fetch video details of filtered IDs\n\n    Note over Frontend, API: Video details request\n\n    activate API\n        API -&gt;&gt; VideoDetails: Store fetched details\n        VideoDetails --&gt;&gt; API: Stored!\n        API -&gt;&gt; ChannelDetails: Store channel's videos ID from fetched data\n        ChannelDetails --&gt;&gt; API: Stored!\n        API --&gt;&gt; Frontend: Video Details\n    deactivate API\n\n    Note over Frontend: Show graphs using fetched data</code></pre> Code for diagram <pre><code>styleMode plain\n\nFrontend [icon: streamlit, color: red]\nAPI [icon: fastapi, color: green]\nChannelDetails [icon: mongodb, color: purple, label: Channel Videos ID DB]\nVideoDeatils [icon: mongodb, color: orange, label: Video Details DB]\nFrontend &gt; API: All IDs from User's watch history\n\nactivate Frontend\nAPI &gt; ChannelDetails: Exclude IDs already available in DB\nChannelDetails &gt; API: IDs not in Database\nAPI &gt; Frontend: Filtered IDs\ndeactivate Frontend\n\nFrontend &gt; API: Fetch video details of filtered IDs\n\nactivate API\nAPI &gt; VideoDeatils: Store fetched details\nVideoDeatils &gt; API : Stored!\nAPI &gt; ChannelDetails: Store channel's videos ID from fetched data\nChannelDetails &gt; API : Stored!\nAPI &gt; Frontend: Video Details\ndeactivate API\n</code></pre>"},{"location":"project/yt-watch-history/v2-architecture/#ml-model-working","title":"ML Model Working","text":"<ul> <li>How does ML Model inference through API?</li> <li>How to train ML Model on my custom data?</li> </ul> Code for diagram <pre><code>direction right\nstyleMode plain\n\nuser [icon: user, shape: diamond, label: User]\ninputData [icon: data, shape: cylinder, label: Input Data]\nmodel [icon: machine-learning, shape: document, label: ML Model]\nmlApi [icon: fastapi, label: ML API]\nmodelOnWeb [icon: globe, shape: oval, label: Model On Web]\nprediction [icon: graph, label: Model Prediction]\n\nuser &gt; inputData\ninputData &gt; mlApi\nmlApi &gt; model: Exists\nmlApi &gt; modelOnWeb: Not Exists\nmodelOnWeb &gt; model: Download and Store\nmodel &gt; prediction\nprediction &gt; user\n</code></pre>"},{"location":"project/yt-watch-history/v2-architecture/#user-data-flow","title":"User Data Flow","text":"<ul> <li>Data manipulation after user uploads their data on application.</li> <li>Use of YouTube Data v3 API to fetch more details about watched videos.</li> <li> <p>Leveraging the power of Python libraries like Polars for data manipulation. We can use Pandas also.</p> </li> </ul> <p></p> Code for diagram <pre><code>styleMode plain\n\nwatchHistory [icon: user, color: blue] {\n    title_ string\n    titleUrl string\n    time datetime\n    subtitle dict\n}\n\npreprocessedWatchHistory [icon: pandas, color: blue] {\n    videoId string\n    title_ string\n    channelId string\n    channelTitle string\n}\n\nwatchHistory.title_ - preprocessedWatchHistory.title_\nwatchHistory.titleUrl - preprocessedWatchHistory.videoId\nwatchHistory.subtitle - preprocessedWatchHistory.channelTitle\nwatchHistory.subtitle - preprocessedWatchHistory.channelId\n\nvideoDetails [icon: youtube, color: red] {\n    id string\n    title_ string\n    channelId string\n    description string\n    tags list[string]\n    publishedAt datetime\n}\n\nmergedData [icon: table, color: yellow] {\n    videoId string\n    title_ string\n    channelId string\n    channelTitle string\n    description string\n    tags string\n    publishedAt datetime\n}\n\npreprocessedWatchHistory.videoId - mergedData.videoId\npreprocessedWatchHistory.title_ - mergedData.title_\npreprocessedWatchHistory.channelId - mergedData.channelId\npreprocessedWatchHistory.channelTitle - mergedData.channelTitle\nvideoDetails.tags - mergedData.tags\nvideoDetails.description - mergedData.description\nvideoDetails.publishedAt - mergedData.publishedAt\n</code></pre> Diagram with mermaid <pre><code>erDiagram\n    WATCH_HISTORY {\n        title_ string\n        titleUrl string\n        time datetime\n        subtitle dict\n    }\n\n    PREPROCESSED_WATCH_HISTORY {\n        videoId string\n        title_ string\n        channelId string\n        channelTitle string\n    }\n\n    VIDEO_DETAILS {\n        id string\n        title_ string\n        channelId string\n        description string\n        tags list\n        publishedAt datetime\n    }\n\n    MERGED_DATA {\n        videoId string\n        title_ string\n        channelId string\n        channelTitle string\n        description string\n        tags string\n        publishedAt datetime\n    }\n\n    WATCH_HISTORY ||--|| PREPROCESSED_WATCH_HISTORY: title_\n    WATCH_HISTORY ||--|| PREPROCESSED_WATCH_HISTORY: titleUrl\n    WATCH_HISTORY ||--|| PREPROCESSED_WATCH_HISTORY: subtitle\n    WATCH_HISTORY ||--|| PREPROCESSED_WATCH_HISTORY: subtitle\n\n    PREPROCESSED_WATCH_HISTORY ||--|| MERGED_DATA: videoId\n    PREPROCESSED_WATCH_HISTORY ||--|| MERGED_DATA: title_\n    PREPROCESSED_WATCH_HISTORY ||--|| MERGED_DATA: channelId\n    PREPROCESSED_WATCH_HISTORY ||--|| MERGED_DATA: channelTitle\n\n    VIDEO_DETAILS ||--|| MERGED_DATA: tags\n    VIDEO_DETAILS ||--|| MERGED_DATA: description\n    VIDEO_DETAILS ||--|| MERGED_DATA: publishedAt</code></pre>"},{"location":"project/yt-watch-history/v2-channel_reco/","title":"Channel Recommender System V2","text":"<p>The system starts with preprocessing the video titles and tags by tokenizing, removing stop words, and stemming/lemmatizing them. Then, these texts are vectorized using TF-IDF to convert them into numerical form. The feature vectors are used to calculate similarities between channels using cosine similarity. The channels are ranked based on these similarity scores, and the top-N channels are recommended to the user.</p> <p>This approach leverages the content-based filtering technique to find and recommend channels that have similar video titles and tags to those the user has already watched, ensuring relevant and personalized recommendations.</p>"},{"location":"project/yt-watch-history/v2-channel_reco/#approach-layers","title":"Approach Layers","text":"<ol> <li>Input Layer: Accepts video titles and tags.</li> <li>Text Processing Layer: Applies tokenization, stop words removal, and stemming/lemmatization.</li> <li>Vectorization Layer: Converts processed text to TF-IDF vectors.</li> <li>Similarity Calculation Layer: Computes similarities between channels using cosine similarity.</li> <li>Recommendation Layer: Ranks channels based on similarity scores and recommends the top-N channels.</li> </ol> <pre><code>graph\n    subgraph DataPreprocessing\n        C(Tokenization) --&gt; D(Stop Words Removal)\n        D --&gt; E(Stemming/Lemmatization)\n        E --&gt; F(TF-IDF Vectorization)\n    end\n\n    subgraph SimilarityCalculation\n        G(Feature Vectors) --&gt; H(Similarity Calculation)\n        H --&gt; I(Channel Recommendations)\n    end\n\n    A1(Video Title) --&gt; DataPreprocessing\n    A2(Video Tags) --&gt; DataPreprocessing\n    DataPreprocessing --&gt; pca{{ PCA }} --&gt; SimilarityCalculation</code></pre>"},{"location":"project/yt-watch-history/v2-channel_reco/#process-explanation","title":"Process Explanation","text":"<ol> <li> <p>Data Preprocessing:</p> <ul> <li>Tokenization: Split titles and tags into individual words or phrases.</li> <li>Stop Words Removal: Remove common words that do not add value to the recommendation (e.g., \"the\", \"and\").</li> <li>Stemming/Lemmatization: Reduce words to their base form to handle different variations.</li> <li>Vectorization: Convert the processed text into numerical vectors using methods like TF-IDF (Term Frequency-Inverse Document Frequency).</li> </ul> </li> <li> <p>Feature Extraction:</p> <ul> <li>TF-IDF Vectorizer: Transform the textual data into feature vectors.</li> <li>Dimensionality Reduction: Optionally, use techniques like PCA (Principal Component Analysis) to reduce the feature space.</li> </ul> </li> <li> <p>Model Training:</p> <ul> <li>Content-Based Filtering: Calculate the similarity between videos using cosine similarity or other distance metrics.</li> <li>Training Algorithm: Use algorithms like k-Nearest Neighbors (k-NN) for finding similar channels based on the content similarity of their videos.</li> </ul> </li> <li> <p>Recommendation:</p> <ul> <li>For a given user's watched history, calculate the similarity scores of channels and recommend the top-N channels with the highest scores.</li> </ul> </li> </ol>"},{"location":"blog/archive/2025/07/","title":"July, 2025","text":""},{"location":"blog/archive/2025/02/","title":"February, 2025","text":""},{"location":"blog/archive/2025/01/","title":"January, 2025","text":""},{"location":"blog/archive/2024/12/","title":"December, 2024","text":""},{"location":"blog/archive/2024/06/","title":"June, 2024","text":""},{"location":"blog/archive/2024/05/","title":"May, 2024","text":""},{"location":"blog/archive/2024/04/","title":"April, 2024","text":""},{"location":"blog/archive/2024/03/","title":"March, 2024","text":""},{"location":"blog/archive/2024/02/","title":"February, 2024","text":""},{"location":"blog/archive/2024/01/","title":"January, 2024","text":""},{"location":"blog/c/ai/","title":"ai","text":""},{"location":"blog/c/llm/","title":"llm","text":""},{"location":"blog/c/deep-learning/","title":"deep-learning","text":""},{"location":"blog/c/pytorch/","title":"pytorch","text":""},{"location":"blog/c/transformers/","title":"transformers","text":""},{"location":"blog/c/python/","title":"python","text":""},{"location":"blog/c/docker/","title":"docker","text":""},{"location":"blog/c/fastapi/","title":"fastapi","text":""},{"location":"blog/c/others/","title":"others","text":""},{"location":"blog/c/eda/","title":"eda","text":""},{"location":"blog/c/statistics/","title":"statistics","text":""},{"location":"blog/c/ml/","title":"ml","text":""},{"location":"blog/c/bash/","title":"bash","text":""},{"location":"blog/c/tips/","title":"tips","text":""},{"location":"blog/c/interview-questions/","title":"interview-questions","text":""},{"location":"blog/c/mlops/","title":"mlops","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/page/5/","title":"Blog","text":""},{"location":"blog/archive/2024/03/page/2/","title":"March, 2024","text":""},{"location":"blog/c/ml/page/2/","title":"ml","text":""}]}